
@inproceedings{vinciarelli_proceeding_2008,
	address = {Vancouver, British Columbia, Canada},
	title = {Proceeding of the 16th {ACM} international conference on Multimedia},
	isbn = {978-1-60558-303-7},
	shorttitle = {Social signal processing},
	doi = {10.1145/1459359.1459573},
	abstract = {The ability to understand and manage social signals of a person we are communicating with is the core of social intelligence. Social intelligence is a facet of human intelligence that has been argued to be indispensable and perhaps the most important for success in life. This paper argues that next-generation computing needs to include the essence of social intelligence - the ability to recognize human social signals and social behaviours like politeness, and disagreement - in order to become more effective and more efficient. Although each one of us understands the importance of social signals in everyday life situations, and in spite of recent advances in machine analysis of relevant behavioural cues like blinks, smiles, crossed arms, laughter, and similar, design and development of automated systems for Social Signal Processing {(SSP)} are rather difficult. This paper surveys the past efforts in solving these problems by a computer, it summarizes the relevant findings in social psychology, and it proposes a set of recommendations for enabling the development of the next generation of socially-aware computing.},
	publisher = {{ACM}},
	author = {Alessandro Vinciarelli and Maja Pantic and Hervé Bourlard and Alex Pentland},
	year = {2008},
	keywords = {behaviour analysis, human centered computing, social signals},
	pages = {1061--1070}
},

@inproceedings{vinciarelli_proceedings_2008,
	address = {Chania, Crete, Greece},
	title = {Proceedings of the 10th international conference on Multimodal interfaces},
	isbn = {978-1-60558-198-9},
	shorttitle = {Social signals, their function, and automatic analysis},
	doi = {10.1145/1452392.1452405},
	abstract = {Social Signal Processing {(SSP)} aims at the analysis of social behaviour in both {Human-Human} and {Human-Computer} interactions. {SSP} revolves around automatic sensing and interpretation of social signals, complex aggregates of nonverbal behaviours through which individuals express their attitudes towards other human (and virtual) participants in the current social context. As such, {SSP} integrates both engineering (speech analysis, computer vision, etc.) and human sciences (social psychology, anthropology, etc.) as it requires multimodal and multidisciplinary approaches. As of today, {SSP} is still in its early infancy, but the domain is quickly developing, and a growing number of works is appearing in the literature. This paper provides an introduction to nonverbal behaviour involved in social signals and a survey of the main results obtained so far in {SSP.} It also outlines possibilities and challenges that {SSP} is expected to face in the next years if it is to reach its full maturity.},
	publisher = {{ACM}},
	author = {Alessandro Vinciarelli and Maja Pantic and Hervé Bourlard and Alex Pentland},
	year = {2008},
	keywords = {computer vision, social behaviour analysis, social signal processing, speech analysis},
	pages = {61--68}
},

@book{pentland_honest_2008,
	title = {Honest Signals: How They Shape Our World},
	isbn = {0262162563},
	shorttitle = {Honest Signals},
	publisher = {The {MIT} Press},
	author = {Alex Pentland},
	month = oct,
	year = {2008}
},

@book{_socially_????,
	title = {Socially Smart in 60 Seconds: Etiquette Dos and Donts for Personal and Business Success},
	isbn = {0736920501},
	shorttitle = {Socially Smart in 60 Seconds}
},

@inproceedings{ur_rehman_manifold_2007,
	title = {Manifold of Facial Expressions for Tactile Perception},
	abstract = {To enhance their social interactive ability, we study how to provide the sight-impaired with reliable, '"on-line" emotion information. The technical challenge we address here is how to render rich facial expressions in an intuitive way. We demonstrate that manifold of facial expressions is a compact and natural way to characterize human emotions. To compute manifold of facial expressions the standard locally linear embedding {(LLE)} algorithm is extended to handle the problem of real-time coding of new videos. Experimental results show that using manifold of facial expressions for vibrotactile rendering is very encouraging.},
	author = {S. ur Rehman and Li Liu and Haibo Li},
	year = {2007},
	keywords = {emotion recognition, face recognition, facial expressions, human emotions, social interactive ability, standard locally linear embedding algorithm, tactile perception, vibrotactile rendering, video coding},
	pages = {239--242}
},

@inproceedings{kim_predicting_2009,
	address = {Boston, {MA,} {USA}},
	title = {Predicting shoppers' interest from social interactions using sociometric sensors},
	isbn = {978-1-60558-247-4},
	doi = {10.1145/1520340.1520692},
	abstract = {Marketing research has longed for better ways to measure consumer behavior. In this paper, we explore using sociometric data to study social behaviors of group shoppers. We hypothesize that the interaction patterns among shoppers will convey their interest level, predicting probability of purchase. To verify our hypotheses, we observed co-habiting couples shopping for furniture. We have verified that there are sensible differences in customer behavior depending on their interest level. When couples are interested in an item they observe the item for a longer duration of time and have a more balanced speaking style. A real-time prediction model was constructed using a decision tree with a prediction accuracy reaching 79.8\% and a sensitivity of 63\%.},
	publisher = {{ACM}},
	author = {Taemie J. Kim and Maurice Chu and Oliver Brdiczka and James Begole},
	year = {2009},
	keywords = {behavior modeling, group dynamics, interaction pattern, interest, shopping, sociometric sensors},
	pages = {4513--4518}
},

@inproceedings{wu_mining_2008,
	address = {Paris, France},
	title = {Mining {Face-to-Face} Interaction Networks Using Sociometric Badges: Predicting Productivity in an {IT} Configuration Task},
	shorttitle = {Mining {Face-to-Face} Interaction Networks Using Sociometric Badges},
	url = {http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1130251},
	abstract = {work theories (e.g. Granovetter 1973, Burt 1992) and information richness theory {(Daft} \& Lengel 1987) have both been used independently to understand knowledge transfer in information intensive work settings. Social network theories explain how network structures covary with the diffusion and distribution of information, but largely ignore characteristics of the communication channels (or media) through which information and knowledge are transferred. Information richness theory on the other hand focuses explicitly on the communication channel requirements for different types of knowledge transfer but ignores the population level topology through which information is transferred in a network. This paper aims to bridge these two sets of theories to understand what types of social structures are most conducive to transferring knowledge and improving work performance in face-to-face communication networks. Using a novel set of data collection tools, techniques and methodologies, we were able to record precise data on the face-to-face interaction networks, tonal conversational variation and physical proximity of a group of {IT} configuration specialists over a one month period while they conducted their work. Linking these data to detailed performance and productivity metrics, we find four main results. First, the face-to-face communication networks of productive workers display very different topological structures compared to those discovered for email networks in previous research. In face-to-face networks, network cohesion is positively correlated with higher worker productivity, while the opposite is true in email communication. Second, network cohesion in face-to-face networks is associated with even higher work performance when executing complex tasks. This result suggests that network cohesion may complement information-rich communication media for transferring the complex or tacit knowledge needed to complete complex tasks. Third, the most effective network structures for latent social networks (those that characterize the network of available communication partners) differ from in-task social networks (those that characterize the network of communication partners that are actualized during the execution of a particular task). Finally, the effect of cohesion is much stronger in face-to-face networks than in physical proximity networks, demonstrating that information flows in actual conversations (rather than mere physical proximity) are driving our results. Our work bridges two influential bodies of research in order to contrast face-to-face network structure with network structure in electronic communication. We also contribute a novel set of tools and techniques for discovering and recording precise face-to-face interaction data in real world work settings.},
	author = {Lynn Wu and Benjamin N. Waber and Sinan Aral and Erik Brynjolfsson and Alex Pentland},
	month = may,
	year = {2008},
	keywords = {{Face-to-Face} Communication, Information Worker, Productivity, Social Networks}
},

@inproceedings{kim_meeting_2008,
	address = {San Diego, {CA,} {USA}},
	title = {Meeting Mediator: Enhancing Group Collaboration and Leadership with Sociometric Feedback},
	author = {Taemie Kim and Agnes Chang and Lindsey Holland and Alex Pentland},
	year = {2008},
	pages = {457--466}
},

@article{olguin_sensible_2009,
	title = {Sensible Organizations: Technology and Methodology for Automatically Measuring Organizational Behavior},
	volume = {39},
	issn = {1083-4419},
	shorttitle = {Sensible Organizations},
	abstract = {We present the design, implementation, and deployment of a wearable computing platform for measuring and analyzing human behavior in organizational settings. We propose the use of wearable electronic badges capable of automatically measuring the amount of face-to-face interaction, conversational time, physical proximity to other people, and physical activity levels in order to capture individual and collective patterns of behavior. Our goal is to be able to understand how patterns of behavior shape individuals and organizations. By using on-body sensors in large groups of people for extended periods of time in naturalistic settings, we have been able to identify, measure, and quantify social interactions, group behavior, and organizational dynamics. We deployed this wearable computing platform in a group of 22 employees working in a real organization over a period of one month. Using these automatic measurements, we were able to predict employees' self-assessments of job satisfaction and their own perceptions of group interaction quality by combining data collected with our platform and e-mail communication data. In particular, the total amount of communication was predictive of both of these assessments, and betweenness in the social network exhibited a high negative correlation with group interaction satisfaction. We also found that physical proximity and e-mail exchange had a negative correlation of r = -0.55\&nbsp;(p 0.01), which has far-reaching implications for past and future research on social networks.},
	number = {1},
	journal = {Systems, Man, and Cybernetics, Part B: Cybernetics, {IEEE} Transactions on},
	author = {{D.O.} Olguin and {B.N.} Waber and Taemie Kim and A. Mohan and K. Ara and A. Pentland},
	year = {2009},
	keywords = {behavioural sciences computing, conversational time, e-mail exchange, face-to-face interaction, group interaction satisfaction, human behavior measurement, job satisfaction, mobile computing, on-body sensor, organisational aspects, Organizational behavior, organizational setting, physical proximity, social computing, social network, sociometric badges, wearable computers, wearable computing, wearable computing platform, wearable electronic badge},
	pages = {43--55}
}?
@inproceedings{shinohara_designing_2006,
	address = {Portland, Oregon, {USA}},
	title = {Designing assistive technology for blind users},
	isbn = {1-59593-290-9},
	url = {http://portal.acm.org/citation.cfm?id=1169062},
	doi = {10.1145/1168987.1169062},
	abstract = {This project reports on an observational and interview study of a non-sighted person to develop design insights for enhancing interactions between a blind person and everyday technological artifacts found in their home such as wristwatches, cell phones or software applications. Analyzing situations where work-arounds compensate for task failures reveals important insights for future artifact design for the blind such as the value of socialization, tactile and audio feedback, and facilitation of user independence.},
	booktitle = {Proceedings of the 8th international {ACM} {SIGACCESS} conference on Computers and accessibility},
	publisher = {{ACM}},
	author = {Kristen Shinohara},
	year = {2006},
	keywords = {assistive technology, design, human computer interaction, principles, technology biographies},
	pages = {293--294}
},

@inproceedings{shinohara_observing_2007,
	address = {Tempe, Arizona, {USA}},
	title = {Observing Sara: a case study of a blind person's interactions with technology},
	isbn = {978-1-59593-573-1},
	shorttitle = {Observing Sara},
	url = {http://portal.acm.org/citation.cfm?id=1296873&dl=GUIDE&coll=GUIDE&CFID=63001009&CFTOKEN=61436911},
	doi = {10.1145/1296843.1296873},
	abstract = {While software is increasingly being improved to enhance access and use, software interfaces nonetheless often create barriers for people who are blind. In response, the blind computer user develops workarounds, strategies to overcome the constraints of a physical and social world engineered for the sighted. This paper describes an interview and observational study of a blind college student interacting with various technologies within her home. Structured around Blythe, Monk and Park's Technology Biographies, these experience centered sessions focus not only on technology function, but on the relationship of function to the meanings and values that this student attributes to technology use in different settings. Studying a single user across a range of devices and tasks provides a broader and more nuanced understanding of the contexts and causes of task failure and of the workarounds employed than is possible with a more narrowly focused usability study. Themes that were revealed across a range of tasks include the importance for technologies to not "mark" the user as being blind within a predominantly sighted social world, to support user independence through portability and user control, and to allow user "resets" and brute-force fallbacks in the face of persistent task failure.},
	booktitle = {Proceedings of the 9th international {ACM} {SIGACCESS} conference on Computers and accessibility},
	publisher = {{ACM}},
	author = {Kristen Shinohara and Josh Tenenberg},
	year = {2007},
	keywords = {assistive technology, technology biographies, user-centered inclusive design},
	pages = {171--178}
},

@article{kulyukin_robot-assisted_2006,
	title = {Robot-assisted wayfinding for the visually impaired in structured indoor environments},
	volume = {21},
	url = {http://dx.doi.org/10.1007/s10514-006-7223-8},
	doi = {10.1007/s10514-006-7223-8},
	abstract = {Abstract  We present a robot-assisted wayfinding system for the visually impaired in structured indoor environments. The system consists
of a mobile robotic guide and small passive {RFID} sensors embedded in the environment. The system is intended for use in indoor
environments, such as office buildings, supermarkets and airports. We describe how the system was deployed in two indoor environments
and evaluated by visually impaired participants in a series of pilot experiments. We analyze the system’s successes and failures
and outline our plans for future research and development.},
	number = {1},
	journal = {Autonomous Robots},
	author = {Vladimir Kulyukin and Chaitanya Gharpure and John Nicholson and Grayson Osborne},
	year = {2006},
	pages = {29--41}
},

@inproceedings{yuan_dynamic_2005,
	title = {Dynamic environment exploration using a virtual white cane},
	volume = {1},
	isbn = {1063-6919},
	doi = {10.1109/CVPR.2005.136},
	abstract = {The virtual white cane is a range sensing device based on active triangulation, that can measure distances at a rate of 15 measurements/second. A blind person can use this device for sensing the environment, pointing it as if it was a flashlight. Beside measuring distances, this device can detect surface discontinuities, such as the foot of a wall, a step, or a drop-off. This is obtained by analyzing the range data collected as the user swings the device around, tracking planar patches and finding discontinuities. In this paper we briefly describe the range sensing device, and present an online surface tracking algorithm, based on a {Jump-Markov} model. We show experimental results proving the robustness of the tracking system in real-world conditions.},
	booktitle = {Computer Vision and Pattern Recognition, 2005. {CVPR} 2005. {IEEE} Computer Society Conference on},
	author = {D. Yuan and R. Manduchi},
	year = {2005},
	keywords = {active triangulation, dynamic environment exploration, intelligent sensors, {Jump-Markov} model, Markov processes, mesh generation, object detection, online surface tracking algorithm, planar patches tracking, range sensing device, surface discontinuity detection, virtual white cane},
	pages = {243--249 vol. 1}
},

@article{rehman_vibrotactile_2008,
	title = {Vibrotactile Rendering of Human Emotions on the Manifold of Facial Expressions},
	volume = {3},
	number = {3},
	journal = {Journal of Multimedia},
	author = {S. Rehman and Li Liu and Haibo Li},
	year = {2008},
	pages = {18--25}
},

@inproceedings{teeters_self-cam:_2006,
	address = {Boston, Massachusetts},
	title = {{Self-Cam:} feedback from what would be your social partner},
	isbn = {1-59593-364-6},
	shorttitle = {{Self-Cam}},
	url = {http://dx.doi.org/10.1145/1179622.1179782},
	abstract = {Note: {OCR} errors may be found in this Reference List extracted from the full text article. {ACM} has opted to expose the complete List rather than only correct and linked references.},
	booktitle = {{SIGGRAPH} '06: {ACM} {SIGGRAPH} 2006 Research posters},
	publisher = {{ACM}},
	author = {Alea Teeters and Rana Kaliouby and Rosalind Picard},
	year = {2006},
	keywords = {{AUTISM,} picard, royalsocietycemm},
	pages = {138}
},

@inproceedings{vinciarelli_social_2008,
	address = {Chania, Crete, Greece},
	title = {Social signals, their function, and automatic analysis: a survey},
	isbn = {978-1-60558-198-9},
	shorttitle = {Social signals, their function, and automatic analysis},
	url = {http://portal.acm.org/citation.cfm?id=1452405},
	doi = {10.1145/1452392.1452405},
	abstract = {Social Signal Processing {(SSP)} aims at the analysis of social behaviour in both {Human-Human} and {Human-Computer} interactions. {SSP} revolves around automatic sensing and interpretation of social signals, complex aggregates of nonverbal behaviours through which individuals express their attitudes towards other human (and virtual) participants in the current social context. As such, {SSP} integrates both engineering (speech analysis, computer vision, etc.) and human sciences (social psychology, anthropology, etc.) as it requires multimodal and multidisciplinary approaches. As of today, {SSP} is still in its early infancy, but the domain is quickly developing, and a growing number of works is appearing in the literature. This paper provides an introduction to nonverbal behaviour involved in social signals and a survey of the main results obtained so far in {SSP.} It also outlines possibilities and challenges that {SSP} is expected to face in the next years if it is to reach its full maturity.},
	booktitle = {Proceedings of the 10th international conference on Multimodal interfaces},
	publisher = {{ACM}},
	author = {Alessandro Vinciarelli and Maja Pantic and Hervé Bourlard and Alex Pentland},
	year = {2008},
	keywords = {computer vision, social behaviour analysis, social signal processing, speech analysis},
	pages = {61--68}
},

@inproceedings{vinciarelli_social_2008-1,
	address = {Vancouver, British Columbia, Canada},
	title = {Social signal processing: state-of-the-art and future perspectives of an emerging domain},
	isbn = {978-1-60558-303-7},
	shorttitle = {Social signal processing},
	url = {http://portal.acm.org/citation.cfm?id=1459359.1459573&coll=Portal&dl=GUIDE&CFID=63090442&CFTOKEN=27931350},
	doi = {10.1145/1459359.1459573},
	abstract = {The ability to understand and manage social signals of a person we are communicating with is the core of social intelligence. Social intelligence is a facet of human intelligence that has been argued to be indispensable and perhaps the most important for success in life. This paper argues that next-generation computing needs to include the essence of social intelligence - the ability to recognize human social signals and social behaviours like politeness, and disagreement - in order to become more effective and more efficient. Although each one of us understands the importance of social signals in everyday life situations, and in spite of recent advances in machine analysis of relevant behavioural cues like blinks, smiles, crossed arms, laughter, and similar, design and development of automated systems for Social Signal Processing {(SSP)} are rather difficult. This paper surveys the past efforts in solving these problems by a computer, it summarizes the relevant findings in social psychology, and it proposes a set of recommendations for enabling the development of the next generation of socially-aware computing.},
	booktitle = {Proceeding of the 16th {ACM} international conference on Multimedia},
	publisher = {{ACM}},
	author = {Alessandro Vinciarelli and Maja Pantic and Hervé Bourlard and Alex Pentland},
	year = {2008},
	keywords = {behaviour analysis, human centered computing, social signals},
	pages = {1061--1070}
}?
@article{yeasin_recognition_2006,
	title = {Recognition of facial expressions and measurement of levels of interest from video},
	volume = {8},
	issn = {1520-9210},
	doi = {10.1109/TMM.2006.870737},
	abstract = {This paper presents a spatio-temporal approach in recognizing six universal facial expressions from visual data and using them to compute levels of interest. The classification approach relies on a two-step strategy on the top of projected facial motion vectors obtained from video sequences of facial expressions. First a linear classification bank was applied on projected optical flow vectors and decisions made by the linear classifiers were coalesced to produce a characteristic signature for each universal facial expression. The signatures thus computed from the training data set were used to train discrete hidden Markov models {(HMMs)} to learn the underlying model for each facial expression. The performances of the proposed facial expressions recognition were computed using five fold cross-validation on {Cohn-Kanade} facial expressions database consisting of 488 video sequences that includes 97 subjects. The proposed approach achieved an average recognition rate of 90.9\% on {Cohn-Kanade} facial expressions database. Recognized facial expressions were mapped to levels of interest using the affect space and the intensity of motion around apex frame. Computed level of interest was subjectively analyzed and was found to be consistent with "ground truth" information in most of the cases. To further illustrate the efficacy of the proposed approach, and also to better understand the effects of a number of factors that are detrimental to the facial expression recognition, a number of experiments were conducted. The first empirical analysis was conducted on a database consisting of 108 facial expressions collected from {TV} broadcasts and labeled by human coders for subsequent analysis. The second experiment (emotion elicitation) was conducted on facial expressions obtained from 21 subjects by showing the subjects six different movies clips chosen in a manner to arouse spontaneous emotional reactions that would produce natural facial expressions.},
	number = {3},
	journal = {Multimedia, {IEEE} Transactions on},
	author = {M. Yeasin and B. Bullot and R. Sharma},
	year = {2006},
	keywords = {{Cohn-Kanade} facial expressions database, discrete hidden Markov model training, emotion recognition, Emotions, empirical analysis, face detection, face recognition, facial motion vector projection, hidden Markov models, hidden Markov models {(HMMs),} {HMM,} image motion analysis, image sequences, learning (artificial intelligence), levels of interest, levels of interest measurement, linear classification bank, machine learning, optical flow vector projection, pattern classification, spatio-temporal approach, universal facial expression recognition, universal facial expressions, video sequences, visual databases},
	pages = {500--508}
},

@inproceedings{jaewon_sung_combining_2007,
	title = {Combining Local and Global Motion Estimators for Robust Face Tracking},
	doi = {10.1109/ROMAN.2007.4415107},
	abstract = {Faces show both global and local motions, where the former represents rigid head movements due to {3D} translation and rotation and the local motion represents non-rigid deformation due to speech, or facial expressions. Although non- rigid face models can represent both types of the facial motions, they are not enough to track the facial motions correctly. The non-rigid face models have large number of model parameters to explain various deformation of the face and the high dimensionality of their model parameter space make them sensitive to initial model parameters, apt to be stuck to local minimum, and difficult to be recovered (re-initialized) from failure when iterative gradient descent optimization techniques are used. To alleviate these problems, we propose to use two types of face trackers that are suitable for estimating the global and local motions, respectively. In the proposed algorithm, the global motion estimator is applied at first and the estimated global motion is used to compute proper initial model parameters of the local motion estimator to make it converge correctly. In this paper, we used active appearance model {(MM)} and cylinder head model {(CHM)} as the representative examples of the non- rigid and rigid face models. Experimental results showed that face tracking combining {AAMs} and {CHMs} improved the face tracking performance than that of {AAMs} in terms of 170\% higher tracking rate and the 115\% wider pose coverage.},
	author = {Jaewon Sung and Daijin Kim},
	year = {2007},
	keywords = {{3D} rotation, {3D} translation, active appearance model, cylinder head model, face recognition, facial motion, global motion estimator, gradient methods, iterative gradient descent optimization techniques, motion estimation, optical tracking, optimisation, robust face tracking},
	pages = {345--350}
},

@inproceedings{fawky_eye_2007,
	title = {Eye detection to assist drowsy drivers},
	doi = {10.1109/ITICT.2007.4475632},
	abstract = {This paper proposes a new approach to detect the eyes opening state for the purpose of alarming drowsy drivers on highroads. The input image frame is processed in successive steps using a combination of algorithms, namely wavelets transform, edge detection and {YCbCr} transform. The algorithm performs well, regardless the size of the image and is immune to distracting noises, high luminance, and background lighting. The proposed algorithm was able to attain an average eye detection accuracy of 80\% when tested on different data bases and different conditions.},
	author = {A. Fawky and S. Khalil and M. Elsabrouty},
	year = {2007},
	keywords = {drowsy drivers, edge detection, eye detection, object detection, Skin Filter, wavelet transforms, Wavelets Transform, {YCbCr,} {YCbCr} transform},
	pages = {131--134}
},

@inproceedings{kulic_combining_2008,
	title = {Combining automated on-line segmentation and incremental clustering for whole body motions},
	isbn = {1050-4729},
	doi = {10.1109/ROBOT.2008.4543603},
	abstract = {This paper describes a novel approach for incremental learning of human motion pattern primitives through on-line observation of human motion. The observed motion time series data stream is first stochastically segmented into potential motion primitive segments, based on the assumption that data belonging to the same motion primitive will have the same underlying distribution. The motion segments are then abstracted into a stochastic model representation, and automatically clustered and organized. As new motion patterns are observed, they are incrementally grouped together based on their relative distance in the model space. The resulting representation of the knowledge domain is a tree structure, with specialized motions at the tree leaves, and generalized motions closer to the root. The tree leaves, which represent the most specialized learned motion primitives, are then passed back to the segmentation algorithm, so that as the number of known motion primitives increases, the accuracy of the segmentation can also be improved. The combined algorithm is tested on a sequence of continuous human motion data obtained through motion capture, and demonstrates the performance of the proposed approach.},
	author = {D. Kulic and W. Takano and Y. Nakamura},
	year = {2008},
	keywords = {automated online segmentation, humanoid robots, image motion analysis, image segmentation, incremental clustering, incremental learning, intelligent robots, learning (artificial intelligence), motion segments, segmentation algorithm, tree structure, whole body motions},
	pages = {2591--2598}
},

@article{villanueva_novel_2008,
	title = {A Novel Gaze Estimation System With One Calibration Point},
	volume = {38},
	issn = {1083-4419},
	doi = {10.1109/TSMCB.2008.926606},
	abstract = {The design of robust and high-performance gaze-tracking systems is one of the most important objectives of the eye-tracking community. In general, a subject calibration procedure is needed to learn system parameters and be able to estimate the gaze direction accurately. In this paper, we attempt to determine if subject calibration can be eliminated. A geometric analysis of a gaze-tracking system is conducted to determine user calibration requirements. The eye model used considers the offset between optical and visual axes, the refraction of the cornea, and Donder's law. This paper demonstrates the minimal number of cameras, light sources, and user calibration points needed to solve for gaze estimation. The underlying geometric model is based on glint positions and pupil ellipse in the image, and the minimal hardware needed for this model is one camera and multiple light-emitting diodes. This paper proves that subject calibration is compulsory for correct gaze estimation and proposes a model based on a single point for subject calibration. The experiments carried out show that, although two glints and one calibration point are sufficient to perform gaze estimation (error {\textasciitilde} 1deg), using more light sources and calibration points can result in lower average errors.},
	number = {4},
	journal = {Systems, Man, and Cybernetics, Part B, {IEEE} Transactions on},
	author = {A. Villanueva and R. Cabeza},
	year = {2008},
	keywords = {calibration, eye model, Eye tracking, gaze direction estimation, gaze estimation, gaze tracking system, geometric analysis, geometric model, glint position, graphical user interfaces, line of sight {(LOS),} point of regard {(POR),} pupil ellipse, subject calibration, user calibration point, user calibration requirement, video oculography, video signal processing},
	pages = {1123--1138}
},

@inproceedings{bolder_robotics_2007,
	title = {Robotics and Automation, 2007 {IEEE} International Conference on},
	isbn = {1050-4729},
	doi = {10.1109/ROBOT.2007.363936},
	abstract = {We describe a system for visual interaction developed for humanoid robots. It enables the robot to interact with its environment using a smooth whole body motion control driven by stabilized visual targets. Targets are defined as visually extracted "proto-objects" and behavior-relevant object hypotheses and are stabilized by means of a short-term sensory memory. Selection mechanisms are used to switch between behavior alternatives for searching or tracking objects as well as different whole body motion strategies for reaching. The decision between different motion strategies like reaching with right or left hand or with and without walking is made based on internal predictions that use copies of the whole-body control algorithm. The results show robust object tracking and a smooth interaction behavior that includes a large variety of whole-body postures.},
	author = {B. Bolder and M. Dunn and M. Gienger and H. Janssen and H. Sugiura and C. Goerick},
	year = {2007},
	keywords = {humanoid robots, mobile robots, motion control, object tracking, robot vision, selection mechanisms, short-term sensory memory, visually guided whole body interaction},
	pages = {3054--3061}
},

@inproceedings{yinggang_xie_control_2006,
	title = {Control Conference, 2006. {CCC} 2006. Chinese},
	doi = {10.1109/CHICC.2006.280890},
	abstract = {Affective computing is becoming a new research hotspot. An effective method of facial and eye detection is presented in this paper, which uses skin-color model and key facial feature points, based on image processing and expression recognition. Finally the realization is introduced that the solution has been applied to the e-learning system to cope with the emotion and cognition of the student interest},
	author = {Yinggang Xie and Zhiliang Wang and Ning Cheng and Guojiang Wang and M. Nagai},
	year = {2006},
	keywords = {affective computing, affective recognition, Artificial Psychology, e-learning system, expression recognition, Eye, eye detection, face recognition, Facial Detection, image colour analysis, image processing, object detection, skin-color model},
	pages = {1942--1946}
},

@article{bacivarov_statistical_2008,
	title = {Statistical models of appearance for eye tracking and eye-blink detection and measurement},
	volume = {54},
	issn = {0098-3063},
	doi = {10.1109/TCE.2008.4637622},
	abstract = {A statistical Active Appearance Model {(AAM)} is developed to track and detect eye blinking. The model has been designed to be robust to variations of head pose or gaze. In particular we analyze and determine the model parameters which encode the variations caused by blinking. This global model is further extended using a series of sub-models to enable independent modeling and tracking of the two eye regions. Several methods to enable measurement and detection of eye-blink are proposed and evaluated. The results of various tests on different image databases are presented to validate each model.},
	number = {3},
	journal = {Consumer Electronics, {IEEE} Transactions on},
	author = {I. Bacivarov and M. Ionita and P. Corcoran},
	year = {2008},
	keywords = {eye blinking, eye tracking, active appearance model, digital imaging},
	pages = {1312--1320}
},

@article{rajashekar_gaffe:_2008,
	title = {{GAFFE:} A {Gaze-Attentive} Fixation Finding Engine},
	volume = {17},
	issn = {1057-7149},
	shorttitle = {{GAFFE}},
	doi = {10.1109/TIP.2008.917218},
	abstract = {The ability to automatically detect visually interesting regions in images has many practical applications, especially in the design of active machine vision and automatic visual surveillance systems. Analysis of the statistics of image features at observers' gaze can provide insights into the mechanisms of fixation selection in humans. Using a foveated analysis framework, we studied the statistics of four low-level local image features: luminance, contrast, and bandpass outputs of both luminance and contrast, and discovered that image patches around human fixations had, on average, higher values of each of these features than image patches selected at random. Contrast-bandpass showed the greatest difference between human and random fixations, followed by luminance-bandpass, {RMS} contrast, and luminance. Using these measurements, we present a new algorithm that selects image regions as likely candidates for fixation. These regions are shown to correlate well with fixations recorded from human observers.},
	number = {4},
	journal = {Image Processing, {IEEE} Transactions on},
	author = {U. Rajashekar and I. van der Linde and {A.C.} Bovik and {L.K.} Cormack},
	year = {2008},
	keywords = {active machine vision, active vision, automatic visual surveillance systems, bandpass outputs, contrast feature, Eye tracking, feature extraction, fixation selection, foveated analysis framework, foveation, {GAFFE} engine, gaze-attentive fixation finding engine, image feature statistics analysis, image patches, luminance feature, object detection, observer gaze, point-of-gaze, statistical analysis, visually interesting region detection},
	pages = {564--573}
},

@inproceedings{valenti_accurate_2008,
	title = {Accurate eye center location and tracking using isophote curvature},
	isbn = {1063-6919},
	doi = {10.1109/CVPR.2008.4587529},
	abstract = {The ubiquitous application of eye tracking is precluded by the requirement of dedicated and expensive hardware, such as infrared high definition cameras. Therefore, systems based solely on appearance (i.e. not involving active infrared illumination) are being proposed in literature. However, although these systems are able to successfully locate eyes, their accuracy is significantly lower than commercial eye tracking devices. Our aim is to perform very accurate eye center location and tracking, using a simple Web cam. By means of a novel relevance mechanism, the proposed method makes use of isophote properties to gain invariance to linear lighting changes (contrast and brightness), to achieve rotational invariance and to keep low computational costs. In this paper we test our approach for accurate eye location and robustness to changes in illumination and pose, using the {BioIDand} the Yale Face B databases, respectively. We demonstrate that our system can achieve a considerable improvement in accuracy over state of the art techniques.},
	author = {R. Valenti and T. Gevers},
	year = {2008},
	keywords = {{BioID} database, computer vision, eye center location, eye center tracking, infrared high definition cameras, isophote curvature, relevance mechanism, visual databases, Web cam, Yale Face B database},
	pages = {1--8}
},

@inproceedings{funahashi_control_2007,
	title = {Control, Automation and Systems, 2007. {ICCAS} '07. International Conference on},
	doi = {10.1109/ICCAS.2007.4406545},
	abstract = {We propose a system for extracting eye gaze information, and develop a couple of new human interface media by eye gaze information. For example, the observer wants to know visually how effectively the partner is interesting to him or not even on the net environment. To do this it is usually expected to utilize motion images. But, the observer wants to know exclusively facial image and gaze. So, we paid attention to face tracking from the scene and analyzed the features of eye gaze pattern extracted from the facial images by iris recognition.},
	author = {T. Funahashi and T. Fujiwara and H. Koshimizu},
	year = {2007},
	keywords = {eye gaze information extraction, eye gaze patterns, Eye tracking, face recognition, face tracking, facial image, feature extraction, gaze analysis, Gaze Mark Pattern, human interface media, image motion analysis, iris recognition, motion images},
	pages = {1337--1341}
},

@inproceedings{miyazaki_sice_2007,
	title = {{SICE,} 2007 Annual Conference},
	doi = {10.1109/SICE.2007.4421007},
	abstract = {In order to investigate whether or not checkpoints of features surrounding the eye are suitable for eye tracking, we evaluated the checkpoints using statistical hypothesis testing. By combining the checkpoints of features surrounding the eye, the eye tracking system using the template matching was effective for the eye detection. The parameters of the checkpoints were empirically determined to detect the eye in any individual. The statistical hypothesis testing revealed that the empirical setting values of the checkpoints were significantly different from actual facial features obtained using face images. We investigated suitable parameters of the checkpoints derived from actual facial features obtained with facial images. It was revealed that new checkpoint setting's were effective in drastically reducing the possibility of incorrect eye detection, although the correct detection rate was slightly decreased.},
	author = {S. Miyazaki and H. Takano and K. Nakamura},
	year = {2007},
	keywords = {Checkpoints of features surrounding the eye, Eye, eye detection, Eye tracking, eye tracking system, face recognition, facial features, facial images, feature checkpoints, feature extraction, image matching, object detection, Statistical hypothesis testing, statistical testing, template matching, tracking},
	pages = {356--360}
},

@inproceedings{shan_du_robust_2007,
	title = {A Robust Approach for Eye Localization Under Variable Illuminations},
	volume = {1},
	isbn = {1522-4880},
	doi = {10.1109/ICIP.2007.4378970},
	abstract = {Illumination variation is a main obstacle in facial feature detection. This paper presents a novel automated approach that localizes eyes in gray-scale face images and that is robust to illumination changes. The approach does not require prior knowledge about face orientation and illumination strength. Other advantages are that no initialization and training process are needed. Based on an edge map obtained via multi-resolution wavelet transform, this approach first segments an image into different inhomogeneously illuminated regions. The illumination of every region is then adjusted so that the features' details are more pronounced. To locate the different facial features, for every region, Gabor-based image is constructed from the re-lit image. The eyes sub-regions are then identified using the edge map of the re-lit image. This method has been applied successfully to the images of the Yale B face database that have different illuminations.},
	author = {Shan Du and R. Ward},
	year = {2007},
	keywords = {biometrics, edge detection, eye detection, eye localization, face orientation, face recognition, facial feature detection, feature extraction, Gabor-based image, gray-scale face images, illumination strength, image colour analysis, image resolution, lighting, multiresolution wavelet transform, variable illuminations, wavelet transforms, wavelets, Yale B face database},
	pages = {I -- 377-I - 380}
},

@article{mitra_gesture_2007,
	title = {Gesture Recognition: A Survey},
	volume = {37},
	issn = {1094-6977},
	shorttitle = {Gesture Recognition},
	doi = {10.1109/TSMCC.2007.893280},
	abstract = {Gesture recognition pertains to recognizing meaningful expressions of motion by a human, involving the hands, arms, face, head, and/or body. It is of utmost importance in designing an intelligent and efficient human-computer interface. The applications of gesture recognition are manifold, ranging from sign language through medical rehabilitation to virtual reality. In this paper, we provide a survey on gesture recognition with particular emphasis on hand gestures and facial expressions. Applications involving hidden Markov models, particle filtering and condensation, finite-state machines, optical flow, skin color, and connectionist models are discussed in detail. Existing challenges and future research possibilities are also highlighted},
	number = {3},
	journal = {Systems, Man, and Cybernetics, Part C: Applications and Reviews, {IEEE} Transactions on},
	author = {S. Mitra and T. Acharya},
	year = {2007},
	keywords = {condensation, connectionist models, face recognition, facial expressions, finite state machines, finite-state machines, gesture recognition, hand gestures, hidden Markov models, hidden Markov models {(HMMs),} human computer interaction, image color analysis, image sequences, intelligent human-computer interface, optical flow, particle filtering, skin color, soft computing},
	pages = {311--324}
},

@inproceedings{rurainsky_image_2007,
	title = {Image Processing, 2007. {ICIP} 2007. {IEEE} International Conference on},
	volume = {3},
	isbn = {1522-4880},
	doi = {10.1109/ICIP.2007.4379249},
	abstract = {We present our system for the capturing and analysis of {3D} facial motion. A high speed camera is used as capture unit in combination with two surface mirrors. The mirrors provide two additional virtual views of the face without the need of multiple cameras and to avoid synchronization problems. We use this system to capture the motion of a person's face while speaking. Investigations of these facial motions are presented and rigid and non-rigid motion are analyzed. In order to extract only facial deformation independent from head pose, we use a new and simple approach for separating rigid and non-rigid motion named weight-compensated motion estimation {(WCME).} This approach weights the data points according to their influence to the desired motion model. We also present first results of our model-based facial deformation analysis. Such results can be used for facial animations in order to achieve a higher degree of quality.},
	author = {J. Rurainsky and P. Eisert},
	year = {2007},
	keywords = {{3D} facial motion analysis, {3D} modeling \& synthesis, face recognition, facial animation, facial deformation, facial deformation extraction, high speed camera, mirror, mirror-based multiview analysis, mirrors, motion compensation, motion estimation, multiview, nonrigid motion, parametric models for motion estimation, rigid motion, weight-compensated motion estimation},
	pages = {III -- 73-III - 76}
},

@inproceedings{nunamaker_hmm-based_2005,
	title = {{HMM-Based} Deception Recognition from Visual Cues},
	doi = {10.1109/ICME.2005.1521550},
	abstract = {Behavioral indicators of deception and behavioral state are extremely difficult for humans to analyze. This research effort attempts to leverage automated systems to augment humans in detecting deception by analyzing nonverbal behavior on video. By tracking faces and hands of an individual, it is anticipated that objective behavioral indicators of deception can be isolated, extracted and synthesized to create a more accurate means for detecting human deception. Blob analysis, a method for analyzing the movement of the head and hands based on the identification of skin color is presented. A proof-of-concept study is presented that uses Blob analysis to extract visual cues and events, throughout the examined videos. The integration of these cues is done using a hierarchical hidden Markov model to explore behavioral state identification in the detection of deception, mainly involving the detection of agitated and over-controlled behaviors},
	author = {{J.F.} Nunamaker and G. Tsechpenakis and D. Metaxas and M. Adkins and J. Kruse and {J.K.} Burgoon and {M.L.} Jensen and T. Meservy and {D.P.} Twitchell and A. Deokar},
	year = {2005},
	keywords = {automated system, behavioral state identification, Blob analysis, face recognition, face tracking, feature extraction, hand tracking, hidden Markov model, hidden Markov models, {HMM-based} deception recognition, human deception detection, objective behavioral indicator, Skin, skin color identification, tracking, visual cue extraction},
	pages = {824--827}
},

@inproceedings{fukuda_detecting_2008,
	title = {Detecting Emotions and Dangerous Actions for Better {Human-System} Team Working},
	doi = {10.1109/SSIRI.2008.55},
	abstract = {As situations change very often and extensively today, design is moving toward user-centric because designers cannot foresee the operating conditions and it is a user who knows the situations. Thus, what is called for in a system is to help a user understand the situation better and to help him or her to respond better. Thus, human characteristics must be considered more in a system design. But recent brain studies reveal that in an unexpected situation, humanpsilas brain capability decreases considerably, down to the level of about 30\% of the normal state because a human becomes emotionally unstable. Therefore, it will be very important in our future system design to keep a user emotionally stable. We carried out a series of experiments and developed several methods to detect emotions from face, from voice and from body motion and to detect dangerous actions from body motion. Our immediate goal is to apply these techniques to realize safer driving.},
	author = {S. Fukuda},
	year = {2008},
	keywords = {brain study, Dangerous Action, Detection, emotion, emotion detection, emotion recognition, face detection, face recognition, human brain capability, human computer interaction, human-system team working, speech recognition, team working, user centred design, user-centric design},
	pages = {205--206}
},

@article{jingyu_yan_factorization-based_2008,
	title = {A {Factorization-Based} Approach for Articulated Nonrigid Shape, Motion and Kinematic Chain Recovery From Video},
	volume = {30},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2007.70739},
	abstract = {Recovering articulated shape and motion, especially human body motion, from video is a challenging problem with a wide range of applications in medical study, sport analysis, animation, and so forth. Previous work on articulated motion recovery generally requires prior knowledge of the kinematic chain and usually does not concern the recovery of the articulated shape. The nonrigidity of some articulated part, for example, human body motion with nonrigid facial motion, is completely ignored. We propose a factorization-based approach to recover the shape, motion, and kinematic chain of an articulated object with nonrigid parts altogether directly from video sequences under a unified framework. The proposed approach is based on our modeling of the articulated nonrigid motion as a set of intersecting motion subspaces. A motion subspace is the linear subspace of the trajectories of an object. It can model a rigid or nonrigid motion. The intersection of two motion subspaces of linked parts models the motion of an articulated joint or axis. Our approach consists of algorithms for motion segmentation, kinematic chain building, and shape recovery. It handles outliers and can be automated. We test our approach through synthetic and real experiments and demonstrate how to recover an articulated structure with nonrigid parts via a single-view camera without prior knowledge of its kinematic chain.},
	number = {5},
	journal = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {Jingyu Yan and M. Pollefeys},
	year = {2008},
	keywords = {{3D} scene analysis, articulated, articulated nonrigid shape recovery, computer vision, factorization method, factorization-based approach, image motion analysis, image segmentation, image sequences, kinematic chain, kinematic chain recovery, matrix decomposition, motion, motion recovery, motion segmentation, non-rigid, shape, video sequences, video signal processing},
	pages = {865--877}
},

@article{meservy_deception_2005,
	title = {Deception detection through automatic, unobtrusive analysis of nonverbal behavior},
	volume = {20},
	issn = {1541-1672},
	doi = {10.1109/MIS.2005.85},
	abstract = {Every day, hundreds of thousands of people pass through airport security checkpoints, border crossing stations, or other security screening measures. Security professionals must sift through countless interactions and ferret out high-risk individuals who represent a danger to other citizens. During each interaction, the security professional must decide whether the individual is being forthright or deceptive. This task is difficult because of the limits of human vigilance and perception and the small percentage of individuals who actually harbor hostile intent. Our research initiative is based on a behavioral approach to deception detection. We attempted to build an automated system that can infer deception or truthfulness from a set of features extracted from head and hands movements in a video. A validated and reliable behaviorally based deception analysis system could potentially have great impacts in augmenting humans' abilities to assess credibility. An automated, unobtrusive system identifies behavioral patterns that indicate deception from nonverbal behavioral cues and classifies deception and truth more accurately than many humans.},
	number = {5},
	journal = {Intelligent Systems, {IEEE}},
	author = {{T.O.} Meservy and {M.L.} Jensen and J. Kruse and {J.K.} Burgoon and {J.F.} Nunamaker and {D.P.} Twitchell and G. Tsechpenakis and {D.N.} Metaxas},
	year = {2005},
	keywords = {airport security checkpoints, automated deception detection system, behavioral pattern identification, behaviorally based deception analysis system, border crossing stations, decision support, face and gesture recognition, feature representation, nonverbal behavior automatic unobtrusive analysis, nonverbal behavioral cues, public administration, security, security professional, terrorism, video analysis},
	pages = {36--43}
}?
@inproceedings{amft_detection_2005,
	title = {Detection of eating and drinking arm gestures using inertial body-worn sensors},
	doi = {10.1109/ISWC.2005.17},
	abstract = {We propose a two-stage recognition system for detecting arm gestures related to human meal intake. Information retrieved from such a system can be used for automatic dietary monitoring in the domain of behavioural medicine. We demonstrate that arm gestures can be clustered and detected using inertial sensors. To validate our method, experimental results including 384 gestures from two subjects are presented. Using isolated discrimination based on {HMMs} an accuracy of 94\% can be achieved. When spotting the gestures in continuous movement data, an accuracy of up to 87\% is reached.},
	author = {O. Amft and H. Junker and G. Troster},
	year = {2005},
	keywords = {arm gestures, automatic dietary monitoring, behavioural medicine, biology computing, body-worn sensors, gesture recognition, hidden Markov model, hidden Markov models, inertial sensor, information retrieval, sensors},
	pages = {160--163}
},

@article{ravi_activity_2005,
	title = {Activity Recognition from Accelerometer Data},
	url = {http://paul.rutgers.edu/~nravi/accelerometer.pdf},
	abstract = {Activity recognition fits within the bigger framework
of context awareness. In this paper, we report on our
efforts to recognize user activity from accelerometer
data. Activity recognition is formulated as a classification
problem. Performance of base-level classifiers and
meta-level classifiers is compared. Plurality Voting is
found to perform consistently well across different settings.},
	journal = {American Association for Artificial Intelligence},
	author = {Nishkam Ravi and Nikhil Dandekar and Prreetham Mysore and Michael Littman},
	year = {2005},
	keywords = {accelerometer, datamining, xxx}
}?
@inproceedings{russell_british_2007,
	title = {British Machine Vision Conference},
	author = {David Russell and Shaogang Gong},
	year = {2007}
},

@inproceedings{ozden_background_2005,
	title = {Background Recognition in Dynamic Scenes with Motion Constraints},
	isbn = {0-7695-2372-2},
	url = {http://portal.acm.org/citation.cfm?id=1068922},
	abstract = {Consider a monocular image sequence which contains independently moving objects and assume it is already segmented. In order to get a realistic {3D} reconstruction of such a scene, we have to solve the relative scale ambiguity between the reconstructions of different moving objects. Recently, we demonstrated the usefulness of the so-called ýnon-accidentalness and independence constraintsý to disambiguate the mentioned unknown relative scale. However, this technique requires that the video segment which corresponds to the background is known beforehand. In this paper, we analyze the background detection problem in the vein of the aforementioned constraints and show that the background is not just another moving object but the one which results in the simplest overall scene interpretation.},
	publisher = {{IEEE} Computer Society},
	author = {K. E. Ozden and L. Van Gool},
	year = {2005},
	pages = {250--255}
},

@inproceedings{hahnel_color_2004,
	title = {Color and texture features for person recognition},
	volume = {1},
	isbn = {1098-7576},
	abstract = {The need for automatic visual surveillance is increasing and the research on person recognition systems is more and more supported. As many biometric recognition methods, e.g. face recognition, are based on quite high camera resolutions which are not available in many situations, we examine features as well as classifier techniques for full body recognition. We present our experiments with color and texture features in the application of full body person recognition. On a database of 53 individuals we tested approved features for object recognition as well as {MPEG7} color and texture descriptors on a person recognition task. For comparison, we used an {RBF} network classifier as well as a nearest-neighbor classifier. Our experiments showed that color as well as texture information is important for a person recognition system. Additionally, a combination of these two kind of features results in a performance improvement.},
	author = {M. Hahnel and D. Klunder and {K.-F.} Kraiss},
	year = {2004},
	keywords = {automatic visual surveillance, biometric recognition method, classifier technique, color feature, full body person recognition, image colour analysis, image recognition, image texture, nearest-neighbor classifier, object recognition, person recognition system, radial basis function, radial basis function networks, {RBF} network classifier, surveillance, texture feature},
	pages = {652}
},

@inproceedings{todorovic_detection_2004,
	title = {Detection of Artificial Structures in {Natural-Scene} Images Using Dynamic Trees},
	isbn = {0-7695-2128-2},
	url = {http://portal.acm.org/citation.cfm?id=1020433},
	abstract = {We seek a framework that addresses localization, detection and recognition of man-made objects in natural-scene images in a unified manner. We propose to model artificial structures by dynamic tree-structured belief networks {(DTS-BNs).} {DTSBNs} provide for a distribution over tree structures that we learn using our Structured Approximation {(SVA)} inference algorithm. Furthermore, we propose multi-scale linear-discriminant analysis {(MLDA)} as a feature extraction method, which appears well suited for our goals, as we assume that man-made objects are characterized primarily by geometric regularities and by patches of uniform color. {MLDA} extracts edges over a finite range of locations, orientations and scales, decomposing an image into dyadic squares. Both the color of dyadic squares and the geometric properties of extracted edges represent observable input to our {DTSBNs.} Experimental results demonstrate that {DTS-BNs,} trained on {MLDA} features, offer a viable solution for detection of artificial structures in natural-scene images.},
	publisher = {{IEEE} Computer Society},
	author = {Sinisa Todorovic and Michael C. Nechyba},
	year = {2004},
	pages = {35--39}
},

@misc{_ultrasonic_????,
	title = {Ultrasonic range finder},
	url = {http://www.freepatentsonline.com/7330398.html}
},

@article{ren_statistical_2003,
	title = {Statistical background modeling for non-stationary camera},
	volume = {24},
	url = {http://portal.acm.org/citation.cfm?id=641699},
	abstract = {A new background subtraction method is proposed in this paper for the foreground detection from a non-stationary background. Usually, motion compensation is required when applying background subtraction to a non-stationary background. In practice, it is difficult to realize this to sufficient pixel accuracy. The problem is further complicated when the moving objects to be detected/tracked are small, since the pixel error in motion compensating the background will hide the small targets. A spatial distribution of Gaussians model is proposed to deal with moving object detection where the motion compensation is not exact but approximated. The distribution of each background pixel is temporally and spatially modeled. Based on this statistical model, a pixel in the current frame is classified as belonging to the foreground or background. In addition, a new background restoration and adaptation algorithm is developed for the non-stationary background over an extended period of time. Test cases involving a surveillance system to detect small moving objects (human and car) within a highly textured background and a pan-tilt human tracking system are demonstrated successfully.},
	number = {1-3},
	journal = {Pattern Recogn. Lett.},
	author = {Ying Ren and {Chin-Seng} Chua and {Yeong-Khing} Ho},
	year = {2003},
	keywords = {background restoration, foreground detection, non-stationary background, object tracking, sdg model},
	pages = {183--196}
},

@inproceedings{zhang_distributed_2008,
	title = {Distributed Smart Cameras, 2008. {ICDSC} 2008. Second {ACM/IEEE} International Conference on},
	doi = {10.1109/ICDSC.2008.4635727},
	abstract = {In this paper, we present a clothing recognition system that augments clothes recommendation and fashion exploration using the intelligent multi-view vision technology of the Responsive Mirror, an implicitly controlled human-computer interaction system for clothes fitting rooms. The Responsive Mirror provides shoppers with real-time “self” and “social” clothes comparisons. The system recommends clothing that is “similar” and “different” than the clothing that the person is trying on in the mirror. The goal of the research in this paper is to create a recommendation system that uses a definition of “similar” and “different” that matches human perception. We address the social nature of the recognition problem by conducting a user study to identify the salient clothes factors that people use to determine clothes similarity. We describe the computer vision and machine learning techniques employed to recognize the factors that human eyes perceive in term of clothing similarity from frontal-view outfit images. We describe the key components of the motion-tracking and clothes-recognition systems and evaluate their performance by user study and experiments on a simulated clothes fitting image dataset. The approach and results presented here will benefit designers and developers of similar applications in the future.},
	author = {Wei Zhang and Bo Begole and Maurice Chu and Juan Liu and Nicholas Yee},
	year = {2008},
	keywords = {clothes recognition, fashion, human-computer interaction, motion tracking, Multi-view vision},
	pages = {1--10}
},

@inproceedings{matta_behavioural_2006,
	title = {A Behavioural Approach to Person Recognition},
	doi = {10.1109/ICME.2006.262817},
	abstract = {This paper describes a new approach for identity recognition using video sequences. While most image and video recognition systems discriminate identities using physical information only, our approach exploits the behavioral information of head dynamics; in particular the displacement signals of few head features directly extracted in the image plane. Due to the lack of standard video database, identification and verification scores have been obtained using a small collection of video sequences: the results for this new approach are nevertheless promising},
	author = {F. Matta and {J.-L.} Dugelay},
	year = {2006},
	keywords = {feature extraction, head feature extraction, image recognition, image sequences, person recognition, video sequence, video signal processing},
	pages = {1461--1464}
},

@inproceedings{adam_aggregated_2006,
	title = {Aggregated Dynamic Background Modeling},
	isbn = {1522-4880},
	doi = {10.1109/ICIP.2006.312881},
	abstract = {Standard practices in background modeling learn a separate model for every pixel in the image. However, in dynamic scenes the connection between an observation and the place where it was observed is much less important and is usually random. For example, a wave observed in an ocean scene could easily have been observed at another place in the image. Moreover, during a limited learning period, we cannot expect to observe at every pixel all the possible background behaviors. We therefore develop in this paper a background model in which observations are decoupled from the place in the image where they were observed. A single non-parametric model is used to describe the dynamic region of the scene, aggregating the observations from the whole region. Using high-order features, we demonstrate the feasibility of our approach on challenging ocean scenes using only grayscale information},
	author = {A. Adam and E. Rivlin and I. Shimshoni},
	year = {2006},
	keywords = {background modeling, dynamic background modeling, dynamic backgrounds, feature extraction, grayscale information, image processing, nonparametric model, observation aggregation, ocean scene, video signal processing, video surveillance},
	pages = {3313--3316}
},

@article{oliver_bayesian_2000,
	title = {A Bayesian computer vision system for modeling human interactions},
	volume = {22},
	issn = {0162-8828},
	doi = {10.1109/34.868684},
	abstract = {We describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task. The system deals in particularly with detecting when interactions between people occur and classifying the type of interaction. Examples of interesting interaction behaviors include following another person, altering one's path to meet another, and so forth. Our system combines top-down with bottom-up information in a closed feedback loop, with both components employing a statistical Bayesian approach. We propose and compare two different state-based learning architectures, namely, {HMMs} and {CHMMs} for modeling behaviors and interactions. Finally, a synthetic {“Alife-style”} training system is used to develop flexible prior models for recognizing human interactions. We demonstrate the ability to use these a priori models to accurately classify real human behaviors and interactions with no additional tuning or training},
	number = {8},
	journal = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {{N.M.} Oliver and B. Rosario and {A.P.} Pentland},
	year = {2000},
	keywords = {Bayes method, Bayes methods, computer vision, hidden Markov model, hidden Markov models, human behavior recognition, image segmentation, learning systems, machine learning, object recognition, pattern recognition, people detection, real-time systems, surveillance, visual surveillance},
	pages = {831--843}
},

@article{ming-hsuan_yang_detecting_2002,
	title = {Detecting faces in images: a survey},
	volume = {24},
	issn = {0162-8828},
	shorttitle = {Detecting faces in images},
	doi = {10.1109/34.982883},
	abstract = {Images containing faces are essential to intelligent vision-based human-computer interaction, and research efforts in face processing include face recognition, face tracking, pose estimation and expression recognition. However, many reported methods assume that the faces in an image or an image sequence have been identified and localized. To build fully automated systems that analyze the information contained in face images, robust and efficient face detection algorithms are required. Given a single image, the goal of face detection is to identify all image regions which contain a face, regardless of its {3D} position, orientation and lighting conditions. Such a problem is challenging because faces are non-rigid and have a high degree of variability in size, shape, color and texture. Numerous techniques have been developed to detect faces in a single image, and the purpose of this paper is to categorize and evaluate these algorithms. We also discuss relevant issues such as data collection, evaluation metrics and benchmarking. After analyzing these algorithms and identifying their limitations, we conclude with several promising directions for future research},
	number = {1},
	journal = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {{Ming-Hsuan} Yang and {D.J.} Kriegman and N. Ahuja},
	year = {2002},
	keywords = {{3D} position, benchmarking, computer vision, data collection, evaluation metrics, expression recognition, face color, face detection algorithms, face images, face orientation, face recognition, face shape, face size, face texture, face tracking, feature extraction, fully automated systems, image sequence, intelligent vision-based human-computer interaction, lighting conditions, machine learning, object detection, object recognition, pose estimation, reviews, statistical pattern recognition, survey, view-based recognition},
	pages = {34--58}
},

@article{viola_robust_2001,
	title = {Robust Real-time Object Detection},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.2751},
	doi = {10.1.1.23.2751},
	journal = {International Journal of Computer Vision},
	author = {Paul Viola and Michael Jones},
	year = {2001}
},

@inproceedings{bhattacharya_evaluation_2008,
	title = {An evaluation of the Tight Optical Integration {(TOI)} algorithm sensitivity to inertial and camera errors},
	doi = {10.1109/PLANS.2008.4570061},
	abstract = {The objective of this paper is to demonstrate the sensitivity of the {GPS-camera} tight optical integration {(TOI)} algorithm to errors stemming from two sources; (a) inertial measurement errors and, (b) camera measurement errors. The {TOI} algorithm has been described by the authors in a recent paper, {"An} algorithm for {GPS} tight optical integration {(TOI)".} In the {TOI} algorithm, the inertial measurements are primarily used to derive attitude information to transform the camera measurements from the body frame to the navigation frame of reference. Clearly, errors on these inertial measurements contribute directly to the angular information from the camera used for a final position solution. Also, an initial position estimate is required to transform the measurements from the navigation frame of reference to an earth reference frame. As a result, an error in the initial position estimate will introduce some error in the position solution, which is included as an equivalent inertial error in the overall error analysis. The function of the camera is to make angular measurements from a pre-defined "marker". The errors in these measurements are sensitive to the range of the marker from the camera, i.e. marker range. Although the marker range is not a measurement used in the algorithm, it is necessary to investigate the sensitivity of the algorithm to variations in the marker range. It is shown that the sensitivity of the error in the final position estimate increases as this marker range increases. Moreover, the camera needs to identify the marker and hence, an error in the pixel selection in the image or a marker position error will affect the accuracy of the final position estimates. The sensitivity of the algorithm to these errors contributed by the camera is also analyzed. The {TOI} algorithm performance is shown to be similar to {GPS} performance levels, in the presence of the error sources discussed above. This makes it a viable solution for navigation capabilities in environ- - ments where sufficient satellites for a {GPS} only solution are not available, e.g. an urban canyon.},
	author = {S. Bhattacharya and T. Arthur and M. Uijt de Haag and Z. Zhu and K. Scheff},
	year = {2008},
	keywords = {angular measurement, angular measurements, camera measurement errors, inertial measurement errors, measurement errors, navigation, position measurement, tight optical integration},
	pages = {533--540}
},

@article{yacoob_detection_2006,
	title = {Detection and analysis of hair},
	volume = {28},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2006.139},
	abstract = {We develop computational models for measuring hair appearance for comparing different people. The models and methods developed have applications to person recognition and image indexing. An automatic hair detection algorithm is described and results reported. A multidimensional representation of hair appearance is presented and computational algorithms are described. Results on a data set of 524 subjects are reported. Identification of people using hair attributes is compared to eigenface-based recognition along with a joint, eigenface-hair-based identification.},
	number = {7},
	journal = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {Y. Yacoob and {L.S.} Davis},
	year = {2006},
	keywords = {automatic hair detection, eigenfaces, face recognition, feature extraction, hair analysis, hair appearance, hair detection., Human identification, image indexing, image representation, multidimensional representation, object detection, person recognition},
	pages = {1164--1169}
},

@inproceedings{arthur_demonstration_2008,
	title = {Demonstration of Tight Optical Integration {(TOI)} algorithm using field data},
	doi = {10.1109/PLANS.2008.4570065},
	abstract = {Recently a biologically inspired algorithm, called tight optical integration {(TOI),} was developed for tightly integrating optical sensor with {GPS.} The algorithm involves the integration of a standard camera along with {GPS} range (pseudorange or carrier phase) measurements to form position estimates. Initial simulations showed that {TOI} is capable of providing a position solution with an insufficient number of {GPS} satellites and a visible ldquomarkerrdquo at a known location, with an inertial unit to provide attitude information. This paper demonstrates how a marker is selected from a picture frame and tracked among consecutive frames. {TOI} has the potential to navigate with one known marker and two or three {GPS} satellites. In this work attitude information is derived from the {GPS} velocity estimates assuming zero roll for a terrestrial vehicle. Additionally, the same {TOI} algorithm can auto-locate unknown features when the position of the marker is not available, and navigate by these features when location is lost. The {TOI} algorithm is unique because it relies only on {GPS} range measurements and the pixel data from a camera. No ranging sources such as radar or {LIDAR} are required. It has particular application to scenarios involving a reduced constellation; such a reduced constellation may be due either to an urban canyon or a denied signal environment.},
	author = {T. Arthur and Z. Zhu and S. Bhattacharya and K. Johnson and K. Scheff},
	year = {2008},
	keywords = {attitude information, autolocate unknown features, biologically inspired algorithm, cameras, Global Positioning System, {GPS} satellites, optical sensor, optical sensors, tight optical integration algorithm},
	pages = {744--751}
},

@inproceedings{gallagher_clothing_2008,
	title = {Clothing cosegmentation for recognizing people},
	isbn = {1063-6919},
	doi = {10.1109/CVPR.2008.4587481},
	abstract = {Researches have verified that clothing provides information about the identity of the individual. To extract features from the clothing, the clothing region first must be localized or segmented in the image. At the same time, given multiple images of the same person wearing the same clothing, we expect to improve the effectiveness of clothing segmentation. Therefore, the identity recognition and clothing segmentation problems are inter-twined; a good solution for one aides in the solution for the other. We build on this idea by analyzing the mutual information between pixel locations near the face and the identity of the person to learn a global clothing mask. We segment the clothing region in each image using graph cuts based on a clothing model learned from one or multiple images believed to be the same person wearing the same clothing. We use facial features and clothing features to recognize individuals in other images. The results show that clothing segmentation provides a significant improvement in recognition accuracy for large image collections, and useful clothing masks are simultaneously produced. A further significant contribution is that we introduce a publicly available consumer image collection where each individual is identified. We hope this dataset allows the vision community to more easily compare results for tasks related to recognizing people in consumer image collections.},
	author = {{A.C.} Gallagher and Tsuhan Chen},
	year = {2008},
	keywords = {clothing cosegmentation, consumer image collections, face recognition, feature extraction, global clothing mask, image resolution, image segmentation, people recognition, pixel locations},
	pages = {1--8}
},

@inproceedings{ram_people_1998,
	title = {The people sensor: a mobility aid for the visually impaired},
	shorttitle = {The people sensor},
	doi = {10.1109/ISWC.1998.729548},
	abstract = {Electronic Travel Aids, which transform visual environmental cues into another sensory modality, have been proven to help visually impaired people travel with a greater degree of psychological comfort and independence. The People Sensor is an Electronic Travel Aid designed to address two issues of importance to visually impaired people: inadvertent cane contact with other pedestrians and objects, and speaking to a person who is no longer within hearing range. The device uses pyroelectric and ultrasound sensors to locate and differentiate between animate (human) and inanimate (non-human) obstructions in the detection path. The distance between the user and the obstruction along with the nature of the obstruction (human or non-human) is transmitted via modulated vibrotactile feedback. Armed with advance knowledge of the presence and location of objects and people in the environment, users of The People Sensor can travel with increased independence, safety and confidence},
	author = {S. Ram and J. Sharf},
	year = {1998},
	keywords = {Electronic Travel Aid, handicapped aids, modulated vibrotactile feedback, People Sensor, pyroelectric detectors, visual environmental cues, visually impaired people},
	pages = {166--167}
},

@article{radke_image_2005,
	title = {Image change detection algorithms: a systematic survey},
	volume = {14},
	issn = {1057-7149},
	shorttitle = {Image change detection algorithms},
	doi = {10.1109/TIP.2004.838698},
	abstract = {Detecting regions of change in multiple images of the same scene taken at different times is of widespread interest due to a large number of applications in diverse disciplines, including remote sensing, surveillance, medical diagnosis and treatment, civil infrastructure, and underwater sensing. This paper presents a systematic survey of the common processing steps and core decision rules in modern change detection algorithms, including significance and hypothesis testing, predictive models, the shading model, and background modeling. We also discuss important preprocessing methods, approaches to enforcing the consistency of the change mask, and principles for evaluating and comparing the performance of change detection algorithms. It is hoped that our classification of algorithms into a relatively small number of categories will provide useful guidance to the algorithm designer.},
	number = {3},
	journal = {Image Processing, {IEEE} Transactions on},
	author = {{R.J.} Radke and S. Andra and O. {Al-Kofahi} and B. Roysam},
	year = {2005},
	keywords = {background modeling, change detection, change mask, hypothesis testing, illumination invariance, image change detection algorithm, image classification, mixture models, predictive model, predictive models, shading model, significance testing, systematic survey},
	pages = {294--307}
},

@inproceedings{usman_saeed_multimedia_2006,
	title = {Multimedia Signal Processing, 2006 {IEEE} 8th Workshop on},
	doi = {10.1109/MMSP.2006.285262},
	abstract = {Face is considered as an attractive biometric but because of multiple sources of variabilities, the associated recognition rate is not high enough, when working on appearance only, for most of real applications. Considering that most available visual data are videos and not still images, we investigate in this article the possible contribution of some dynamic parameters (head displacements and mouth motion) in person recognition. Some preliminary results tend to validate this original proposal that opens some new perspectives in the possible design of future hybrid and efficient system combining appearance and dynamics of faces},
	author = {Usman Saeed and Federico Matta and {Jean-Luc} Dugelay},
	year = {2006},
	keywords = {biometric, biometrics (access control), dynamic parameters person recognition, face recognition, head-mouth dynamics},
	pages = {29--32}
},

@article{yun_zhai_video_2006,
	title = {Video scene segmentation using Markov chain Monte Carlo},
	volume = {8},
	issn = {1520-9210},
	abstract = {Videos are composed of many shots that are caused by different camera operations, e.g., on/off operations and switching between cameras. One important goal in video analysis is to group the shots into temporal scenes, such that all the shots in a single scene are related to the same subject, which could be a particular physical setting, an ongoing action or a theme. In this paper, we present a general framework for temporal scene segmentation in various video domains. The proposed method is formulated in a statistical fashion and uses the Markov chain Monte Carlo {(MCMC)} technique to determine the boundaries between video scenes. In this approach, a set of arbitrary scene boundaries are initialized at random locations and are automatically updated using two types of updates: diffusion and jumps. Diffusion is the process of updating the boundaries between adjacent scenes. Jumps consist of two reversible operations: the merging of two scenes and the splitting of an existing scene. The posterior probability of the target distribution of the number of scenes and their corresponding boundary locations is computed based on the model priors and the data likelihood. The updates of the model parameters are controlled by the hypothesis ratio test in the {MCMC} process, and the samples are collected to generate the final scene boundaries. The major advantage of the proposed framework is two-fold: 1) it is able to find the weak boundaries as well as the strong boundaries, i.e., it does not rely on the fixed threshold; 2) it can be applied to different video domains. We have tested the proposed method on two video domains: home videos and feature films, and accurate results have been obtained.},
	number = {4},
	journal = {Multimedia, {IEEE} Transactions on},
	author = {Yun Zhai and M. Shah},
	year = {2006},
	keywords = {diffusion process, image segmentation, jumps process, Markov chain Monte Carlo, Markov processes, Monte Carlo methods, posterior probability, target distribution, video analysis, video scene segmentation, video signal processing},
	pages = {686--697}
}?
@article{jianbo_shi_normalized_2000,
	title = {Normalized cuts and image segmentation},
	volume = {22},
	issn = {0162-8828},
	abstract = {We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging},
	number = {8},
	journal = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {Jianbo Shi and J. Malik},
	year = {2000},
	keywords = {computer vision, dissimilarity, eigenvalues, eigenvalues and eigenfunctions, graph partitioning, graph theory, image segmentation, image sequences, normalized cut, perceptual grouping, similarity},
	pages = {888--905}
},

@inproceedings{weiss_computer_1999,
	title = {Computer Vision, 1999. The Proceedings of the Seventh {IEEE} International Conference on},
	volume = {2},
	shorttitle = {Segmentation using eigenvectors},
	doi = {10.1109/ICCV.1999.790354},
	abstract = {Automatic grouping and segmentation of images remains a challenging problem in computer vision. Recently, a number of authors have demonstrated good performance on this task using methods that are based on eigenvectors of the affinity matrix. These approaches are extremely attractive in that they are based on simple eigendecomposition algorithms whose stability is well understood. Nevertheless, the use of eigendecompositions in the context of segmentation is far from well understood. In this paper we give a unified treatment of these algorithms, and show the close connections between them while highlighting their distinguishing features. We then prove results on eigenvectors of block matrices that allow us to analyze the performance of these algorithms in simple grouping settings. Finally, we use our analysis to motivate a variation on the existing methods that combines aspects from different eigenvector segmentation algorithms. We illustrate our analysis with results on real and synthetic images},
	author = {Y. Weiss},
	year = {1999},
	keywords = {affinity matrix, algorithm performance, automatic image grouping, automatic image segmentation, block matrices, computer vision, eigendecomposition algorithms, eigenvalues and eigenfunctions, eigenvectors, image segmentation, matrix algebra, real images, stability, synthetic images},
	pages = {975--982 vol.2}
},

@inproceedings{bhat_computer_2006,
	title = {Computer Vision and Pattern Recognition, 2006 {IEEE} Computer Society Conference on},
	volume = {2},
	isbn = {1063-6919},
	doi = {10.1109/CVPR.2006.225},
	abstract = {We present a technique for computing a dense pixel correspondence between two images of a scene containing multiple large, rigid motions. We model each motion with either a homography (for planar objects) or a fundamental matrix. The various motions in the scene are first extracted by clustering an initial sparse set of correspondences between feature points; we then perform a multi-label graph cut optimization which assigns each pixel to an independent motion and computes its disparity with respect to that motion. We demonstrate our technique on several example scenes and compare our results with previous approaches.},
	author = {P. Bhat and {K.C.} Zheng and N. Snavely and A. Agarwala and M. Agrawala and {M.F.} Cohen and B. Curless},
	year = {2006},
	pages = {2491--2497}
},

@inproceedings{levin_computer_2007,
	title = {Computer Vision and Pattern Recognition, 2007. {CVPR} '07. {IEEE} Conference on},
	doi = {10.1109/CVPR.2007.383147},
	abstract = {We present spectral matting: a new approach to natural image matting that automatically computes a set of fundamental fuzzy matting components from the smallest eigenvectors of a suitably defined Laplacian matrix. Thus, our approach extends spectral segmentation techniques, whose goal is to extract hard segments, to the extraction of soft matting components. These components may then be used as building blocks to easily construct semantically meaningful foreground mattes, either in an unsupervised fashion, or based on a small amount of user input.},
	author = {A. Levin and A. {Rav-Acha} and D. Lischinski},
	year = {2007},
	keywords = {eigenvalues and eigenfunctions, eigenvector, feature extraction, fuzzy matting component, image segmentation, Laplace equations, Laplacian matrix, matrix algebra, natural image matting, soft matting component, spectral matting, spectral segmentation technique},
	pages = {1--8}
},

@inproceedings{hu_image_2006,
	title = {Image Processing, 2006 {IEEE} International Conference on},
	isbn = {1522-4880},
	doi = {10.1109/ICIP.2006.312411},
	abstract = {In this paper a novel local blur estimation method is presented. The focal blur process is usually modeled as a Gaussian low-pass filtering and then the problem of blur estimation is to identify the Gaussian blur kernel. In the proposed method, the blurred input image is first re-blurred by Gaussian blur kernels with different blur radii. Then the difference ratios between the multiple re-blurred images and the input image are used to determine the unknown blur radius. We show that the proposed method does not require edge detection preprocessing and can estimate a wide range of blur radius. Experimental results of the proposed method on both synthetic and natural images and a comparison with a state-of-the-art method are presented},
	author = {H. Hu and G. de Haan},
	year = {2006},
	keywords = {Estimation, Gaussian blur kernel, Gaussian processes, Image edge analysis, image restoration, low-pass filtering, low-pass filters, Optical transfer functions, robust blur estimator},
	pages = {617--620}
},

@inproceedings{sunghyun_cho_computer_2007,
	title = {Computer Vision, 2007. {ICCV} 2007. {IEEE} 11th International Conference on},
	isbn = {1550-5499},
	doi = {10.1109/ICCV.2007.4408904},
	abstract = {We propose a method for removing non-uniform motion blur from multiple blurry images. Traditional methods focus on estimating a single motion blur kernel for the entire image. In contrast, we aim to restore images blurred by unknown, spatially varying motion blur kernels caused by different relative motions between the camera and the scene. Our algorithm simultaneously estimates multiple motions, motion blur kernels, and the associated image segments. We formulate the problem as a regularized energy function and solve it using an alternating optimization technique. Real- world experiments demonstrate the effectiveness of the proposed method.},
	author = {Sunghyun Cho and Y. Matsushita and Seungyong Lee},
	year = {2007},
	keywords = {associated image segments, image motion analysis, image restoration, image segmentation, images blurred restoration, motion blur kernel, multiple blurry images, nonuniform motion blur, regularized energy function, spatially varying motion blur kernels},
	pages = {1--8}
},

@article{elder_local_1998,
	title = {Local scale control for edge detection and blur estimation},
	volume = {20},
	issn = {0162-8828},
	doi = {10.1109/34.689301},
	abstract = {We show that knowledge of sensor properties and operator norms can be exploited to define a unique, locally computable minimum reliable scale for local estimation at each point in the image. This method for local scale control is applied to the problem of detecting and localizing edges in images with shallow depth of field and shadows. We show that edges spanning a broad range of blur scales and contrasts can be recovered accurately by a single system with no input parameters other than the second moment of the sensor noise. A natural dividend of this approach is a measure of the thickness of contours which can be used to estimate focal and penumbral blur. Local scale control is shown to be important for the estimation of blur in complex images, where the potential for interference between nearby edges of very different blur scale requires that estimates be made at the minimum reliable scale},
	number = {7},
	journal = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {{J.H.} Elder and {S.W.} Zucker},
	year = {1998},
	keywords = {blur estimation, blur scales, computer vision, contrasts, defocusing, edge detection, focal blur, image sensors, local scale control, localisation, optical focusing, penumbral blur, scale space, shadows},
	pages = {699--716}
}?
@article{kendon_functions_1967,
	title = {Some functions of gaze-direction in social interaction.},
	volume = {26},
	issn = {0001-6918},
	url = {http://view.ncbi.nlm.nih.gov/pubmed/6043092},
	number = {1},
	journal = {Acta Psychol {(Amst)}},
	author = {A Kendon},
	year = {1967},
	keywords = {dyad-interaction, experimental, gaze, overview},
	pages = {63, 22}
},

@book{argyle_gaze_1976,
	title = {Gaze \& Mutual Gaze},
	isbn = {0521208653},
	publisher = {Cambridge University Press},
	author = {Michael Argyle and Mark Cook},
	month = jan,
	year = {1976}
},

@article{kleinke_gaze_1986,
	title = {Gaze and eye contact: a research review},
	volume = {100},
	issn = {0033-2909},
	shorttitle = {Gaze and eye contact},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/3526377},
	number = {1},
	journal = {Psychological Bulletin},
	author = {C L Kleinke},
	month = jul,
	year = {1986},
	note = {{PMID:} 3526377},
	keywords = {Adolescent, Adult, Age Factors, Attention, Child, Child, Preschool, cognition, Communication, Culture, Emotions, Eye Movements, Female, Fixation, Ocular, Humans, Infant, Interpersonal Relations, Male, Mental Disorders, Personality, Sex Factors, Social Control, Formal, Social Dominance, Social Perception},
	pages = {78--100}
},

@article{bavelas_listener_2002,
	title = {Listener Responses as a Collaborative Process: The Role of Gaze},
	volume = {52},
	shorttitle = {Listener Responses as a Collaborative Process},
	url = {http://dx.doi.org/10.1111/j.1460-2466.2002.tb02562.x},
	doi = {10.1111/j.1460-2466.2002.tb02562.x},
	abstract = {The authors examined precisely when and how listeners insert their responses into a speaker's narrative. A collaborative theory would predict a relationship between the speaker's acts and the listener's responses, and the authors proposed that speaker gaze coordinated this collaboration. The listener typically looks more at the speaker than the reverse, but at key points while speaking the speaker seeks a response by looking at the listener, creating a brief period of mutual gaze called here a gaze window. The listener was very likely to respond with "mhm," a nod, or other reaction during this period, after which the speaker quickly looked away and continued speaking. This model was tested with 9 dyads in which 1 person was telling a close-call story to the other. The results confirmed the model for each dyad, demonstrating both collaboration in dialogue at the microlevel and a high degree of integration and coordination of audible and visible acts, in this case, speech and gaze.},
	number = {3},
	journal = {The Journal of Communication},
	author = {Janet Beavin Bavelas and Linda Coates and Trudy Johnson},
	year = {2002},
	pages = {566--580}
},

@article{mast_dominance_2002,
	title = {Dominance as Expressed and Inferred Through Speaking Time},
	volume = {28},
	url = {http://dx.doi.org/10.1111/j.1468-2958.2002.tb00814.x},
	doi = {10.1111/j.1468-2958.2002.tb00814.x},
	abstract = {Differences in speaking time during a group interaction are hypothesized to reflect differences in individual dominance. In order to test this assumption and to identify potential moderator variables influencing the strength of the predicted association, a meta-analysis was conducted. Whether speaking time is used to convey dominance to the same extent that it is used in inferring dominance was tested by contrasting studies concerned with dominance expressed in speaking time with studies of inferred dominance based on speaking time. Overall, and for the investigated subcategories of studies, the relationship between dominance and speaking time was significant. The strength of the associations, however, differed considerably due to the influence of moderator variables. The results showed that inferred dominance studies showed stronger associations between speaking time and dominance as opposed to the expressed dominance studies. Additionally, if dominance was expressed due to dominance-role assignments, the association between speaking time and dominance was stronger than if individuals with different levels of trait dominance interacted. For men, the association between speaking time and dominance was stronger than for women, and same-gender groups showed stronger associations than opposite-gender groups. Also, increasing group size intensified the strength of the association linearly.},
	number = {3},
	journal = {Human Communication Research},
	author = {Marianne Schmid Mast},
	year = {2002},
	pages = {420--450}
},

@article{van_dulmen_shifts_1997,
	title = {Shifts in {Doctor-Patient} Communication during a Series of Outpatient Consultations in {Non-Insulin-Dependent} Diabetes Mellitus.},
	volume = {30},
	issn = {{ISSN-0738-3991}},
	number = {3},
	journal = {Patient Education and Counseling},
	author = {Alexandra M. van Dulmen and Peter F. M. Verhaak and Henk J. G. Bilo},
	year = {1997},
	pages = {227--37},
	annote = {The first three consultations between 18 new patients with poorly controlled non-insulin-dependent diabetes mellitus and their medical specialist were videotaped. Changes in doctors' and patients' verbal and nonverbal communication behaviors were analyzed, and those most strongly related to patient satisfaction were distinguished. Discussion includes a pattern for an effective doctor-patient relationship. {(Author/EMK)}}
},

@article{glenberg_averting_1998,
	title = {Averting the gaze disengages the environment and facilitates remembering},
	volume = {26},
	issn = {{0090-502X}},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/9701957},
	abstract = {When people are asked moderately difficult questions, they often avert their gazes. We report five experiments in which we documented this phenomenon. They demonstrate that (1) the frequency of gaze aversion is related to the difficulty of cognitive processing, (2) this behavior cannot be due solely to demand characteristics or embarrassment, and (3) the behavior is functional: Averting the gaze improves performance. We speculate that averting the gaze helps people to disengage from environmental stimulation and thereby enhances the efficiency of cognitive processing directed by nonenvironmental stimulation.},
	number = {4},
	journal = {Memory \& Cognition},
	author = {A M Glenberg and J L Schroeder and D A Robertson},
	month = jul,
	year = {1998},
	note = {{PMID:} 9701957},
	keywords = {Adult, Analysis of Variance, Attention, Eye Movements, Humans, Linear Models, Memory, Vision, Ocular},
	pages = {651--658}
}?
@article{carver_development_2002,
	title = {Development and neural bases of face recognition in autism},
	volume = {7},
	issn = {13594184},
	url = {http://www.nature.com/mp/journal/v7/n2s/abs/4001168a.html},
	doi = {10.1038/sj.mp.4001168},
	number = {s2},
	journal = {Molecular Psychiatry},
	author = {L J Carver and G Dawson},
	year = {2002},
	pages = {S18--S20}
},

@article{rinn_neuropsychology_1984,
	title = {The neuropsychology of facial expression: A review of neurological and psychological mechanisms for producing facial expressions},
	volume = {95},
	journal = {Psychological Bulletin},
	author = {W. E. Rinn},
	year = {1984},
	pages = {52--77}
},

@book{ekman_facial_1978,
	title = {Facial Action Coding System: A Technique for the Measurement of Facial Movement.},
	shorttitle = {Facial Action Coding System},
	publisher = {Consulting Psychologists Press},
	author = {P Ekman and W Friesen},
	year = {1978},
	keywords = {emotion, expression, Face, facs, head}
},

@book{izard_maximally_1983,
	edition = {Revised},
	title = {The maximally discriminative facial movement coding system},
	publisher = {Instructional Resources Center, University of Delaware},
	author = {Carroll E Izard},
	year = {1983}
},

@article{zhihong_zeng_survey_2009,
	title = {A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions},
	volume = {31},
	issn = {0162-8828},
	shorttitle = {A Survey of Affect Recognition Methods},
	doi = {10.1109/TPAMI.2008.52},
	abstract = {Automated analysis of human affective behavior has attracted increasing attention from researchers in psychology, computer science, linguistics, neuroscience, and related disciplines. However, the existing methods typically handle only deliberately displayed and exaggerated expressions of prototypical emotions despite the fact that deliberate behaviour differs in visual appearance, audio profile, and timing from spontaneously occurring behaviour. To address this problem, efforts to develop algorithms that can process naturally occurring human affective behaviour have recently emerged. Moreover, an increasing number of efforts are reported toward multimodal fusion for human affect analysis including audiovisual fusion, linguistic and paralinguistic fusion, and multi-cue visual fusion based on facial expressions, head movements, and body gestures. This paper introduces and surveys these recent advances. We first discuss human emotion perception from a psychological perspective. Next we examine available approaches to solving the problem of machine understanding of human affective behavior, and discuss important issues like the collection and availability of training and test data. We finally outline some of the scientific and engineering challenges to advancing human affect sensing technology.},
	number = {1},
	journal = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {Zhihong Zeng and M. Pantic and {G.I.} Roisman and {T.S.} Huang},
	year = {2009},
	keywords = {affect recognition methods, audio expressions, behavioural sciences, emotion recognition, Evaluation/methodology, human affect analysis, human affective behavior, human emotion perception, human factors, human-centered computing, Introductory and Survey, multimodal fusion, psychology, spontaneous expressions, visual expressions},
	pages = {39--58}
},

@inproceedings{martin_real-time_2008,
	title = {A real-time facial expression recognition system based on Active Appearance Models using gray images and edge images},
	abstract = {In this paper, we present an approach for facial expression classification, based on Active Appearance Models. To be able to work in real-world, we applied the {AAM} framework on edge images, instead of gray images. This yields to more robustness against varying lighting conditions. Additionally, three different facial expression classifiers {(AAM} classifier set, {MLP} and {SVM)} are compared with each other. An essential advantage of the developed system is, that it is able to work in real-time - a prerequisite for the envisaged implementation on an interactive social robot. The real-time capability was achieved by a two-stage hierarchical {AAM} tracker and a very efficient implementation.},
	author = {C. Martin and U. Werner and {H.-M.} Gross},
	year = {2008},
	keywords = {{AAM} classifier set, active appearance models, edge images, face recognition, facial expression recognition system, gray images, interactive social robot, {MLP,} robot vision, support vector machines, {SVM}},
	pages = {1--6}
},

@inproceedings{mayer_real_2008,
	title = {A real time system for model-based interpretation of the dynamics of facial expressions},
	abstract = {Our system runs at 10 fps on a 2.0 {GHz} processor and an image resolution of 640times480 pixels. High quality objective functions that are learned from annotated example images ensure both an accurate and fast computation of the model parameters. Our demonstrator for facial expression estimation has been presented at several events with political audience and on {TV.} However, the approach of robust face models fitting, forms the basis of various more applications such as gaze detection or gender estimation. The drawback of our approach is that the data base from which the objective function is learned needs to cover all aspects of face properties. If, for instance, the database did not contain images of bearded men the objective function will fail when confronted with such an image. Furthermore, the data base has to be manually annotated. Although no expert knowledge is required, this task requires a considerable amount of time. An online fitting demonstration is available.},
	author = {C. Mayer and M. Wimmer and F. Stulp and Z. Riaz and A. Roth and M. Eggers and B. Radig},
	year = {2008},
	keywords = {computer vision, emotion recognition, face model fitting, face property, face recognition, facial expression dynamics, facial expression estimation, gaze detection, gender estimation, image resolution, learning (artificial intelligence), model-based interpretation, objective function learning},
	pages = {1--2}
},

@inproceedings{orozco_confidence_2008,
	title = {Confidence assessment on eyelid and eyebrow expression recognition},
	doi = {10.1109/AFGR.2008.4813454},
	abstract = {In this paper, we address the recognition of subtle facial expressions by reasoning on the classification confidence. Psychological evidences have determined that eyelids and eyebrows are significant for the recognition of subtle facial expressions and the early perception of human emotions. This early perception results in a more complex problem, which requires a confidence assessment for any provided solution. Thus, traditional score-based classifiers (e.g. {k-NN} and {NN)} are not able to produce confident estimates. Instead, we first present five confidence estimators and a confidence classification assessment for {Case-Based} Reasoning {(CBR).} Second, we improve the expression retrieval from the database by learning the neighbourhood's dimensions for the expected classification confidences. Third, we reuse the previous classified expressions and the confidence assessment to improve the classification achieved by {k-NN.} Fourth, we improve the database for generalization with new subjects by learning thresholds to minimize misclassification with low confidence, maximize correct classifications with high confidence and re-arrange misclassification with high confidence. The proposed system represents an effective contribution for both subtle expression recognition and {CBR} methodology. It achieves an average recognition of 97\% plusmn 1\% with a confidence of 96\% plusmn 2\% for expressiveness between 20\% and 100\%.},
	booktitle = {Automatic Face \& Gesture Recognition, 2008. {FG} '08. 8th {IEEE} International Conference on},
	author = {J. Orozco and O. Rudovic and {F.X.} Roca and J. Gonzalez},
	year = {2008},
	keywords = {case-based reasoning, classification confidences, confidence assessment, confidence classification assessment, eyebrow expression recognition, eyelid, face recognition, human emotions, minimize misclassification, psychological evidences, score-based classifier, subtle expression recognition, subtle facial expression},
	pages = {1--8}
},

@inproceedings{littlewort_faces_2007,
	address = {Nagoya, Aichi, Japan},
	title = {Faces of pain: automated measurement of spontaneousallfacial expressions of genuine and posed pain},
	isbn = {978-1-59593-817-6},
	shorttitle = {Faces of pain},
	url = {http://portal.acm.org/citation.cfm?id=1322198&dl=GUIDE&coll=GUIDE&CFID=61778475&CFTOKEN=20406149},
	doi = {10.1145/1322192.1322198},
	abstract = {We present initial results from the application of an automated facial expression recognition system to spontaneous facial expressions of pain. In this study, 26 participants were videotaped under three experimental conditions: baseline, posed pain, and real pain. In the real pain condition, subjects experienced cold pressor pain by submerging their arm in ice water. Our goal was to automatically determine which experimental condition was shown in a 60 second clip from a previously unseen subject. We chose a machine learning approach, previously used successfully to categorize basic emotional facial expressions in posed datasets as well as to detect individual facial actions of the Facial Action Coding System {(FACS)} {(Littlewort} et al, 2006; Bartlett et al., 2006). For this study, we trained 20 Action Unit {(AU)} classifiers on over 5000 images selected from a combination of posed and spontaneous facial expressions. The output of the system was a real valued number indicating the distance to the separating hyperplane for each classifier. Applying this system to the pain video data produced a 20 channel output stream, consisting of one real value for each learned {AU,} for each frame of the video. This data was passed to a second layer of classifiers to predict the difference between baseline and pained faces, and the difference between expressions of real pain and fake pain. Naíve human subjects tested on the same videos were at chance for differentiating faked from real pain, obtaining only 52\% accuracy. The automated system was successfully able to differentiate faked from real pain. In an analysis of 26 subjects, the system obtained 72\% correct for subject independent discrimination of real versus fake pain on a 2-alternative forced choice. Moreover, the most discriminative facial action in the automated system output was {AU} 4 (brow lower), which all was consistent with findings using human expert {FACS} codes.},
	publisher = {{ACM}},
	author = {Gwen C. Littlewort and Marian Stewart Bartlett and Kang Lee},
	year = {2007},
	keywords = {computer vision, deception, facial action coding system, facial expression recognition, facs, machine learning, pain, spontaneous behavior},
	pages = {15--21}
},

@inproceedings{valstar_motion_2004,
	title = {Motion history for facial action detection in video},
	volume = {1},
	isbn = {{1062-922X}},
	doi = {10.1109/ICSMC.2004.1398371},
	abstract = {Enabling computer systems to recognize human facial expressions is a challenging research problem with many applications in behavioral science, medicine, security, and human-machine interaction. Instead of being another approach to automatic detection of prototypic facial expressions of emotion, this work attempts to analyze subtle changes in facial behavior by recognizing facial action units {(AU,} i.e. atomic facial signals) that produce expressions. This work proposes {AU} recognition based upon multilevel motion history images {(MMHI),} which can be seen as an extension to temporal templates introduced by Bobick and Davis. By recording motion history at multiple time Intervals (i.e., multilevel {MHI)} instead of recording it once for the entire image sequence, we overcome the problem of self-occlusion which is inherent to temporal templates original definition. For automatic classification of an input {MMHI-represented} face video in terms of 21 {AU} classes, two approaches are compared: a sparse network of Winnows {(SNoW)} and a standard {kNearest} neighbour {(kNN)} classifier. The system was tested on two different databases, the {MMI-Face-DB} developed by the authors and the {Cohn-Kanade} face database.},
	author = {M. Valstar and M. Pantic and I. Patras},
	year = {2004},
	keywords = {computer vision, emotion recognition, face recognition, facial action detection, human facial expressions recognition, image motion analysis, image sequence, image sequences, multilevel motion history images, sparse network of Winnows, standard {kNearest} neighbour classifier},
	pages = {635--640 vol.1}
},

@inproceedings{cohn_automatic_2004,
	title = {Automatic analysis and recognition of brow actions and head motion in spontaneous facial behavior},
	volume = {1},
	isbn = {{1062-922X}},
	doi = {10.1109/ICSMC.2004.1398367},
	abstract = {Previous efforts in automatic facial expression recognition have been limited to posed facial behavior under well-controlled conditions (e.g., frontal pose and minimal out-of-plane head motion). The {CMU/Pitt} automated facial image analysis system {(AFA)} accommodates varied pose, moderate out-of-plane head motion, and occlusion. {AFA} was tested in video of two-person interviews originally collected to answer substantive questions in psychology, and represent a substantial challenge to automatic recognition of facial expression. This report focuses on two action units, brow raising and brow lowering because of their importance to emotion expression and paralinguistic communication. For two-state recognition, {AFA} achieved 89\% accuracy. For three-state recognition (brow raising, brow lowering, and no brow action), accuracy was 76\%. Brow and head motion were temporally coordinated. These findings demonstrate the feasibility of action unit recognition in spontaneous facial behavior.},
	author = {{J.F.} Cohn and {L.I.} Reed and Z. Ambadar and Jing Xiao and T. Moriyama},
	year = {2004},
	keywords = {brow actions recognition, emotion recognition, face recognition, facial expression recognition, head motion, multimodal coordination, spontaneous facial behavior},
	pages = {610--616 vol.1}
},

@inproceedings{zhen_wen_capturing_2003,
	title = {Capturing subtle facial motions in {3D} face tracking},
	doi = {10.1109/ICCV.2003.1238646},
	abstract = {Facial motions produce not only facial feature points motions, but also subtle appearance changes such as wrinkles and shading changes. These subtle changes are important yet difficult issues for both analysis (tracking) and synthesis (animation). Previous approaches were mostly based on models learned from extensive training appearance examples. However, the space of all possible facial motion appearance is huge. Thus, it is not feasible to collect samples covering all possible variations due to lighting conditions, individualities, and head poses. Therefore, it is difficult to adapt such models to new conditions. In this paper, we present an adaptive technique for analyzing subtle facial appearance changes. We propose a new ratio-image based appearance feature, which is independent of a person's face albedo. This feature is used to track face appearance variations based on exemplars. To adapt the exemplar appearance model to new people and lighting conditions, we develop an online {EM-based} algorithm. Experiments show that the proposed method improves classification results in a facial expression recognition task, where a variety of people and lighting conditions are involved.},
	author = {Zhen Wen and {T.S.} Huang},
	year = {2003},
	keywords = {{3D} face tracking, adaptive technique, animation, appearance feature, computer vision, exemplar appearance model, face albedo, face recognition, Facial expression, facial feature point motions, facial motion appearance, feature extraction, gesture recognition, head poses, image analysis, image classification, image colour analysis, image recognition, image texture, image tracking, lighting conditions, motion estimation, online {EM-based} algorithm, shading changes, subtle facial motions, wrinkles},
	pages = {1343--1350 vol.2}
},

@inproceedings{kapoor_multimodal_2005,
	address = {Hilton, Singapore},
	title = {Multimodal affect recognition in learning environments},
	isbn = {1-59593-044-2},
	url = {http://portal.acm.org/citation.cfm?id=1101300},
	doi = {10.1145/1101149.1101300},
	abstract = {We propose a multi-sensor affect recognition system and evaluate it on the challenging task of classifying interest (or disinterest) in children trying to solve an educational puzzle on the computer. The multimodal sensory information from facial expressions and postural shifts of the learner is combined with information about the learner's activity on the computer. We propose a unified approach, based on a mixture of Gaussian Processes, for achieving sensor fusion under the problematic conditions of missing channels and noisy labels. This approach generates separate class labels corresponding to each individual modality. The final classification is based upon a hidden random variable, which probabilistically combines the sensors. The multimodal Gaussian Process approach achieves accuracy of over 86\%, significantly outperforming classification using the individual modalities, and several other combination schemes.},
	publisher = {{ACM}},
	author = {Ashish Kapoor and Rosalind W. Picard},
	year = {2005},
	pages = {677--682}
},

@inproceedings{pantic_case-based_2004,
	title = {Case-based reasoning for user-profiled recognition of emotions from face images},
	volume = {1},
	doi = {10.1109/ICME.2004.1394211},
	abstract = {To allow for rich and sometimes subtle shadings of emotion that humans recognize in a facial expression, user-profiled recognition of emotions from images of faces is needed. In this work, we introduce a case-based reasoning system capable of classifying facial expressions (given in terms of facial muscle actions) into the emotion categories learned from the user. The utilized case base is a dynamic, incrementally self-organizing-event-content-addressable memory that allows fact retrieval and evaluation of encountered events, based upon the user preferences and the generalizations formed from prior input. Two versions of a prototype system are presented: one aims at recognition of six "universal" emotions and the other aims at recognition of affective states learned from the user. Validation studies suggest that in 100\% and in 97\% of the test cases, respectively, interpretations produced by the system are consistent with those of the two users who trained the two versions of the prototype system.},
	author = {M. Pantic and L. Rothkrantz},
	year = {2004},
	keywords = {affective computing, automatic facial expression analysis, case-based reasoning, case-based reasoning system, content-addressable storage, dynamic {CAM,} emotion categories, emotion recognition, facial image emotion recognition, facial muscle actions classification, incrementally self-organizing {CAM,} self-organising storage, self-organizing-event-content-addressable memory, universal emotions, user learned affective states, user-profiled emotion recognition},
	pages = {391--394 Vol.1}
},

@inproceedings{bartlett_recognizing_2005,
	title = {Recognizing facial expression: machine learning and application to spontaneous behavior},
	volume = {2},
	isbn = {1063-6919},
	shorttitle = {Recognizing facial expression},
	doi = {10.1109/CVPR.2005.297},
	abstract = {We present a systematic comparison of machine learning methods applied to the problem of fully automatic recognition of facial expressions. We report results on a series of experiments comparing recognition engines, including {AdaBoost,} support vector machines, linear discriminant analysis. We also explored feature selection techniques, including the use of {AdaBoost} for feature selection prior to classification by {SVM} or {LDA.} Best results were obtained by selecting a subset of Gabor filters using {AdaBoost} followed by classification with support vector machines. The system operates in real-time, and obtained 93\% correct generalization to novel subjects for a 7-way forced choice on the {Cohn-Kanade} expression dataset. The outputs of the classifiers change smoothly as a function of time and thus can be used to measure facial expression dynamics. We applied the system to to fully automated recognition of facial actions {(FACS).} The present system classifies 17 action units, whether they occur singly or in combination with other actions, with a mean accuracy of 94.8\%. We present preliminary results for applying this system to spontaneous facial expressions.},
	author = {{M.S.} Bartlett and G. Littlewort and M. Frank and C. Lainscsek and I. Fasel and J. Movellan},
	year = {2005},
	keywords = {{AdaBoost,} {Cohn-Kanade} expression dataset, face recognition, facial expression recognition, feature extraction, feature selection techniques, Gabor filter, generalisation (artificial intelligence), gesture recognition, image classification, learning (artificial intelligence), linear discriminant analysis, machine learning method, support vector machines},
	pages = {568--573 vol. 2}
},

@article{borkenau_thin_2004,
	title = {Thin slices of behavior as cues of personality and intelligence.},
	volume = {86},
	issn = {0022-3514},
	url = {http://dx.doi.org/10.1037/0022-3514.86.4.599},
	abstract = {Self-reports, peer reports, intelligence tests, and ratings of personality and intelligence from 15 videotaped episodes were collected for 600 participants. The average cross-situational consistency of trait impressions across the 15 episodes was .43. Shared stereotypes related to gender and age were mostly accurate and contributed little to agreement among judges. Agreement was limited mainly by nonshared meaning systems and by nonoverlapping information. Personality inferences from thin slices of behavior were significantly associated with reports by knowledgeable informants. This association became stronger when more episodes were included, but gains in prediction were low beyond 6 episodes. Inferences of intelligence from thin slices of behavior strongly predicted intelligence test scores. A particularly strong single predictor was how persons read short sentences.},
	number = {4},
	journal = {Journal of personality and social psychology},
	author = {P. Borkenau and N. Mauer and R. Riemann and {F. M.} Spinath and A. Angleitner},
	year = {2004},
	keywords = {thin-slice},
	pages = {614, 599}
},

@article{pantic_dynamics_2006,
	title = {Dynamics of facial expression: recognition of facial actions and their temporal segments from face profile image sequences},
	volume = {36},
	issn = {1083-4419},
	shorttitle = {Dynamics of facial expression},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/16602602},
	abstract = {Automatic analysis of human facial expression is a challenging problem with many applications. Most of the existing automated systems for facial expression analysis attempt to recognize a few prototypic emotional expressions, such as anger and happiness. Instead of representing another approach to machine analysis of prototypic facial expressions of emotion, the method presented in this paper attempts to handle a large range of human facial behavior by recognizing facial muscle actions that produce expressions. Virtually all of the existing vision systems for facial muscle action detection deal only with frontal-view face images and cannot handle temporal dynamics of facial actions. In this paper, we present a system for automatic recognition of facial action units {(AUs)} and their temporal models from long, profile-view face image sequences. We exploit particle filtering to track 15 facial points in an input face-profile sequence, and we introduce facial-action-dynamics recognition from continuous video input using temporal rules. The algorithm performs both automatic segmentation of an input video into facial expressions pictured and recognition of temporal segments (i.e., onset, apex, offset) of 27 {AUs} occurring alone or in a combination in the input face-profile video. A recognition rate of 87\% is achieved.},
	number = {2},
	journal = {{IEEE} Transactions on Systems, Man, and Cybernetics. Part B, Cybernetics: A Publication of the {IEEE} Systems, Man, and Cybernetics Society},
	author = {Maja Pantic and Ioannis Patras},
	month = apr,
	year = {2006},
	note = {{PMID:} 16602602},
	keywords = {Algorithms, Artificial Intelligence, Cluster Analysis, Face, Facial expression, Humans, Image Enhancement, Image Interpretation, {Computer-Assisted,} Information Storage and Retrieval, Movement, Pattern Recognition, Automated, Photography, Reproducibility of Results, Sensitivity and Specificity, Subtraction Technique, Time Factors, Video Recording},
	pages = {433--449}
},

@article{qiang_ji_probabilistic_2006,
	title = {A probabilistic framework for modeling and real-time monitoring human fatigue},
	volume = {36},
	issn = {1083-4427},
	doi = {10.1109/TSMCA.2005.855922},
	abstract = {A probabilistic framework based on the Bayesian networks for modeling and real-time inferring human fatigue by integrating information from various sensory data and certain relevant contextual information is introduced. A static fatigue model that captures the static relationships between fatigue, significant factors that cause fatigue, and various sensory observations that typically result from fatigue is first presented. Such a model provides mathematically coherent and sound basis for systematically aggregating uncertain evidences from different sources, augmented with relevant contextual information. The static model, however, fails to capture the dynamic aspect of fatigue. Fatigue is a cognitive state that is developed over time. To account for the temporal aspect of human fatigue, the static fatigue model is extended based on dynamic Bayesian networks. The dynamic fatigue model allows to integrate fatigue evidences not only spatially but also temporally, therefore, leading to a more robust and accurate fatigue modeling and inference. A real-time nonintrusive fatigue monitor was built based on integrating the proposed fatigue model with a computer vision system developed for extracting various visual cues typically related to fatigue. Performance evaluation of the fatigue monitor using both synthetic and real data demonstrates the validity of the proposed fatigue model in both modeling and real-time inference of fatigue},
	number = {5},
	journal = {Systems, Man and Cybernetics, Part A: Systems and Humans, {IEEE} Transactions on},
	author = {Qiang Ji and P. Lan and C. Looney},
	year = {2006},
	keywords = {Bayes methods, Bayesian networks, Bayesian networks {(BNs),} behavioural sciences computing, cognition, computer vision, computer vision system, condition monitoring, contextual information, dynamic Bayesian networks {(DBNs),} human fatigue monitoring, information fusion, probabilistic framework, real-time human fatigue monitoring, static fatigue model, vigilance test},
	pages = {862--875}
},

@article{ashraf_painful_2009,
	title = {The painful face - Pain expression recognition using active appearance models},
	volume = {27},
	url = {http://portal.acm.org/citation.cfm?id=1621314},
	abstract = {Pain is typically assessed by patient self-report. Self-reported pain, however, is difficult to interpret and may be impaired or in some circumstances (i.e., young children and the severely ill) not even possible. To circumvent these problems behavioral scientists have identified reliable and valid facial indicators of pain. Hitherto, these methods have required manual measurement by highly skilled human observers. In this paper we explore an approach for automatically recognizing acute pain without the need for human observers. Specifically, our study was restricted to automatically detecting pain in adult patients with rotator cuff injuries. The system employed video input of the patients as they moved their affected and unaffected shoulder. Two types of ground truth were considered. Sequence-level ground truth consisted of Likert-type ratings by skilled observers. Frame-level ground truth was calculated from presence/absence and intensity of facial actions previously associated with pain. Active appearance models {(AAM)} were used to decouple shape and appearance in the digitized face images. Support vector machines {(SVM)} were compared for several representations from the {AAM} and of ground truth of varying granularity. We explored two questions pertinent to the construction, design and development of automatic pain detection systems. First, at what level (i.e., sequence- or frame-level) should datasets be labeled in order to obtain satisfactory automatic pain detection performance? Second, how important is it, at both levels of labeling, that we non-rigidly register the face?},
	number = {12},
	journal = {Image Vision Comput.},
	author = {Ahmed Bilal Ashraf and Simon Lucey and Jeffrey F. Cohn and Tsuhan Chen and Zara Ambadar and Kenneth M. Prkachin and Patricia E. Solomon},
	year = {2009},
	keywords = {active appearance models, automatic facial image analysis, Facial expression, facs, pain, support vector machines},
	pages = {1788--1796}
},

@inproceedings{sebe_authentic_2004,
	title = {Authentic facial expression analysis},
	doi = {10.1109/AFGR.2004.1301585},
	abstract = {It is argued that for the computer to be able to interact with humans, it needs to havve the communication skills o humans. One of these skills is the ability to understand the emotional state of the person. The most expressive way humans display emotions is through facial expressions. In most facial expression systems and databases, the emotion data was collected by asking the subjects to perform a series of facial expressions. However, these directed or deliberate facial action tasks typically differ in appearance and timing from the authentic facial expressions induced through events in the normal environment of the subject. In this paper, we present our effort in creating an authentic facial expression database based on spontaneous emotions derived from the environment. Furthermore, we test and compare a wide range of classifiers from the machine learning literature that can be used for facial expression classification.},
	author = {N. Sebe and {M.S.} Lew and I. Cohen and Yafei Sun and T. Gevers and {T.S.} Huang},
	year = {2004},
	keywords = {authentic facial expression database, emotion recognition, facial expression analysis, image classification, learning (artificial intelligence), machine learning literature, visual databases},
	pages = {517--522}
},

@article{yan_tong_facial_2007,
	title = {Facial Action Unit Recognition by Exploiting Their Dynamic and Semantic Relationships},
	volume = {29},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2007.1094},
	abstract = {A system that could automatically analyze the facial actions in real time has applications in a wide range of different fields. However, developing such a system is always challenging due to the richness, ambiguity, and dynamic nature of facial actions. Although a number of research groups attempt to recognize facial action units {(AUs)} by improving either the facial feature extraction techniques or the {AU} classification techniques, these methods often recognize {AUs} or certain {AU} combinations individually and statically, ignoring the semantic relationships among {AUs} and the dynamics of {AUs.} Hence, these approaches cannot always recognize {AUs} reliably, robustly, and consistently. In this paper, we propose a novel approach that systematically accounts for the relationships among {AUs} and their temporal evolutions for {AU} recognition. Specifically, we use a dynamic Bayesian network {(DBN)} to model the relationships among different {AUs.} The {DBN} provides a coherent and unified hierarchical probabilistic framework to represent probabilistic relationships among various {AUs} and to account for the temporal changes in facial action development. Within our system, robust computer vision techniques are used to obtain {AU} measurements. Such {AU} measurements are then applied as evidence to the {DBN} for inferring various {AUs.} The experiments show that the integration of {AU} relationships and {AU} dynamics with {AU} measurements yields significant improvement of {AU} recognition, especially for spontaneous facial expressions and under more realistic environment including illumination variation, face pose variation, and occlusion.},
	number = {10},
	journal = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {Yan Tong and Wenhui Liao and Qiang Ji},
	year = {2007},
	keywords = {{AU} classification techniques, Bayes methods, Bayesian networks, computer vision, dynamic Bayesian network, dynamic relationships, face pose variation, face recognition, facial action coding system, Facial Action Unit Recognition, facial expression analysis, facial expressions, facial feature extraction techniques, feature extraction, illumination variation, image classification, occlusion, semantic relationships, unified hierarchical probabilistic framework},
	pages = {1683--1699}
},

@article{ioannou_emotion_2005,
	title = {Emotion recognition through facial expression analysis based on a neurofuzzy network},
	volume = {18},
	url = {http://portal.acm.org/citation.cfm?id=1150563},
	abstract = {Extracting and validating emotional cues through analysis of users' facial expressions is of high importance for improving the level of interaction in man machine communication systems. Extraction of appropriate facial features and consequent recognition of the user's emotional state that can be robust to facial expression variations among different users is the topic of this paper. Facial animation parameters {(FAPs)} defined according to the {ISO} {MPEG-4} standard are extracted by a robust facial analysis system, accompanied by appropriate confidence measures of the estimation accuracy. A novel neurofuzzy system is then created, based on rules that have been defined through analysis of {FAP} variations both at the discrete emotional space, as well as in the {2D} continuous activation-evaluation one. The neurofuzzy system allows for further learning and adaptation to specific users' facial expression characteristics, measured though {FAP} estimation in real life application of the system, using analysis by clustering of the obtained {FAP} values. Experimental studies with emotionally expressive datasets, generated in the {EC} {IST} {ERMIS} project indicate the good performance and potential of the developed technologies.},
	number = {4},
	journal = {Neural Netw.},
	author = {Spiros V. Ioannou and Amaryllis T. Raouzaiou and Vasilis A. Tzouvaras and Theofilos P. Mailis and Kostas C. Karpouzis and Stefanos D. Kollias},
	year = {2005},
	keywords = {activation evaluation emotion representation, adaptation, facial expression analysis, mpeg-4 facial animation parameters, neurofuzzy network, rule extraction},
	pages = {423--435}
},

@article{yeasin_recognition_2006,
	title = {Recognition of facial expressions and measurement of levels of interest from video},
	volume = {8},
	issn = {1520-9210},
	doi = {10.1109/TMM.2006.870737},
	abstract = {This paper presents a spatio-temporal approach in recognizing six universal facial expressions from visual data and using them to compute levels of interest. The classification approach relies on a two-step strategy on the top of projected facial motion vectors obtained from video sequences of facial expressions. First a linear classification bank was applied on projected optical flow vectors and decisions made by the linear classifiers were coalesced to produce a characteristic signature for each universal facial expression. The signatures thus computed from the training data set were used to train discrete hidden Markov models {(HMMs)} to learn the underlying model for each facial expression. The performances of the proposed facial expressions recognition were computed using five fold cross-validation on {Cohn-Kanade} facial expressions database consisting of 488 video sequences that includes 97 subjects. The proposed approach achieved an average recognition rate of 90.9\% on {Cohn-Kanade} facial expressions database. Recognized facial expressions were mapped to levels of interest using the affect space and the intensity of motion around apex frame. Computed level of interest was subjectively analyzed and was found to be consistent with "ground truth" information in most of the cases. To further illustrate the efficacy of the proposed approach, and also to better understand the effects of a number of factors that are detrimental to the facial expression recognition, a number of experiments were conducted. The first empirical analysis was conducted on a database consisting of 108 facial expressions collected from {TV} broadcasts and labeled by human coders for subsequent analysis. The second experiment (emotion elicitation) was conducted on facial expressions obtained from 21 subjects by showing the subjects six different movies clips chosen in a manner to arouse spontaneous emotional reactions that would produce natural facial expressions.},
	number = {3},
	journal = {Multimedia, {IEEE} Transactions on},
	author = {M. Yeasin and B. Bullot and R. Sharma},
	year = {2006},
	keywords = {{Cohn-Kanade} facial expressions database, discrete hidden Markov model training, emotion recognition, Emotions, empirical analysis, face detection, face recognition, facial motion vector projection, hidden Markov models, hidden Markov models {(HMMs),} {HMM,} image motion analysis, image sequences, learning (artificial intelligence), levels of interest, levels of interest measurement, linear classification bank, machine learning, optical flow vector projection, pattern classification, spatio-temporal approach, universal facial expression recognition, universal facial expressions, video sequences, visual databases},
	pages = {500--508}
},

@inproceedings{valstar_how_2007,
	address = {Nagoya, Aichi, Japan},
	title = {How to distinguish posed from spontaneous smiles using geometric features},
	isbn = {978-1-59593-817-6},
	url = {http://portal.acm.org/citation.cfm?id=1322202},
	doi = {10.1145/1322192.1322202},
	abstract = {Automatic distinction between posed and spontaneous expressions is an unsolved problem. Previously cognitive sciences' studies indicated that the automatic separation of posed from spontaneous expressions is possible using the face modality alone. However, little is known about the information contained in head and shoulder motion. In this work, we propose to (i) distinguish between posed and spontaneous smiles by fusing the head, face, and shoulder modalities, (ii) investigate which modalities carry important information and how the information of the modalities relate to each other, and (iii) to which extent the temporal dynamics of these signals attribute to solving the problem. We use a cylindrical head tracker to track the head movements and two particle filtering techniques to track the facial and shoulder movements. Classification is performed by kernel methods combined with ensemble learning techniques. We investigated two aspects of multimodal fusion: the level of abstraction (i.e., early, mid-level, and late fusion) and the fusion rule used (i.e., sum, product and weight criteria). Experimental results from 100 videos displaying posed smiles and 102 videos displaying spontaneous smiles are presented. Best results were obtained with late fusion of all modalities when 94.0\% of the videos were classified correctly.},
	publisher = {{ACM}},
	author = {Michel F. Valstar and Hatice Gunes and Maja Pantic},
	year = {2007},
	keywords = {deception detection, human information processing, multimodal video processing},
	pages = {38--45}
},

@inproceedings{wang_facial_2003,
	title = {Facial Expression Decomposition},
	isbn = {0-7695-1950-4},
	url = {http://portal.acm.org/citation.cfm?id=946680},
	abstract = {In this paper, we propose a novel approach for facialexpression decomposition - {Higher-Order} Singular {ValueDecomposition} {(HOSVD),} a natural generalization ofmatrix {SVD.} We learn the expression subspace and personsubspace from a corpus of images showing seven basicfacial expressions, rather than resort to expert-coded facialexpression parameters as in [3]. We propose a simultaneousface and facial expression recognition algorithm,which can classify the given image into one of the sevenbasic facial expression categories, and then other facialexpressions of the new person can be synthesized using thelearned expression subspace model. The contributions ofthis work lie mainly in two aspects. First, we propose a {newHOSVD} based approach to model the mapping betweenpersons and expressions, used for facial expression synthesisfor a new person. Second, we realize simultaneous faceand facial expression recognition as a result of facialexpression decomposition. Experimental results are presentedthat illustrate the capability of the person subspaceand expression subspace in both synthesis and recognitiontasks. As a quantitative measure of the quality of synthesis,we propose using Gradient Minimum Square Error {(GMSE)which} measures the gradient difference between the originaland synthesized images.},
	publisher = {{IEEE} Computer Society},
	author = {Hongcheng Wang and Narendra Ahuja},
	year = {2003},
	pages = {958}
},

@inproceedings{fasel_latent_2004,
	address = {New York, {NY,} {USA}},
	title = {Latent semantic analysis of facial action codes for automatic facial expression recognition},
	isbn = {1-58113-940-3},
	url = {http://portal.acm.org/citation.cfm?id=1026742},
	doi = {10.1145/1026711.1026742},
	abstract = {For supervised training of automatic facial expression recognition systems, adequate ground truth labels that describe relevant facial expression categories are necessary. One possibility is to label facial expressions into emotion categories. Another approach is to label facial expressions independently from any interpretation attempts. This can be achieved via the facial action coding system {(FACS).} In this paper we present a novel approach that allows to automatically cluster {FACScodes} into meaningful categories. Our approach exploits the fact that {FACScodes} can be seen as documents containing terms -the action units {(AUs)} present in the codes-and so text modeling methods that capture co-occurrence information in low-dimensional spaces can be used. The {FACScode} derived descriptions are computed by Latent Semantic Analysis {(LSA)} and Probabilistic Latent Semantic Analysis {(PLSA).} We show that, as a high-level description of facial actions, the newly derived codes constitute a competitive alternative to both basic emotion and {FACScodes.} We have used them to train different types of artificial neural networks},
	publisher = {{ACM}},
	author = {Beat Fasel and Florent Monay and Daniel {Gatica-Perez}},
	year = {2004},
	keywords = {automatic facial expression recognition, latent semantic analysis},
	pages = {181--188}
},

@inproceedings{m.s._bartlett_prototype_2003,
	title = {A Prototype for Automatic Recognition of Spontaneous Facial Actions"},
	volume = {15},
	author = {{M.S.} Bartlett and G. Littlewort and P. Braathen and {T.J.} Sejnowski and {J.R.} Movellan},
	year = {2003},
	pages = {1271--1278}
},

@inproceedings{kaliouby_real-time_2004,
	title = {{Real-Time} Inference of Complex Mental States from Facial Expressions and Head Gestures},
	isbn = {0-7695-2158-4},
	url = {http://portal.acm.org/citation.cfm?id=1033044},
	abstract = {This paper presents a system for inferring complex mental states from video of facial expressions and head gestures in real-time. The system is based on a multi-level dynamic Bayesian network classifier which models complex mental states as a number of interacting facial and head displays, identified from component-based facial features. Experimental results for 6 mental states groups- agreement, concentrating, disagreement, interested, thinking and unsure are reported. Real-time performance, unobtrusiveness and lack of preprocessing make our system particularly suitable for user-independent human computer interaction.},
	publisher = {{IEEE} Computer Society},
	author = {Rana El Kaliouby and Peter Robinson},
	year = {2004},
	pages = {154}
},

@inproceedings{valstar_spontaneous_2006,
	address = {Banff, Alberta, Canada},
	title = {Spontaneous vs. posed facial behavior: automatic analysis of brow actions},
	isbn = {{1-59593-541-X}},
	shorttitle = {Spontaneous vs. posed facial behavior},
	url = {http://portal.acm.org/citation.cfm?id=1181031&dl=GUIDE&coll=GUIDE&CFID=61782021&CFTOKEN=81111183},
	doi = {10.1145/1180995.1181031},
	abstract = {Past research on automatic facial expression analysis has focused mostly on the recognition of prototypic expressions of discrete emotions rather than on the analysis of dynamic changes over time, although the importance of temporal dynamics of facial expressions for interpretation of the observed facial behavior has been acknowledged for over 20 years. For instance, it has been shown that the temporal dynamics of spontaneous and volitional smiles are fundamentally different from each other. In this work, we argue that the same holds for the temporal dynamics of brow actions and show that velocity, duration, and order of occurrence of brow actions are highly relevant parameters for distinguishing posed from spontaneous brow actions. The proposed system for discrimination between volitional and spontaneous brow actions is based on automatic detection of Action Units {(AUs)} and their temporal segments (onset, apex, offset) produced by movements of the eyebrows. For each temporal segment of an activated {AU,} we compute a number of mid-level feature parameters including the maximal intensity, duration, and order of occurrence. We use Gentle Boost to select the most important of these parameters. The selected parameters are used further to train Relevance Vector Machines to determine per temporal segment of an activated {AU} whether the action was displayed spontaneously or volitionally. Finally, a probabilistic decision function determines the class (spontaneous or posed) for the entire brow action. When tested on 189 samples taken from three different sets of spontaneous and volitional facial data, we attain a 90.7\% correct recognition rate.},
	publisher = {{ACM}},
	author = {Michel F. Valstar and Maja Pantic and Zara Ambadar and Jeffrey F. Cohn},
	year = {2006},
	keywords = {automatic facial expression analysis, temporal dynamics},
	pages = {162--170}
},

@inproceedings{gunes_affect_2005,
	title = {Affect recognition from face and body: early fusion vs. late fusion},
	volume = {4},
	shorttitle = {Affect recognition from face and body},
	url = {http://dx.doi.org/10.1109/ICSMC.2005.1571679},
	abstract = {This paper presents an approach to automatic visual emotion recognition from two modalities: face and body. Firstly, individual classifiers are trained from individual modalities. Secondly, we fuse facial expression and affective body gesture information first at a feature-level, in which the data from both modalities are combined before classification, and later at a decision-level, in which we integrate the outputs of the monomodal systems by the use of suitable criteria. We then evaluate these two fusion approaches, in terms of performance over monomodal emotion recognition based on facial expression modality only. In the experiments performed the emotion classification using the two modalities achieved a better recognition accuracy outperforming the classification using the individual facial modality. Moreover, fusion at the feature-level proved better recognition than fusion at the decision-level.},
	author = {H Gunes and M Piccardi},
	year = {2005},
	pages = {3443 Vol. 4, 3437}
},

@inproceedings{bartlett_fully_2006,
	title = {Fully Automatic Facial Action Recognition in Spontaneous Behavior},
	isbn = {0-7695-2503-2},
	url = {http://portal.acm.org/citation.cfm?id=1126342},
	abstract = {We present results on a user independent fully automatic system for real time recognition of facial actions from the Facial Action Coding System {(FACS).} The system automatically detects frontal faces in the video stream and codes each frame with respect to 20 Action units. We present preliminary results on a task of facial action detection in spontaneous expressions during discourse. Support vector machines and {AdaBoost} classifiers are compared. For both classifiers, the output margin predicts action unit intensity.},
	publisher = {{IEEE} Computer Society},
	author = {Marian Stewart Bartlett and Gwen Littlewort and Mark Frank and Claudia Lainscsek1 and Ian Fasel and Javier Movellan},
	year = {2006},
	pages = {223--230}
},

@article{kapoor_automatic_2007,
	title = {Automatic prediction of frustration},
	volume = {65},
	url = {http://portal.acm.org/citation.cfm?id=1266059},
	abstract = {Predicting when a person might be frustrated can provide an intelligent system with important information about when to initiate interaction. For example, an automated Learning Companion or Intelligent Tutoring System might use this information to intervene, providing support to the learner who is likely to otherwise quit, while leaving engaged learners free to discover things without interruption. This paper presents the first automated method that assesses, using multiple channels of affect-related information, whether a learner is about to click on a button saying {''I'm} frustrated.'' The new method was tested on data gathered from 24 participants using an automated Learning Companion. Their indication of frustration was automatically predicted from the collected data with 79\% accuracy (chance=58\%). The new assessment method is based on Gaussian process classification and Bayesian inference. Its performance suggests that non-verbal channels carrying affective cues can help provide important information to a system for formulating a more intelligent response.},
	number = {8},
	journal = {Int. J. {Hum.-Comput.} Stud.},
	author = {Ashish Kapoor and Winslow Burleson and Rosalind W. Picard},
	year = {2007},
	keywords = {affect recognition, affective learning companion, intelligent tutoring system, learner state assessment},
	pages = {724--736}
},

@misc{zhihong_zeng_spontaneous_2006,
	title = {Spontaneous Emotional Facial Expression Detection},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.87.6628},
	abstract = {Abstract — Change in a speaker’s emotion is a
fundamental component in human communication. Automatic
recognition of spontaneous emotion would significantly
impact human-computer interaction and emotion-related
studies in education, psychology and psychiatry. In this
paper, we explore methods for detecting emotional facial
expressions occurring in a realistic human conversation
setting—the Adult Attachment Interview {(AAI).} Because
non-emotional facial expressions have no distinct
description and are expensive to model, we treat emotional
facial expression detection as a one-class classification
problem, which is to describe target objects (i.e.,
emotional facial expressions) and distinguish them from
outliers (i.e., non-emotional ones). Our preliminary
experiments on {AAI} data suggest that one-class
classification methods can reach a good balance between cost
(labeling and computing) and recognition performance by
avoiding non-emotional expression labeling and modeling.
Index Terms—affective computing, facial expression,
oneclass classification, emotion recognition I.},
	author = {Zhihong Zeng and Yun Fu and Glenn I. Roisman and Zhen Wen and Yuxiao Hu and Thomas S. Huang},
	year = {2006}
},

@incollection{lee_facial_2005,
	title = {Facial Expression Analysis Using Nonlinear Decomposable Generative Models},
	url = {http://dx.doi.org/10.1007/11564386_3},
	abstract = {We present a new framework to represent and analyze dynamic facial motions using a decomposable generative model. In this paper, we consider facial expressions which lie on a one dimensional closed manifold, i.e., start from some configuration and coming back to the same configuration, while there are other sources of variability such as different classes of expression, and different people, etc., all of which are needed to be parameterized. The learned model supports tasks such as facial expression recognition, person identification, and synthesis. We aim to learn a generative model that can generate different dynamic facial appearances for different people and for different expressions. Given a single image or a sequence of images, we can use the model to solve for the temporal embedding, expression type and person identification parameters. As a result we can directly infer intensity of facial expression, expression type, and person identity from the visual input. The model can successfully be used to recognize expressions performed by different people never seen during training. We show experiment results for applying the framework for simultaneous face and facial expression recognition.},
	booktitle = {Analysis and Modelling of Faces and Gestures},
	author = {{Chan-Su} Lee and Ahmed Elgammal},
	year = {2005},
	pages = {17--31}
},

@article{cohen_facial_2003,
	title = {Facial expression recognition from video sequences: temporal and static modeling},
	volume = {91},
	shorttitle = {Facial expression recognition from video sequences},
	url = {http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6WCX-491BXV8-2&_user=10&_rdoc=1&_fmt=&_orig=search&_sort=d&_docanchor=&view=c&_rerunOrigin=google&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=3fda688ebffe63a62aac72bfc5e89dea},
	doi = {10.1016/S1077-3142(03)00081-X},
	number = {1-2},
	journal = {Computer Vision and Image Understanding},
	author = {Ira Cohen and Nicu Sebe and Ashutosh Garg and Lawrence S. Chen and Thomas S. Huang},
	month = aug,
	year = {2003},
	pages = {160--187}
},

@inproceedings{ashraf_painful_2007,
	address = {Nagoya, Aichi, Japan},
	title = {The painful face: pain expression recognition using active appearance models},
	isbn = {978-1-59593-817-6},
	shorttitle = {The painful face},
	url = {http://portal.acm.org/citation.cfm?id=1322197&dl=GUIDE&coll=GUIDE&CFID=61771564&CFTOKEN=39031643},
	doi = {10.1145/1322192.1322197},
	abstract = {Pain is typically assessed by patient self-report. Self-reported pain, however, is difficult to interpret and may be impaired or not even possible, as in young children or the severely ill. Behavioral scientists have identified reliable and valid facial indicators of pain. Until now they required manual measurement by highly skilled observers. We developed an approach that automatically recognizes acute pain. Adult patients with rotator cuff injury were video-recorded while a physiotherapist manipulated their affected and unaffected shoulder. Skilled observers rated pain expression from the video on a 5-point Likert-type scale. From these ratings, sequences were categorized as no-pain (rating of 0), pain (rating of 3, 4, or 5), and indeterminate (rating of 1 or 2). We explored machine learning approaches for pain-no pain classification. Active Appearance Models {(AAM)} were used to decouple shape and appearance parameters from the digitized face images. Support vector machines {(SVM)} were used with several representations from the {AAM.} Using a leave-one-out procedure, we achieved an equal error rate of 19\% (hit rate = 81\%) using canonical appearance and shape features. These findings suggest the feasibility of automatic pain detection from video.},
	publisher = {{ACM}},
	author = {Ahmed Bilal Ashraf and Simon Lucey and Jeffrey F. Cohn and Tsuhan Chen and Zara Ambadar and Ken Prkachin and Patty Solomon and Barry J. Theobald},
	year = {2007},
	keywords = {active appearance models, automatic facial image analysis, Facial expression, pain, support vector machines},
	pages = {9--14}
},

@inproceedings{whitehill_haar_2006,
	title = {Haar Features for {FACS} {AU} Recognition},
	isbn = {0-7695-2503-2},
	url = {http://portal.acm.org/citation.cfm?id=1126278},
	abstract = {We examined the effectiveness of using Haar features and the Adaboost boosting algorithm for {FACS} action unit {(AU)} recognition. We evaluated both recognition accuracy and processing time of this new approach compared to the state-of-the-art method of classifying Gabor responses with support vector machines. Empirical results on the {Cohn-Kanade} facial expression database showed that the {Haar+Adaboost} method yields {AU} recognition rates comparable to those of the {Gabor+SVM} method but operates at least two orders of magnitude more quickly.},
	publisher = {{IEEE} Computer Society},
	author = {Jacob Whitehill and Christian W. Omlin},
	year = {2006},
	pages = {97--101}
},

@incollection{lucey_investigating_2007,
	title = {Investigating Spontaneous Facial Action Recognition through {AAM} Representations of the Face},
	booktitle = {Face Recognition Book},
	publisher = {Pro Literatur Verlag},
	author = {Simon Lucey and Ahmed Bilal and {J.F.} Cohn},
	editor = {K. Kurihara},
	year = {2007}
},

@inproceedings{brewster_tactons:_2004,
	title = {Tactons: structured tactile messages for non-visual information display},
	shorttitle = {Tactons},
	url = {http://portal.acm.org/citation.cfm?id=976310.976313},
	booktitle = {{AUIC} '04: Proceedings of the fifth conference on Australasian user interface},
	publisher = {Australian Computer Society, Inc.},
	author = {Stephen Brewster and Lorna Brown},
	year = {2004},
	keywords = {tactile},
	pages = {23, 15}
},

@inproceedings{ertan_wearable_1998,
	title = {A wearable haptic navigation guidance system},
	doi = {10.1109/ISWC.1998.729547},
	abstract = {This paper describes a wearable navigation system based on a
haptic directional display embedded in the back of a vest. The system
consists of a 4-by-4 array of micromotors for delivering haptic
navigational signals to the user's back, an infrared-based input system
for locating the user in an environment, and a wearable computer for
route planning. User testing was conducted to evaluate the effectiveness
of this system as a navigation guide for sighted users in an unfamiliar
lab area. It is hoped that such a system can be a useful navigation
guide for individuals with severe visual impairments in an unfamiliar
environment. Future work will address the specific issues concerning
blind navigation},
	booktitle = {Wearable Computers, 1998. Digest of Papers. Second International Symposium on},
	author = {S. Ertan and C. Lee and A. Willets and H. Tan and A. Pentland},
	year = {1998},
	keywords = {blind navigation, handicapped aids, haptic directional display, haptic interfaces, haptic navigational signals, lab area, navigation, navigation guide, portable computers, unfamiliar environment, visual impairments, wearable navigation system},
	pages = {164--165}
},

@incollection{tsukada_activebelt:_2004,
	title = {{ActiveBelt:} {Belt-Type} Wearable Tactile Display for Directional Navigation},
	shorttitle = {{ActiveBelt}},
	url = {http://www.springerlink.com/content/m62n21ptynyre66n},
	abstract = {In this paper we propose a novel wearable interface called {“ActiveBelt”} that enables users to obtain multiple directional information with the tactile sense. Since the information provided by the tactile sense is relatively unobtrusive, it is suited for daily use in mobile environments. However, many existing systems don’t transmit complex information via the tactile sense. Most of them send only simple signals, such as vibration in cellular phones. {ActiveBelt} is a novel belt-type wearable tactile display that can transmit directional information. We have developed prototype systems and applications, evaluated system performance and usability, and demonstrated the possibility of practical use.},
	booktitle = {{UbiComp} 2004: Ubiquitous Computing},
	author = {Koji Tsukada and Michiaki Yasumura},
	year = {2004},
	keywords = {hci, mobile, navigation, tactile, wearable},
	pages = {399, 384}
},

@inproceedings{jones_development_2004,
	title = {Development of a tactile vest},
	doi = {10.1109/HAPTIC.2004.1287181},
	abstract = {This research is focused on the development of a torso-based haptic display that can be used to present navigational cues to a human operator. The requirements for this display are that it can be worn on a mobile operator, is light weight and robust. Two tactile vests are being developed to meet these objectives, one based on small electric motors and the other on contractile shape-memory alloy {(SMA)} fibers. Four electromechanical actuators were evaluated for use in the vest and a small vibration motor was selected for the initial prototype. A 3×3 array was fabricated for psychophysical testing and positioned on the lower back. High accuracy in perceiving the direction of factor activation was achieved by all subjects in this task. A factor array based on {SMA} actuators was fabricated and tested and results indicate that this low-bandwidth device can generate tactile inputs on the torso that are perceptible.},
	booktitle = {Haptic Interfaces for Virtual Environment and Teleoperator Systems, 2004. {HAPTICS} '04. Proceedings. 12th International Symposium on},
	author = {{L.A.} Jones and M. Nakamura and B. Lockyer},
	year = {2004},
	keywords = {contractile shape-memory alloy fibers, electric actuators, electric motors, electromechanical actuators, haptic interfaces, mobile operator, navigation, tactile vest, torso-based haptic display},
	pages = {82--89}
},

@article{rupert_instrumentation_2000,
	title = {An instrumentation solution for reducing spatial disorientation mishaps},
	volume = {19},
	issn = {0739-5175},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/10738664},
	number = {2},
	journal = {{IEEE} Engineering in Medicine and Biology Magazine: The Quarterly Magazine of the Engineering in Medicine \& Biology Society},
	author = {A H Rupert},
	month = apr,
	year = {2000},
	note = {{PMID:} 10738664},
	keywords = {Aerospace Medicine, Aircraft, Computer Simulation, Confusion, Cues, Data Display, Equipment Design, Feedback, Humans, Military Personnel, Models, Theoretical, Orientation, Sensation, Space Perception, United States, {User-Computer} Interface},
	pages = {71--80}
},

@inproceedings{traylor_development_2002,
	title = {Development of a Wearable Haptic Display for Situation Awareness in Altered-gravity Environment: Some Initial Findings},
	isbn = {0-7695-1489-8},
	shorttitle = {Development of a Wearable Haptic Display for Situation Awareness in Altered-gravity Environment},
	url = {http://portal.acm.org/citation.cfm?id=797505},
	abstract = {Efforts are under way to develop a wearable haptic display that can impart directional information on a user's back for situation awareness. To date, two studies have been conducted aboard the {NASA} {KC-135A} reduced gravity aircraft to investigate the perception of tactile information in altered-gravity environments. This paper reports our results on perceived loudness of vibrotactile stimulation under different gravity conditions. Subjects compared seven fixed-frequency varying-amplitude vibrations in 1.8-g to a reference vibration delivered in zero-g using the method of constant stimuli. Our results show that the points of subjective equality {(PSE)} measured in 1.8-g are essentially the same as the intensity of the reference signal delivered in zero-g. The difference between {PSE} and the reference is less than the difference threshold {(DL)} measured in 1.8-g. We also found that the displacements (measured with an accelerometer) produced by our tactors in one-g and zero-g conditions are the same using identical driving waveforms. Our data suggest that the perceived loudness of vibrotactile stimuli remains the same in altered-gravity environments. However, a user's ability to interpret vibrotactile signals in zero-g environment may be hampered by increased cognitive load due to the need to continuously monitor the position and movement of one's body.},
	booktitle = {Proceedings of the 10th Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems},
	publisher = {{IEEE} Computer Society},
	author = {Ryan Traylor and Hong Z. Tan},
	year = {2002},
	keywords = {altered-gravity environment, discrimination threshold, Haptic Display, situation awareness, wearable haptic display},
	pages = {159}
},

@article{wall_balance_2003,
	title = {Balance prostheses for postural control},
	volume = {22},
	issn = {0739-5175},
	doi = {10.1109/MEMB.2003.1195701},
	abstract = {There is a clear need for a prosthesis that improves postural stability in the balance impaired. Such a device would be used as a temporary aid during recovery from ablative inner-ear surgery and as a permanent prosthesis for those elderly prone to falls. Research using a one-axis device that estimates body tilt and displays it to vestibulopathic subjects via an array of tactile vibrators has demonstrated feasibility. The noninvasive, vibrotactile display of body tilt helped the balance-impaired subjects to reduce their body sway during standardized tests. The motion sensor array is comprised of three {MEMS} linear accelerometers and three {MEMS} rate gyros whose sensitive axes are aligned along three orthogonal directions to provide six-degree-of-freedom (dof) motion information.},
	number = {2},
	journal = {Engineering in Medicine and Biology Magazine, {IEEE}},
	author = {C. Wall and {M.S.} Weinberg},
	year = {2003},
	keywords = {ablative inner-ear surgery, accelerometers, balance prostheses, balance-impaired subjects, biocontrol, biomechanics, body sway, body tilt, display instrumentation, elderly, falls, geriatrics, handicapped aids, mechanoception, medical control systems, {MEMS} linear accelerometers, {MEMS} rate gyros, microsensors, motion sensor array, noninvasive vibrotactile display, one-axis device, patient rehabilitation, permanent prosthesis, postural control, postural stability, prosthetics, recovery, sensory aids, six-degree-of-freedom motion information, standardized tests, tactile sensors, tactile vibrator array, temporary aid, three orthogonal directions, vestibulopathic subjects},
	pages = {84--90}
},

@article{erp_waypoint_2005,
	title = {Waypoint navigation with a vibrotactile waist belt},
	volume = {2},
	url = {http://portal.acm.org/citation.cfm?id=1060581.1060585},
	doi = {10.1145/1060581.1060585},
	abstract = {Presenting waypoint navigation on a visual display is not suited for all situations. The present experiments investigate if it is feasible to present the navigation information on a tactile display. Important design issue of the display is how direction and distance information must be coded. Important usability issues are the resolution of the display and its usefulness in vibrating environments. In a pilot study with 12 pedestrians, different distance-coding schemes were compared. The schemes translated distance to vibration rhythm while the direction was translated into vibration location. The display consisted of eight tactors around the user's waist. The results show that mapping waypoint direction on the location of vibration is an effective coding scheme that requires no training, but that coding for distance does not improve performance compared to a control condition with no distance information. In Experiment 2, the usefulness of the tactile display was shown in two case studies with a helicopter and a fast boat.},
	number = {2},
	journal = {{ACM} Trans. Appl. Percept.},
	author = {Jan B. F. Van Erp and Hendrik A. H. C. Van Veen and Chris Jansen and Trevor Dobbins},
	year = {2005},
	keywords = {navigation, vehicle control, vibrotactile displays, visually handicapped},
	pages = {106--117}
},

@inproceedings{heuten_tactile_2008,
	address = {Lund, Sweden},
	title = {Tactile wayfinder: a non-visual support system for wayfinding},
	isbn = {978-1-59593-704-9},
	shorttitle = {Tactile wayfinder},
	url = {http://portal.acm.org/citation.cfm?id=1463179},
	doi = {10.1145/1463160.1463179},
	abstract = {Digital maps and route descriptions on a {PDA} have become very popular for navigation, not the least with the advent of the {iPhone} and its Google Maps application. A visual support for wayfinding, however, is not reasonable or even possible all the time. A pedestrian must pay attention to traffic on the street, a hiker should concentrate on the narrow trail, and a blind person relies on other modalities to find her way. To overcome these limitations, we developed a non-visual support for wayfinding that guides and keeps a mobile user en route by a tactile display. We designed a belt with vibrators that indicates directions and deviations from the path in an accurate and unobtrusive way. Our first user evaluation showed that on an open field without any landmarks the participants stayed well to given test routes and that wayfinding support is possible with our Tactile Wayfinder.},
	booktitle = {Proceedings of the 5th Nordic conference on Human-computer interaction: building bridges},
	publisher = {{ACM}},
	author = {Wilko Heuten and Niels Henze and Susanne Boll and Martin Pielot},
	year = {2008},
	keywords = {pedestrian navigation, Tactile Display, wayfinding},
	pages = {172--181}
},

@inproceedings{jones_localization_2008,
	title = {Localization and Pattern Recognition with Tactile Displays},
	isbn = {978-1-4244-2005-6},
	url = {http://portal.acm.org/citation.cfm?id=1546720},
	abstract = {A set of four experiments was conducted to evaluate tactile localization and tactile pattern recognition on the torso. A one- dimensional eight-tactor display and a two-dimensional 16-tactor display were used to present tactile cues to the waist and back respectively. The results from the spatial localization experiments indicated that a display with eight tactors mounted circumferentially around the waist can provide tactile cues that are perceived very accurately in terms of the location of stimulation. In contrast, the 16-tactor array on the back was found to be inadequate to support precise spatial mapping, but an array with fewer elements could provide such spatial cues. The second set of experiments evaluated tactile pattern recognition around the waist and on the back with the objective of determining what types of tactile stimuli could be useful for creating tactons. The tactile display used in these experiments stimulated the skin at a fixed frequency and amplitude and varied the number and location of tactors simultaneously active to convey information. For both the waist and back, tactile patterns were identified with high accuracy, 99\% and 95\% correct response rate respectively. These findings suggest that simple navigational and instructional commands can be presented factually on the torso.},
	booktitle = {Proceedings of the 2008 Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems},
	publisher = {{IEEE} Computer Society},
	author = {L. A. Jones and K. Ray},
	year = {2008},
	keywords = {Tactile Cues, tactile displays, tactile localization, tactile pattern recognition, tactile stimuli},
	pages = {33--39}
},

@inproceedings{ferscha_vibro-tactile_2008,
	title = {Vibro-tactile space awareness},
	author = {A. Ferscha and B. Emsenhuber and A. Riener and C. Holzman and M. Hechinger and D. Hochreiter and M. Franz and D. Zeider and M. D. S. Rocha and C. Klein},
	year = {2008}
},

@incollection{brell_fusion_2008,
	title = {Fusion of Vibrotactile Signals Used in a Tactile Display in Computer Aided Surgery},
	url = {http://dx.doi.org/10.1007/978-3-540-69057-3_50},
	abstract = {In this paper the humans’ hand movement as response to a vibration signal is presented. The signal thereby is composed of
single vibration stimuli. The results of this study are used for signal generation of a tactile display for computer aided
surgery. Each of the display’s tactors represents one movement direction of the hand. To present directions between single
movement axes it is necessary to combine single vibration signals. Therefore an experiment is presented to examine the fusion
of single tactile signals. The experiment is divided into three parts to compare three different ways of merging vibration
signals. Best results were achieved with a modulation of the pulse duty factor.},
	booktitle = {Haptics: Perception, Devices and Scenarios},
	author = {Melina Brell and Dirk Roßkamp and Andreas Hein},
	year = {2008},
	pages = {383--388}
},

@article{murray_psychophysical_2003,
	title = {Psychophysical characterization and testbed validation of a wearable vibrotactile glove for telemanipulation},
	volume = {12},
	url = {http://portal.acm.org/citation.cfm?id=782658},
	abstract = {This paper describes and evaluates a high-fidelity, low-cost haptic interface for teleoperation. The interface is a wearable vibrotactile glove containing miniature voice coils that provides continuous, proportional force information to the user's fingertips. In psychophysical experiments, correlated variations in the frequency and amplitude of the stimulators extended the user's perceptual response range compared to varying amplitude or frequency alone. In an adaptive, force-limited, pick-and-place manipulation task, the interface allowed users to control the grip forces more effectively than no feedback or binary feedback, which produced equivalent performance. A sorting experiment established that proportional tactile feedback enhances the user's ability to discriminate the relative properties of objects, such as weight. We conclude that correlated amplitude and frequency signals, simulating force in a remote environment, substantially improve teleoperation.},
	number = {2},
	journal = {Presence: Teleoper. Virtual Environ.},
	author = {Anne M. Murray and Roberta L. Klatzky and Pradeep K. Khosla},
	year = {2003},
	pages = {156--182}
},

@article{cha_framework_2009,
	title = {A Framework for Haptic Broadcasting},
	volume = {16},
	issn = {{1070-986X}},
	abstract = {This article presents a comprehensive exploration of the issues underlying haptic multimedia broadcasting. It also describes the implementation of a prototype system as a proof of concept.},
	number = {3},
	journal = {{IEEE} {MultiMedia}},
	author = {Jongeun Cha and {Yo-Sung} Ho and Yeongmi Kim and Jeha Ryu and Ian Oakley},
	year = {2009},
	keywords = {haptic media broadcasting, haptics, mpeg-4},
	pages = {16--27},
	annote = {Complete {PDF} document was either not available or accessible. Please make sure you're logged in to the digital library to retrieve the complete {PDF} document.}
},

@article{oron-gilad_vibrotactile_2007,
	title = {Vibrotactile Guidance Cues for Target Acquisition},
	volume = {37},
	issn = {1094-6977},
	doi = {10.1109/TSMCC.2007.900646},
	abstract = {Three experiments examined the use of vibrotactile cues to guide an operator toward a target. Vibrotactile stimulation on the hand can provide spatially stabilizing cues for feedback of subtle changes in position. When such feedback is present, a deviation from the point of origin results in tactile stimulation indicating the direction and magnitude of the positional error. Likewise, spatial deviation from a desired position displayed tactually can provide robust position guidance and stabilization sufficient to improve the acquisition time and accuracy of fine cursor control. A major advantage of this mode of information representation is that it can be present at the same time as visual cues with minimal cross-modal interference. Our findings suggest that performance is actually enhanced when both tactile and visual cues are present. Although previous studies have suggested that various forms of tactile feedback can provide position guidance and stabilization, to our knowledge, this work is the first that details the effect of tactile feedback on target acquisition directly.},
	number = {5},
	journal = {Systems, Man, and Cybernetics, Part C: Applications and Reviews, {IEEE} Transactions on},
	author = {T. {Oron-Gilad} and {J.L.} Downs and {R.D.} Gilson and {P.A.} Hancock},
	year = {2007},
	keywords = {cross-modal interference, fine cursor control, Guidance cues, haptic interfaces, military systems, position control, position guidance, stabilization, target acquisition, vibrotactile, vibrotactile guidance cues, vibrotactile stimulation},
	pages = {993--1004}
},

@inproceedings{cha_authoring/editing_2007,
	title = {An {Authoring/Editing} Framework for Haptic Broadcasting: Passive Haptic Interactions using {MPEG-4} {BIFS}},
	shorttitle = {An {Authoring/Editing} Framework for Haptic Broadcasting},
	doi = {10.1109/WHC.2007.20},
	abstract = {In this paper, we propose an authoring/editing framework for haptic broadcasting that provides viewers with passive haptic interactions synchronized with audiovisual media by extending {MPEG-4} {BIFS} (binary format for scenes). Based on this framework, we could conveniently author haptically enhanced broadcast contents and provide them to viewers by streaming via a {VOD} (video on demand) context. This work will also appear in the demonstration session during the conference},
	booktitle = {{EuroHaptics} Conference, 2007 and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems. World Haptics 2007. Second Joint},
	author = {Jongeun Cha and Yongwon Seo and Yeongmi Kim and Jeha Ryu},
	year = {2007},
	keywords = {audiovisual media, audio-visual systems, authoring framework, authoring systems, binary format for scenes, editing framework, haptic broadcasting, haptic interfaces, {MPEG-4} {BIFS,} passive haptic interactions, video coding, video on demand, video streaming},
	pages = {274--279}
},

@inproceedings{uchiyama_vibrotactile_2008,
	address = {Auburn, Alabama},
	title = {Vibrotactile Glove guidance for semi-autonomous wheelchair operations},
	isbn = {978-1-60558-105-7},
	url = {http://portal.acm.org/citation.cfm?id=1593195},
	doi = {10.1145/1593105.1593195},
	abstract = {This paper presents the design of a novel tactile display, the Vibrotactile Glove, which provides a wheelchair user who has severe visual impairment with vibration (vibrotactile) signals to operate a powered wheelchair. The vibrotactile signals are conducted to the user's skin through a 3-by-3 array of vibrating elements (also known as vibrotactor). The vibrotactor array is placed on the back side of the Vibrotactile Glove. We designed a motor array controller which generates sequences of aligned stimuli indicating directional guidance (vertical, horizontal, and diagonal) and points of stimuli indicating the orientation and distance of obstacles. The haptic sensitivity of stimuli localization is reinforced by signal repetition with short inter-stimuli period. The preliminary results reveal the positive potential of the Vibrotactile Glove as an effective and robust tactile display that can convey essential information of wheelchair operation to a user with severe visual impairment.},
	booktitle = {Proceedings of the 46th Annual Southeast Regional Conference on {XX}},
	publisher = {{ACM}},
	author = {H. Uchiyama and M. A. Covington and W. D. Potter},
	year = {2008},
	keywords = {semi-autonomous wheelchair, Tactile Display, vibrotactile glove},
	pages = {336--339}
},

@inproceedings{cappelleti_vibrotactile_1998,
	title = {Vibrotactile   colour  rendering   for   the   visually   impaired   within  the   {VIDET}   project},
	volume = {3524},
	author = {L. Cappelleti and M. Feeri and G. Nicoletti},
	year = {1998},
	pages = {92--96}
},

@article{zelek_haptic_2003,
	title = {A Haptic glove as a tactile-vision sensory substitution for wayfinding},
	volume = {97},
	number = {10},
	journal = {Journal of Visual Impairment \& Blindness},
	author = {John S. Zelek and Sam Bromley and Daniel Asmar and David Thompson},
	year = {2003},
	pages = {1--24},
	annote = {A device that relays navigational information using a portable tactile glove and a wearable computer and camera system was tested with nine adults with visual impairments. Paths traversed by subjects negotiating an obstacle course were not qualitatively different from paths produced with existing wayfinding devices and hitting probabilities were minimized. {(Contains} references.) {(Author/CR)}}
},

@inproceedings{hein_contact_2007,
	address = {Los Alamitos, {CA,} {USA}},
	title = {{conTACT} - A Vibrotactile Display for Computer Aided Surgery},
	volume = {0},
	isbn = {0-7695-2738-8},
	doi = {http://doi.ieeecomputersociety.org/10.1109/WHC.2007.33},
	abstract = {In this paper the concept and the first proving experiments of a navigation system for assistance during surgical interventions is presented. Novel aspects of this system are a new approach to transmit the navigation information to the surgeon. Contrary to known navigation systems the proposed approach does not only use visual support but provides information primarily by tactile stimuli. So the surgeon is not constrained to avert the gaze from the field of surgery during the navigation process. This paper concludes with the experimental evaluation of signal perception and a positioning task with tactile transmission of the navigation information.},
	booktitle = {World Haptics Conference},
	publisher = {{IEEE} Computer Society},
	author = {Andreas Hein and Melina Brell},
	year = {2007},
	pages = {531--536},
	annote = {Complete {PDF} document was either not available or accessible. Please make sure you're logged in to the digital library to retrieve the complete {PDF} document.}
},

@inproceedings{romano_toward_2009,
	title = {Toward tactilely transparent gloves: Collocated slip sensing and bibrotactile actuation},
	isbn = {978-1-4244-3858-7},
	shorttitle = {Toward tactilely transparent gloves},
	url = {http://portal.acm.org/citation.cfm?id=1550301},
	abstract = {Tactile information plays a critical role in the human ability to manipulate objects with one's hands. Many environments require the use of protective gloves that diminish essential tactile feedback. Under these circumstances, seemingly simple tasks such as picking up an object can become very difficult. This paper introduces the {SlipGlove,} a novel device that uses an advanced sensing and actuation system to return this vital tactile information to the user. Our {SlipGlove} prototypes focus on providing tactile cues associated with slip between the glove and a contact surface. Relative motion is sensed using optical mouse sensors embedded in the glove's surface. This information is conveyed to the wearer via miniature vibration motors placed inside the glove against the wearer's skin. The collocation of slip sensing and tactile feedback creates a system that is natural and intuitive to use. We report results from a human subject study demonstrating that the {SlipGlove} allows the wearer to approach the capabilities of bare skin in detecting and reacting to fingertip slip. Users of the {SlipGlove} also had significantly faster and more consistent reaction to fingertip slip when compared to a traditional glove design. The {SlipGlove} technology allows us to enhance human perception when interacting with real environments and move toward the goal of a tactilely transparent glove.},
	booktitle = {Proceedings of the World Haptics 2009 - Third Joint {EuroHaptics} conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems},
	publisher = {{IEEE} Computer Society},
	author = {Joseph M. Romano and Steven R. Gray and Nathan T. Jacobs and Katherine J. Kuchenbecker},
	year = {2009},
	pages = {279--284}
}?
@inproceedings{strong_feather_1996,
	title = {Feather, scent, and shaker: Supporting simple intimacy},
	shorttitle = {Feather, scent, and shaker},
	booktitle = {Proceedings of {CSCW'96}},
	author = {R Strong and W Gaver},
	year = {1996},
	keywords = {artifact, Communication, emotional}
},

@inproceedings{brave_intouch:_1997,
	address = {Atlanta, Georgia},
	title = {{inTouch:} a medium for haptic interpersonal communication},
	isbn = {0-89791-926-2},
	shorttitle = {{inTouch}},
	url = {http://portal.acm.org/citation.cfm?id=1120212.1120435},
	doi = {10.1145/1120212.1120435},
	abstract = {In this paper, we introduce a new approach for applying haptic feedback technology to interpersonal communication. We present the design of our prototype {inTouch} system which provides a physical link between users separated by distance.},
	booktitle = {{CHI} '97 extended abstracts on Human factors in computing systems: looking to the future},
	publisher = {{ACM}},
	author = {Scott Brave and Andrew Dahley},
	year = {1997},
	keywords = {force feedback, haptics, interpersonal communication, telepresence},
	pages = {363--364}
},

@inproceedings{dodge_bed:_1997,
	address = {Atlanta, Georgia},
	title = {The bed: a medium for intimate communication},
	isbn = {0-89791-926-2},
	shorttitle = {The bed},
	url = {http://portal.acm.org/citation.cfm?id=1120212.1120439},
	doi = {10.1145/1120212.1120439},
	abstract = {In this paper, I present {"The} Bed", an environment providing a new form of abstracted presence for intimate, non-verbal inter-personal communication. This secure and familiar environment is explored for its ability to become a shared virtual space for bridging the distance between two remotely located individuals through aural, visual, and tactile manifestations of subtle emotional qualities. As an example, I describe the application of these tangible interfaces and ambient media into a working prototype.},
	booktitle = {{CHI} '97 extended abstracts on Human factors in computing systems: looking to the future},
	publisher = {{ACM}},
	author = {Chris Dodge},
	year = {1997},
	keywords = {abstracted presence, ambient media, physical avatars, tangible interfaces, telepresence},
	pages = {371--372}
},

@inproceedings{rueb_billow:_1997,
	address = {Atlanta, Georgia},
	title = {Billow: networked hospital playspace for children},
	isbn = {0-89791-926-2},
	shorttitle = {Billow},
	url = {http://portal.acm.org/citation.cfm?id=1120212.1120432&coll=Portal&dl=GUIDE&CFID=63320608&CFTOKEN=59979857},
	doi = {10.1145/1120212.1120432},
	abstract = {Through exploring play as a therapeutic process, we have developed a system called {"Billow"} which allows children in hospitals, who are quarantined or otherwise isolated, to play in a virtual audio-visual cloudscape using a malleable, egg-shaped input/output device. This prototype was designed in collaboration with child psychologists and art therapists who are advocates for these children in the hospital setting. It is intended to address the children's need for increased human interaction and social development, mastery and control, and comfort and security. Billow addresses these needs by enabling isolated children to play together and communicate in a locally networked, audio-visual play environment.},
	booktitle = {{CHI} '97 extended abstracts on Human factors in computing systems: looking to the future},
	publisher = {{ACM}},
	author = {Teri Rueb and John Wardzala and Jessica Millstone},
	year = {1997},
	keywords = {audio, {CHILDREN,} hospitals, tactile input device, telephony, virtual community},
	pages = {357--358}
},

@inproceedings{fogg_handjive:_1998,
	address = {Los Angeles, California, United States},
	title = {{HandJive:} a device for interpersonal haptic entertainment},
	isbn = {0-201-30987-4},
	shorttitle = {{HandJive}},
	url = {http://portal.acm.org/citation.cfm?id=274644.274653&coll=ACM&dl=ACM&type=series&idx=SERIES260&part=series&WantType=Proceedings&title=CHI},
	doi = {10.1145/274644.274653},
	abstract = {Note: {OCR} errors may be found in this Reference List extracted from the full text article. {ACM} has opted to expose the complete List rather than only correct and linked references.},
	booktitle = {Proceedings of the {SIGCHI} conference on Human factors in computing systems},
	publisher = {{ACM} {Press/Addison-Wesley} Publishing Co.},
	author = {B. J. Fogg and Lawrence D. Cutler and Perry Arnold and Chris Eisbach},
	year = {1998},
	keywords = {entertainment, haptics, interaction design, interpersonal communication, product design, rapid prototyping, tactile feedback, ubiquitous computing, user testing},
	pages = {57--64}
},

@inproceedings{morikawa_sense_2000,
	address = {The Hague, The Netherlands},
	title = {The sense of physically crossing paths: creating a soft initiation in {HyperMirror} communication},
	isbn = {1-58113-248-4},
	shorttitle = {The sense of physically crossing paths},
	url = {http://portal.acm.org/citation.cfm?id=633393},
	doi = {10.1145/633292.633393},
	abstract = {In this report we discuss adding a new sensor which allows the user to perceive changes in {HyperMirror} space. The sensation caused by the tactile displays allows the user to perceive other users' movements. The tactile signal is not intrusive, although it works to arouse attention. It is possible to immerse oneself in the {HyperMirror} conversation environment without paying constant attention to the screen.},
	booktitle = {{CHI} '00 extended abstracts on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Osamu Morikawa and Juli Yamashita and Yukio Fukui},
	year = {2000},
	keywords = {bodily sensation, haptic, hypermirror, the sense of physically crossing paths, the sense of shared space, wisiwys},
	pages = {183--184}
},

@inproceedings{dobson_creating_2001,
	address = {Seattle, Washington},
	title = {Creating visceral personal and social interactions in mediated spaces},
	isbn = {1-58113-340-5},
	url = {http://portal.acm.org/citation.cfm?id=634160},
	doi = {10.1145/634067.634160},
	abstract = {We introduce vibration and temperature as visceral modes to aid intuitive social perception in networked interaction. We describe two implementations of these ideas for mediated systems -- {VibroBod} for interpersonal communication and What's Shaking for newsgroup navigation.},
	booktitle = {{CHI} '01 extended abstracts on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Kelly Dobson and danah boyd and Wendy Ju and Judith Donath and Hiroshi Ishii},
	year = {2001},
	keywords = {social intuition, tangible media, temperature, Vibration},
	pages = {151--152}
},

@inproceedings{chang_lumitouch:_2001,
	address = {Seattle, Washington},
	title = {{LumiTouch:} an emotional communication device},
	isbn = {1-58113-340-5},
	shorttitle = {{LumiTouch}},
	url = {http://portal.acm.org/citation.cfm?id=634252},
	doi = {10.1145/634067.634252},
	abstract = {We present the Lumitouch system consisting of a pair of interactive picture frames. When one user touches her picture frame, the other picture frame lights up. This touch is translated to light over an Internet connection. We introduce a semi-ambient display that can transition seamlessly from periphery to foreground in addition to communicating emotional content. In addition to enhancing the communication between loved ones, people can use {LumiTouch} to develop a personal emotional {language.Based} upon prior work on telepresence and tangible interfaces, {LumiTouch} explores emotional communication in tangible form. This paper describes the components, interactions, implementation and design approach of the {LumiTouch} system.},
	booktitle = {{CHI} '01 extended abstracts on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Angela Chang and Ben Resner and Brad Koerner and {XingChen} Wang and Hiroshi Ishii},
	year = {2001},
	keywords = {ambient media, emotional communication, tangible interfaces, telepresence},
	pages = {313--314}
},

@inproceedings{sekiguchi_robotphone:_2001,
	address = {Seattle, Washington},
	title = {{RobotPHONE:} {RUI} for interpersonal communication},
	isbn = {1-58113-340-5},
	shorttitle = {{RobotPHONE}},
	url = {http://portal.acm.org/citation.cfm?id=634231},
	doi = {10.1145/634067.634231},
	abstract = {{RobotPHONE} is a Robotic User Interface {(RUI)} that uses robots as physical avatars for interpersonal communication. Using {RobotPHONE,} users in remote locations can communicate shapes and motion with each other. In this paper we present the concept of {RobotPHONE,} and describe implementations of two prototypes.},
	booktitle = {{CHI} '01 extended abstracts on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Dairoku Sekiguchi and Masahiko Inami and Susumu Tachi},
	year = {2001},
	keywords = {bilateral servo, interface, interpersonal communication, physical avatar, robot, rui},
	pages = {277--278}
},

@inproceedings{hansson_lovebomb:_2001,
	address = {Seattle, Washington},
	title = {The {LoveBomb:} encouraging the communication of emotions in public spaces},
	isbn = {1-58113-340-5},
	shorttitle = {The {LoveBomb}},
	url = {http://portal.acm.org/citation.cfm?id=634319},
	doi = {10.1145/634067.634319},
	abstract = {We are exploring the use of persuasive computational technology as an instrument for the communication of human emotions. Our current focus is on encouraging such communication between strangers. We present the concept of the {LoveBomb} - a mobile and persuasive device that allows people to anonymously communicate feelings of love (happiness) and sadness. The device contains a radio transceiver that the user can employ to send out shock waves of love, affecting people in the proximity carrying a {LoveBomb} device. The device also lets its users cry for compassion, quietly signaling to others that they are sad. The {LoveBomb} is intended to encourage people to express themselves emotionally when situated amongst strangers in public spaces. Focus group studies have provided us with an initial understanding regarding the {LoveBomb's} potential social impact.},
	booktitle = {{CHI} '01 extended abstracts on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Rebecca Hansson and Tobias Skog},
	year = {2001},
	keywords = {emotional communication, mobile devices, persuasive technology},
	pages = {433--434}
},

@inproceedings{maynes-aminzade_hover:_2002,
	address = {San Antonio, Texas},
	title = {Hover: conveying remote presence},
	isbn = {1-58113-525-4},
	shorttitle = {Hover},
	url = {http://portal.acm.org/citation.cfm?id=1242207},
	doi = {10.1145/1242073.1242207},
	abstract = {This sketch presents Hover, a device that enhances remote telecommunication by providing a sense of the activity and presence of remote users. The motion of a remote persona is manifested as the playful movements of a ball floating in midair. Hover is both a communication medium and an aesthetic object.},
	booktitle = {{ACM} {SIGGRAPH} 2002 conference abstracts and applications},
	publisher = {{ACM}},
	author = {Dan {Maynes-Aminzade} and {Beng-Kiang} Tan and Ken Goulding and Catherine Vaucelle},
	year = {2002},
	pages = {194--194}
},

@inproceedings{chang_comtouch:_2002,
	title = {{ComTouch:} design of a vibrotactile communication device},
	isbn = {1581135157},
	shorttitle = {{ComTouch}},
	url = {http://dx.doi.org/10.1145/778712.778755},
	booktitle = {{DIS} '02: Proceedings of the conference on Designing interactive systems},
	publisher = {{ACM} Press},
	author = {Angela Chang and Sile {O'Modhrain} and Rob Jacob and Eric Gunther and Hiroshi Ishii},
	year = {2002},
	keywords = {Communication, mobile, tactile},
	pages = {320, 312}
},

@inproceedings{raffle_super_2003,
	address = {Ft. Lauderdale, Florida, {USA}},
	title = {Super cilia skin: an interactive membrane},
	isbn = {1-58113-637-4},
	shorttitle = {Super cilia skin},
	url = {http://portal.acm.org/citation.cfm?id=765891.766004},
	doi = {10.1145/765891.766004},
	abstract = {In this paper we introduce Super Cilia Skin, a new approach for integrating haptic and visual communication. Super Cilia Skin is conceived as a computationally enhanced membrane coupling tactile input with tactile and visual output. We present the design of our prototype, an array of individual actuators (cilia) that use changes in orientation to display images or physical gestures. We discuss ongoing research to develop tactile input capabilities and we present examples of how it can enrich interpersonal communication and children's learning.},
	booktitle = {{CHI} '03 extended abstracts on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Hayes Raffle and Mitchell W. Joachim and James Tichenor},
	year = {2003},
	keywords = {actuation, education, haptics, interpersonal communication, kinesthesia, tangible interface, toys},
	pages = {808--809}
},

@inproceedings{meyer_bodys_2004,
	address = {Los Angeles, California},
	title = {The body's surface as a multimedia interface: closed-eyes nonverbal telehaptic communication},
	isbn = {1-59593-896-2},
	shorttitle = {The body's surface as a multimedia interface},
	url = {http://portal.acm.org/citation.cfm?id=1186376},
	doi = {10.1145/1186223.1186376},
	abstract = {Note: {OCR} errors may be found in this Reference List extracted from the full text article. {ACM} has opted to expose the complete List rather than only correct and linked references.},
	booktitle = {{ACM} {SIGGRAPH} 2004 Sketches},
	publisher = {{ACM}},
	author = {William Meyer},
	year = {2004},
	pages = {122}
},

@inproceedings{suzuki_feellight:_2004,
	address = {New York, {NY,} {USA}},
	title = {Feellight: a communication device for distant nonverbal exchange},
	isbn = {1-58113-933-0},
	shorttitle = {Feellight},
	url = {http://portal.acm.org/citation.cfm?id=1026776.1026786},
	doi = {10.1145/1026776.1026786},
	abstract = {This paper describes a methodology of a bidirectional {I/O} device design and introduces a unique nterface {{\textless}i{\textgreater}FeelLight{\textless}/i{\textgreater},} which enables a simple and seamless communication among people in istant places. The authors focused on the following key issues of the development of interface for elepresence systems: (i) simplicity: simple tool and communication method (ii) affordability: affordance nd embodiment of the interface, (iii) synchronicity: intuitiveness and connectedness, and (iv) creativity: mergence of communication.},
	booktitle = {Proceedings of the 2004 {ACM} {SIGMM} workshop on Effective telepresence},
	publisher = {{ACM}},
	author = {Kenji Suzuki and Shuji Hashimoto},
	year = {2004},
	pages = {40--44}
},

@inproceedings{rovers_him:_2004,
	address = {Vienna, Austria},
	title = {{HIM:} a framework for haptic instant messaging},
	isbn = {1-58113-703-6},
	shorttitle = {{HIM}},
	url = {http://portal.acm.org/citation.cfm?id=986052},
	doi = {10.1145/985921.986052},
	abstract = {Instant Messaging {(IM)} is a popular chatting platform on the internet and increasingly permeates teenage life. Even intimate and emotional content is discussed. As touch is a powerful signal for emotional content, haptic signals, and especially hapticons can contribute to overcome the inevi-table loss of subtle non-verbal communication cues. Audio-visual extensions of {IM} to share emotions, in particular emoticons, have been received enthusiastically by {IM} users. This indicates a realistic user-need for hapticons in {IM.The} Haptic Instant Messaging {(HIM)} framework introduced in this paper combines communication of textual messages with haptic effects and hapticons. The application is build as an open framework and supports small chatting communities to explore the design and use of hapticons and haptic {IO} devices. Researchers can use the {HIM} framework to monitor the use of haptics in communication and how haptics contribute to the fun and meaning of instant messaging.},
	booktitle = {{CHI} '04 extended abstracts on Human factors in computing systems},
	publisher = {{ACM}},
	author = {{A.F.} Rovers and {H.A.} van Essen},
	year = {2004},
	keywords = {Communication, emoticons, hapticons, haptics, instant messaging (im), intimacy, presence},
	pages = {1313--1316}
},

@inproceedings{rovers_footio_2005,
	title = {{FootIO} " Design and Evaluation of a Device to Enable Foot Interaction over a Computer Network},
	isbn = {0-7695-2310-2},
	url = {http://portal.acm.org/citation.cfm?id=1050095},
	abstract = {The development of a device that enables haptic foot interaction for communication over a network is presented. Considering the physical properties of feet we demonstrate that feet are suited for personal, concealed communication over a computer network. First experiments to investigate both the usability and fun of using foot interaction indicate promising results and concrete opportunities for further development.},
	booktitle = {Proceedings of the First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems},
	publisher = {{IEEE} Computer Society},
	author = {A. F. Rovers and H. A. van Essen},
	year = {2005},
	pages = {521--522}
},

@inproceedings{mueller_hug_2005,
	address = {Portland, {OR,} {USA}},
	title = {Hug over a distance},
	isbn = {1-59593-002-7},
	url = {http://portal.acm.org/citation.cfm?id=1056808.1056994},
	doi = {10.1145/1056808.1056994},
	abstract = {People in close relationships, who are separated by distance, often have difficulty expressing intimacy adequately. Based on the results of an ethnographic study with couples, a prototype was developed to test the feasibility of technology in the domain of intimacy. Hug Over a Distance is an air-inflatable vest that can be remotely triggered to create a sensation resembling a hug. Although the couples did not consider the vest to be useful in their daily lives, the prototype served to provoke and stimulate design ideas from the couples during participative design workshops. An additional and unexpected benefit was also found: the prototype enhanced the couples' understanding of the researchers' methods, suggesting that prototypes can serve as tools to make participatory design volunteers aware of their importance in academic research.},
	booktitle = {{CHI} '05 extended abstracts on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Florian {'Floyd'} Mueller and Frank Vetere and Martin R. Gibbs and Jesper Kjeldskov and Sonja Pedell and Steve Howard},
	year = {2005},
	keywords = {design workshop, haptic interface, intimacy, participatory design, remote interaction, {SOCIAL} interaction, Tactile Display, ubiquitous computing, wearable computing},
	pages = {1673--1676}
},

@inproceedings{teh_internet_2006,
	address = {Hollywood, California},
	title = {Internet pajama},
	isbn = {1-59593-380-8},
	url = {http://portal.acm.org/citation.cfm?id=1178940},
	doi = {10.1145/1178823.1178940},
	abstract = {Internet Pajama is a novel application aimed at promoting physical closeness in remote communication between parent and child. A special pajama suit which is able to reproduce a hugging sensation is developed for the child. Parent is able to hug and physically interact with child by hugging a doll. Both pajama and doll are connected wirelessly to the Internet. Other features are incorporated to enhance the interaction and physical presence.},
	booktitle = {Proceedings of the 2006 {ACM} {SIGCHI} international conference on Advances in computer entertainment technology},
	publisher = {{ACM}},
	author = {James Teh and Shang Ping Lee and Adrian David Cheok},
	year = {2006},
	keywords = {haptic, intimate computing, remote hugging, remote interaction},
	pages = {102}
},

@inproceedings{bonanni_affective_2006,
	address = {Boston, Massachusetts},
	title = {Affective {TouchCasting}},
	isbn = {1-59593-364-6},
	url = {http://portal.acm.org/citation.cfm?id=1179893},
	doi = {10.1145/1179849.1179893},
	abstract = {The sense of touch is not only informative: certain kinds of touch are directly related to emotions. Haptics can enrich the experience of broadcast media through tactile stimulus that is mapped to emotional response and distributed over the body. This sketch applies affective touch research to haptic broadcast in a wearable device that can record, distribute and play back touch information. {TouchCasting} augments broadcast media with affective haptics that can be experienced in public as a new form of art.},
	booktitle = {{ACM} {SIGGRAPH} 2006 Sketches},
	publisher = {{ACM}},
	author = {Leonardo Bonanni and Cati Vaucelle},
	year = {2006},
	pages = {35}
},

@inproceedings{bonanni_taptap:_2006,
	address = {Montr{\textbackslash}'{e}al, Qu{\textbackslash}'{e}bec, Canada},
	title = {{TapTap:} a haptic wearable for asynchronous distributed touch therapy},
	isbn = {1-59593-298-4},
	shorttitle = {{TapTap}},
	url = {http://dx.doi.org/10.1145/1125451.1125573},
	abstract = {{TapTap} is a wearable haptic system that allows nurturing human touch to be recorded, broadcast and played back for emotional therapy. Haptic input/output modules in a convenient modular scarf provide affectionate touch that can be personalized. We present a working prototype informed by a pilot study.},
	booktitle = {{CHI} '06: {CHI} '06 extended abstracts on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Leonardo Bonanni and Cati Vaucelle and Jeff Lieberman and Orit Zuckerman},
	year = {2006},
	keywords = {haptic, Touch, wearable},
	pages = {585, 580}
},

@inproceedings{bonanni_playpals:_2006,
	address = {Montréal, Québec, Canada},
	title = {{PlayPals:} tangible interfaces for remote communication and play},
	isbn = {1-59593-298-4},
	shorttitle = {{PlayPals}},
	url = {http://portal.acm.org/citation.cfm?id=1125572},
	doi = {10.1145/1125451.1125572},
	abstract = {{PlayPals} are a set of wireless figurines with their electronic accessories that provide children with a playful way to communicate between remote locations. {PlayPals} is designed for children aged 5-8 to share multimedia experiences and virtual co-presence. We learned from our pilot study that embedding digital communication into existing play pattern enhances both remote play and communication.},
	booktitle = {{CHI} '06 extended abstracts on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Leonardo Bonanni and Cati Vaucelle and Jeff Lieberman and Orit Zuckerman},
	year = {2006},
	keywords = {{CHILDREN,} remote play, tangible interfaces, toys},
	pages = {574--579}
},

@inproceedings{teh_poultry.internet:_2006,
	address = {Montréal, Québec, Canada},
	title = {{Poultry.Internet:} a remote human-pet interaction system},
	isbn = {1-59593-298-4},
	shorttitle = {{Poultry.Internet}},
	url = {http://portal.acm.org/citation.cfm?id=1125451.1125505&coll=Portal&dl=GUIDE&CFID=61920281&CFTOKEN=18747542},
	doi = {10.1145/1125451.1125505},
	abstract = {{Poultry.Internet} leverages on the reach of the Internet to connect humans and pets at different locations. This system has a tangible interface encompassing both visual and tactile modes of communication. It allows humans to interact remotely with pets anytime, anywhere. The pet owner views the real time movement of the pet in the form of a pet doll sitting on a mechanical positioning system. Meanwhile, the real pet wears a special jacket, which is able to reproduce the touching sensation. The pet owner can tangibly touch the pet doll, sending touch signals to the pet far away. Also, the pet owner receives a haptic feedback from the movement of the pet.},
	booktitle = {{CHI} '06 extended abstracts on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Keng Soon Teh and Shang Ping Lee and Adrian David Cheok},
	year = {2006},
	keywords = {haptics, human-pet interaction, tangible interaction, wearable computing},
	pages = {251--254}
},

@inproceedings{chung_lovers_2006,
	address = {Montréal, Québec, Canada},
	title = {Lover's cups: drinking interfaces as new communication channels},
	isbn = {1-59593-298-4},
	shorttitle = {Lover's cups},
	url = {http://portal.acm.org/citation.cfm?id=1125451.1125532},
	doi = {10.1145/1125451.1125532},
	abstract = {This paper shows how computer interfaces can enhance common activities and use them as communication method between people. In this paper, the act of drinking is used as an input of remote communication with the support of computer interfaces. We present Lover's Cups which enable people to share the time of drinking with someone they care about in different places. Using a wireless connection, an otherwise ordinary pair of cups becomes a communication device, amplifying the social aspect of drinking behavior.},
	booktitle = {{CHI} '06 extended abstracts on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Hyemin Chung and {Chia-Hsun} Jackie Lee and Ted Selker},
	year = {2006},
	keywords = {ambient media, Communication, drinking interfaces, implicit interaction},
	pages = {375--380}
},

@inproceedings{motamedi_keep_2007,
	address = {Baton Rouge, Louisiana},
	title = {Keep in touch: a tactile-vision intimate interface},
	isbn = {978-1-59593-619-6},
	shorttitle = {Keep in touch},
	url = {http://portal.acm.org/citation.cfm?id=1226969.1226974},
	doi = {10.1145/1226969.1226974},
	abstract = {We present an overview of Keep in Touch, a networked fabric touchscreen designed to support and maintain intimacy for couples in long distance relationships. To achieve this, a novel sensorial interface was created by combining the visual and tactile senses together. Each partner is presented with a blurred digital projection of their lover. When they touch their partner's body, the image comes into focus revealing their features. We describe how this sensory mapping creates an expressive and emotional interface allowing couples to communicate through touch, gestures, and body language.},
	booktitle = {Proceedings of the 1st international conference on Tangible and embedded interaction},
	publisher = {{ACM}},
	author = {Nima Motamedi},
	year = {2007},
	keywords = {intimacy, sensorial interfaces, sensory mapping, tactile},
	pages = {21--22}
},

@inproceedings{eichhorn_stroking_2008,
	address = {Amsterdam, The Netherlands},
	title = {A stroking device for spatially separated couples},
	isbn = {978-1-59593-952-4},
	url = {http://portal.acm.org/citation.cfm?id=1409274},
	doi = {10.1145/1409240.1409274},
	abstract = {In this paper we present a device to support the communication of couples in long-distance relationships. While a synchronous exchange of factual information over distance is supported by telephone, e-mail and chat-systems, the transmission of nonverbal aspects of communication is still unsatisfactory. Videocalls let us see the partners' facial expression in real time. However, to experience a more intimate conversation physical closeness is needed. Stroking while holding hands is a special and emotional gesture for couples. Hence, we developed a device that enables couples to exchange the physical gesture of stroking regardless of distance and location. The device allows both sending and receiving. A user test supported our concept and provided new insights for future development.},
	booktitle = {Proceedings of the 10th international conference on Human computer interaction with mobile devices and services},
	publisher = {{ACM}},
	author = {Elisabeth Eichhorn and Reto Wettach and Eva Hornecker},
	year = {2008},
	keywords = {force-feedback, intimacy, mobile, physical presence, tangible interface},
	pages = {303--306}
},

@inproceedings{hoffmann_nurturing_2007,
	address = {San Jose, {CA,} {USA}},
	title = {On nurturing strong-tie distant relationships: from theory to prototype},
	isbn = {978-1-59593-642-4},
	shorttitle = {On nurturing strong-tie distant relationships},
	url = {http://portal.acm.org/citation.cfm?id=1241016},
	doi = {10.1145/1240866.1241016},
	abstract = {This paper presents our research on the process of creating new communication experiences in the private sphere of users. To do this, we have chosen to study the concept of social presence, and also the notion of communication. The communication models that we have examined come from various disciplines and all have in common the fact that they focus on exploring the communication act once the decision to communicate has been taken. We have build upon them and conceived a model that includes an analysis of the factors that influence the decision to communicate. This initial analysis has helped us integrate the specific needs of our targeted users and has allowed us to materialize them in a working prototype of a communication terminal called {TACT.}},
	booktitle = {{CHI} '07 extended abstracts on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Cristina Hoffmann and Sylvie Jumpertz and Bernard Marquet},
	year = {2007},
	keywords = {emotion and affective user interface, home, interaction design, tactile \& haptic uis, user-centered design},
	pages = {2411--2416}
},

@inproceedings{werner_united-pulse:_2008,
	address = {Amsterdam, The Netherlands},
	title = {United-pulse: feeling your partner's pulse},
	isbn = {978-1-59593-952-4},
	shorttitle = {United-pulse},
	url = {http://portal.acm.org/citation.cfm?id=1409240.1409338},
	doi = {10.1145/1409240.1409338},
	abstract = {This paper introduces a device that creates remote intimacy by the use of two rings named "united-pulse". Each ring can measure the wearer's heartbeat and send it to the partner's ring. Hereby, artificial corporeality is created between the couple. By means of a working prototype, united-pulse has been successfully tested. Among the 28 participants the prototype has attracted large interest. Through the heartbeat -- the essential vital sign -- a feeling of being very close to the partner is provided. Touching the ring allows a small moment of intimacy in situations where emotional support is needed.},
	booktitle = {Proceedings of the 10th international conference on Human computer interaction with mobile devices and services},
	publisher = {{ACM}},
	author = {Julia Werner and Reto Wettach and Eva Hornecker},
	year = {2008},
	keywords = {body-data, design, digital jewelry, human-machine-interaction, remote intimacy, tangible, user experience, wearable},
	pages = {535--538}
},

@inproceedings{ur_rehman_manifold_2007,
	title = {Manifold of Facial Expressions for Tactile Perception},
	doi = {10.1109/MMSP.2007.4412862},
	abstract = {To enhance their social interactive ability, we study how to provide the sight-impaired with reliable, '"on-line" emotion information. The technical challenge we address here is how to render rich facial expressions in an intuitive way. We demonstrate that manifold of facial expressions is a compact and natural way to characterize human emotions. To compute manifold of facial expressions the standard locally linear embedding {(LLE)} algorithm is extended to handle the problem of real-time coding of new videos. Experimental results show that using manifold of facial expressions for vibrotactile rendering is very encouraging.},
	booktitle = {Multimedia Signal Processing, 2007. {MMSP} 2007. {IEEE} 9th Workshop on},
	author = {S. ur Rehman and Li Liu and Haibo Li},
	year = {2007},
	keywords = {emotion recognition, face recognition, facial expressions, human emotions, social interactive ability, standard locally linear embedding algorithm, tactile perception, vibrotactile rendering, video coding},
	pages = {239--242}
},

@inproceedings{jongeun_cha_hugme:_2008,
	title = {{HugMe:} An interpersonal haptic communication system},
	shorttitle = {{HugMe}},
	doi = {10.1109/HAVE.2008.4685306},
	abstract = {Traditional teleconferencing multimedia systems have been limited to audio and video information. However, human touch, in the form of handshake, encouraging pat, comforting hug, among other physical contacts, is fundamental to physical and emotional development between persons. This paper presents the motivation and design of a synchronous haptic teleconferencing system with touch interaction to convey affection and nurture. We present a preliminary prototype for an interpersonal haptic communication system called {HugMe.} Examples of potential applications for {HugMe} include the domains of physical and/or emotional therapy, understaffed hospitals, remote children caring and distant loverspsila communication. This paper is submitted for demonstration.},
	booktitle = {Haptic Audio visual Environments and Games, 2008. {HAVE} 2008. {IEEE} International Workshop on},
	author = {Jongeun Cha and M. Eid and L. Rahal and A. El Saddik},
	year = {2008},
	keywords = {Haptic communication, haptic interfaces, haptics, {HugMe,} interpersonal haptic communication system, Remote interpersonal communication, synchronous haptic teleconferencing system, teleconferencing},
	pages = {99--102}
},

@inproceedings{he_designing_2009,
	address = {Boston, {MA,} {USA}},
	title = {Designing a wearable social network},
	isbn = {978-1-60558-247-4},
	url = {http://portal.acm.org/citation.cfm?id=1520485},
	doi = {10.1145/1520340.1520485},
	abstract = {This paper presents a framework and design for a wearable social network based on Facebook. We begin with a discussion of social networking by isolating key characteristics of social interactions in three research areas: Social Networking Sites, Mobile Computing, and Wearable Computing. These characteristics are analyzed to suggest a design framework that can be applied to the design of social networks. Using this framework, we have designed and created a wearable social network called Patches, which extends the social interactions available in most wearable devices today.},
	booktitle = {Proceedings of the 27th international conference extended abstracts on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Yin He and Thecla Schiphorst},
	year = {2009},
	keywords = {interaction, social networking, wearable computing},
	pages = {3353--3358}
},

@article{graham-rowe_cutting_2006,
	title = {The Cutting Edge of Haptics: Touch-based interfaces get to the point},
	journal = {Technology Review},
	author = {Duncan {Graham-Rowe}},
	month = aug,
	year = {2006}
},

@article{alhalabi_experimental_2003,
	title = {An experimental study on the effects of Network delay in Cooperative Shared Haptic Virtual Environment},
	volume = {27},
	issn = {0097-8493},
	url = {http://www.sciencedirect.com/science/article/B6TYG-47XWHCR-4/2/125bd6eb5203f79455173508d445d18a},
	doi = {10.1016/S0097-8493(02)00277-7},
	abstract = {A cooperative shared haptic virtual environment {(CSHVE),} where the users can kinesthetically interact and simultaneously feel each other over the network, is beneficial for many distributed {VR} simulations. A little is known about the influences of the network delay on the quality of haptic sensation and the task performance in such environments. This paper has addressed these issues by conducting a subjective evaluation to the force feedback and the task performance in a tele-handshake cooperative shared haptic system for different delay setting. Also, four subjective measures to evaluate the quality of haptic in {CSHVEs} have been proposed. These measures are the feeling of force, the consistency between the haptic-visual feedback, the vibration, and the rebound in the haptic device. In addition, a detailed description of the haptic sensation for different time delays is also described. A network emulator was utilized to simulate the real network cloud. An objective evaluation of the force feedback and the performance showed that there was no effect of the delay on the force feedback. It had a negative impact on the task performance. In general, the quality of haptic deteriorated as the delay increased and vibration and rebound hampered the users for large time delay. The haptic-visual consistency was robust in the presented system even for large time delays. Nevertheless, the examined tele-handshake system was able to deliver a high quality of haptic sensation, good performance, and stability for large time delay over the network.},
	number = {2},
	journal = {Computers \& Graphics},
	author = {M. Osama Alhalabi and Susumu Horiguchi and Susumu Kunifuji},
	month = apr,
	year = {2003},
	keywords = {haptic, Network latency, Quality of haptic, Shared virtual environment, Task performance},
	pages = {205--213}
},

@book{hargie_social_1994,
	edition = {3},
	title = {Social Skills in Interpersonal Communication},
	isbn = {0415081378},
	publisher = {Routledge},
	author = {Owen Hargie},
	month = jun,
	year = {1994}
},

@book{walsh_person-environment_2000,
	title = {Person-environment psychology},
	isbn = {0805824715, 9780805824711},
	publisher = {Routledge},
	author = {W. Bruce Walsh and Kenneth H. Craik and Richard H. Price},
	year = {2000}
},

@article{kenrick_ambient_1986,
	title = {Ambient Temperature and Horn Honking: A Field Study of the {Heat/Aggression} Relationship},
	volume = {18},
	shorttitle = {Ambient Temperature and Horn Honking},
	url = {http://eab.sagepub.com/cgi/content/abstract/18/2/179},
	doi = {10.1177/0013916586182002},
	abstract = {Using a method developed in previous field studies of aggression, this study examined the influence of ambient temperature on responses to a car stopped at a green light. To investigate alternative models of the effects of high temperature on interpersonal hostility, the study was conducted during the spring and summer in Phoenix, Arizona, and included a range on the temperaturehumidity discomfort index up to 1 160. Results indicated a direct linear increase in horn honking with increasing temperature. Stronger results were obtained by examining only those subjects who had their windows rolled down (and presumably did not have air conditioners operating).},
	number = {2},
	journal = {Environment and Behavior},
	author = {Douglas T. Kenrick and Steven W. {MacFarlane}},
	month = mar,
	year = {1986},
	pages = {179--191}
},

@article{berry_attractive_1991,
	title = {Attractive Faces Are not all Created Equal: Joint Effects of Facial Babyishness and Attractiveness on Social Perception},
	volume = {17},
	shorttitle = {Attractive Faces Are not all Created Equal},
	url = {http://psp.sagepub.com/cgi/content/abstract/17/5/523},
	doi = {10.1177/0146167291175007},
	abstract = {Subjects provided their impressions of stimulus faces that systematically varied in attractiveness and babyishness. The results indicate that variations in facial babyishness can qualify the effects of attractiveness on social perception. For example facially attractive people are thought to be more honest, warm, and sincere than average when facial babyishness is high but not when it is low. The data are consistent with the proposal that there are different types of facial attractiveness that yield different impressions. The data also revealed that the effects of facial babyishness on impressions can be modulated by variations in attractiveness. The implications of these data for our under-standing of the effects of facial appearance on social perceptions are discussed.},
	number = {5},
	journal = {Pers Soc Psychol Bull},
	author = {Diane S. Berry},
	month = oct,
	year = {1991},
	pages = {523--531}
},

@book{helen_h._jennings_sociometry_1959,
	address = {{(Washington)}},
	edition = {105 p.},
	title = {Sociometry in group relations},
	url = {http://openlibrary.org/b/OL6270558M/Sociometry_in_group_relations},
	abstract = {a manual for teachers.},
	publisher = {American Council on Education},
	author = {Helen H. Jennings},
	year = {1959},
	keywords = {Borrow, Browse, Buy, Data comes from Amazon,, Library of Congress, and users like you., Not available for this book., Your local library}
},

@book{zebrowitz_reading_1997,
	address = {Boulder {CO}},
	title = {Reading Faces},
	publisher = {Westview Press},
	author = {L. A. Zebrowitz},
	year = {1997}
},

@article{berry_perceiving_1986,
	title = {Perceiving character in faces: the impact of age-related craniofacial changes on social perception},
	volume = {100},
	issn = {0033-2909},
	shorttitle = {Perceiving character in faces},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/3526376},
	number = {1},
	journal = {Psychological Bulletin},
	author = {D S Berry and L Z {McArthur}},
	month = jul,
	year = {1986},
	note = {{PMID:} 3526376},
	keywords = {Adolescent, Adult, Age Factors, Aged, Beauty, Character, Child, Child, Preschool, Chin, Eye, Face, Forehead, Humans, Middle Aged, Skin, Skull, Social Perception},
	pages = {3--18}
},

@article{cortes_physique_1965,
	title = {Physique and self-description of temperament},
	volume = {29},
	issn = {0095-8891},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/5827516},
	number = {5},
	journal = {Journal of Consulting Psychology},
	author = {J B Cortés and F M Gatti},
	month = oct,
	year = {1965},
	note = {{PMID:} 5827516},
	keywords = {Adolescent, Adult, Body Constitution, Female, Humans, Male, Psychological Tests, Self Concept, Somatotypes},
	pages = {432--439}
},

@article{tucker_physical_1984,
	title = {Physical Attractiveness, Somatotype, and the Male Personality: A Dynamic Interactional Perspective.},
	volume = {40},
	shorttitle = {Physical Attractiveness, Somatotype, and the Male Personality},
	number = {5},
	journal = {Journal of Clinical Psychology},
	author = {Larry A. Tucker},
	year = {1984},
	keywords = {Physical Attractiveness, Somatotyping},
	pages = {1226--34},
	annote = {Determined whether measures of personality, considered compositely and individually, differ significantly among groups of college males {(N=285)} differentiated according to subjective-perception of attractiveness. Results indicated that self-perceived mesomorphs manifested psychological qualities that were significantly more favorable than those of their counterparts. {(LLL)}}
},

@article{cameron_courtship_1977,
	title = {Courtship American Style: Newspaper Ads},
	volume = {26},
	issn = {00147214},
	shorttitle = {Courtship American Style},
	url = {http://www.jstor.org/stable/581857},
	abstract = {This study investigated an increasing social phenomenon--newspaper advertising for dating or marital partners--in terms of the bargaining process involved. Content analysis of personal ads in a popular "respectable" singles newspaper revealed a pattern of offers and requests reminiscent of a heterosexual stock market. Exchange theory provided a theoretical context for analyzing the personal ads. Findings confirmed expectations that advertisers sought to maximize their profit by presenting a positive image of themselves. In addition, traditionally sex-appropriate characteristics were claimed and desired, suggesting that shifting sex role expectations are not evident in this type of mate-selection.},
	number = {1},
	journal = {The Family Coordinator},
	author = {Catherine Cameron and Stuart Oskamp and William Sparks},
	month = jan,
	year = {1977},
	note = {{ArticleType:} primary\_article / Full publication date: Jan., 1977 / Copyright © 1977 National Council on Family Relations},
	pages = {27--30}
},

@article{ogden_prevalence_2002,
	title = {Prevalence and Trends in Overweight Among {US} Children and Adolescents, 1999-2000},
	volume = {288},
	url = {http://jama.ama-assn.org/cgi/content/abstract/288/14/1728},
	doi = {10.1001/jama.288.14.1728},
	abstract = {Context The prevalence of overweight among children in the United States increased between 1976-1980 and 1988-1994, but estimates for the current decade are unknown. Objective To determine the prevalence of overweight in {US} children using the most recent national data with measured weights and heights and to examine trends in overweight prevalence. Design, Setting, and Participants Survey of 4722 children from birth through 19 years of age with weight and height measurements obtained in 1999-2000 as part of the National Health and Nutrition Examination Survey {(NHANES),} a cross-sectional, stratified, multistage probability sample of the {US} population. Main Outcome Measure Prevalence of overweight among {US} children by sex, age group, and race/ethnicity. Overweight among those aged 2 through 19 years was defined as at or above the 95th percentile of the sex-specific body mass index {(BMI)} for age growth charts. Results The prevalence of overweight was 15.5\% among 12- through 19-year-olds, 15.3\% among 6- through 11-year-olds, and 10.4\% among 2- through 5-year-olds, compared with 10.5\%, 11.3\%, and 7.2\%, respectively, in 1988-1994 {(NHANES} {III).} The prevalence of overweight among {non-Hispanic} black and {Mexican-American} adolescents increased more than 10 percentage points between 1988-1994 and 1999-2000. Conclusion The prevalence of overweight among children in the United States is continuing to increase, especially among {Mexican-American} and {non-Hispanic} black adolescents.},
	number = {14},
	journal = {{JAMA}},
	author = {Cynthia L. Ogden and Katherine M. Flegal and Margaret D. Carroll and Clifford L. Johnson},
	month = oct,
	year = {2002},
	pages = {1728--1732}
},

@book{griffin_black_1996,
	edition = {35th anniversary},
	title = {Black Like Me},
	isbn = {0451192036},
	publisher = {Signet},
	author = {John Howard Griffin and Robert Bonazzi and John Howard Griffin and Robert Bonazzi},
	month = nov,
	year = {1996}
},

@article{lord_identification_1989,
	title = {Identification of self through olfaction.},
	volume = {69},
	issn = {0031-5125},
	url = {http://view.ncbi.nlm.nih.gov/pubmed/2780181},
	abstract = {To study olfactory communication in humans, 100 undergraduates ranging from age 18 to 45 yr. wore freshly washed, identical T-shirts continuously for 24 hr. During this time, the participants did not bathe or shower or apply any scent producing substance to their bodies, i.e., deodorants, perfumes. Upon retrieval, each shirt was placed in an identical brown bag. In groups of 10, each participant attempted to identify the T-shirt he had worn the previous 24 hr. The task was administered separately for each individual so that no participants knew the choice by a previous colleague. Analysis shows that the participants were able to identify correctly their own shirts on the first try three-quarters of the time. Furthermore, sex, age, smoking habit of the participant, and menstrual cycle phase in the women were factors in successful outcomes.},
	number = {1},
	journal = {Perceptual and motor skills},
	author = {T Lord and M Kasprzak},
	year = {1989},
	keywords = {self, smell},
	pages = {224, 219}
},

@article{porter_olfaction_1998,
	title = {Olfaction and human kin recognition},
	volume = {104},
	url = {http://dx.doi.org/10.1023/A:1026404319384},
	doi = {10.1023/A:1026404319384},
	abstract = {Abstract  Humans, like other mammals, are capable of discriminating between kin and non?kin by olfactory cues alone. Shortly after birth,
breastfed infants become familiar with, and respond preferentially to, their mother' unique odor signature. Mothers likewise
recognize the characteristic scent of their newborn infant. Close biological relatives share somewhat similar odor signatures
(presumably resulting from genetically mediated similarities in bodily biochemistry and metabolism) that could facilitate
kin recognition.},
	number = {3},
	journal = {Genetica},
	author = {Richard Porter},
	month = dec,
	year = {1998},
	pages = {259--263}
},

@article{russell_human_1976,
	title = {Human olfactory communication},
	volume = {260},
	url = {http://dx.doi.org/10.1038/260520a0},
	doi = {10.1038/260520a0},
	number = {5551},
	journal = {Nature},
	author = {{MICHAEL} J. {RUSSELL}},
	month = apr,
	year = {1976},
	pages = {520--522}
},

@article{barber_mustache_2001,
	title = {Mustache Fashion Covaries with a Good Marriage Market for Women},
	volume = {25},
	url = {http://dx.doi.org/10.1023/A:1012515505895},
	doi = {10.1023/A:1012515505895},
	abstract = {It was predicted that men would emphasize sexually-selected traits, including mustaches, beards, and sideburns, when they have difficulty obtaining spouses. Using annual data on British beard fashions extending from 1842–1971, it was found that mustaches, and facial hair in general, are more frequent when there is a good supply of single men of marriageable age. Facial hair fashions, particularly mustaches and beards, were reduced when illegitimacy ratios were high. Regression analyses showed that the relationship between mustache fashion and the marriage market and illegitimacy, respectively, is independent of linear time trend. Results suggest that facial hair is worn to enhance a man's marriage prospects by increasing physical attractiveness and perception of social status. Men shave their mustaches, possibly to convey an impression of trustworthiness, when the marriage market is weak and women might fear sexual exploitation and desertion.},
	number = {4},
	journal = {Journal of Nonverbal Behavior},
	author = {Nigel Barber},
	month = dec,
	year = {2001},
	pages = {261--272}
},

@article{johnson_clothing_1977,
	title = {Clothing Style Differences: Their Effect on the Impression of Sociability},
	volume = {6},
	shorttitle = {Clothing Style Differences},
	url = {http://fcs.sagepub.com/cgi/content/abstract/6/1/58},
	doi = {10.1177/1077727X7700600107},
	abstract = {This research examines the effect of clothing style differences on the formation of the impression of sociability. A two-by-four factorial experiment was designed to measure the effect of four costumes on the impressions of sociability formed by males and females of a female peer. The sample included 60 male and 60 female college students, from which 15 males and 15 females were assigned randomly to view each of the experimental costumes and to respond on an Impression Measure. Photographs of a female college student wearing two in-fashion costumes and two out-of-fashion costumes were used to determine the degree of sociability attributed to her when she wore different clothing styles. Analysis of variance and the eta squared correlation ratio were used to determine the type and strength of the relationships within the data. Both male and female college students evalu ated a female peer when she wore in-fashion clothing as being more sociable than when she wore out-of-fashion clothing. The effect of clothing style on the impression of sociability was found to be statistically significant and conceptually important. {(Home} Economics Research Journal, September 1977, Vol. 6, No. 1)},
	number = {1},
	journal = {Family and Consumer Sciences Research Journal},
	author = {Barbara Hunt Johnson and Richard H. Nagasawa and Kathleen Peters},
	month = sep,
	year = {1977},
	pages = {58--63}
},

@article{hensley_effects_1981,
	title = {The effects of attire, location, and sex on aiding behavior: A similarity explanation},
	volume = {6},
	shorttitle = {The effects of attire, location, and sex on aiding behavior},
	url = {http://dx.doi.org/10.1007/BF00987932},
	doi = {10.1007/BF00987932},
	abstract = {This study examined two distinct theoretical explanations for the effect of attire on aiding behavior. The reinforcing value of attire itself predicted that well-dressed persons would always receive more assistance than poorly dressed persons. The reinforcing value of perceived similarity predicted that similar persons would receive more assistance than dissimilar persons. Female confederates {(N=17)} who were well-dressed or poorly dressed approached males and females in an airport or bus station asking for a dime to complete a telephone call. Well-dressed confederates received more money at the airport; poorly dressed confederates received more money at the bus station. These results were interpreted as support for the similarity hypothesis.},
	number = {1},
	journal = {Journal of Nonverbal Behavior},
	author = {Wayne E. Hensley},
	year = {1981},
	pages = {3--11}
},

@book{joseph_uniforms_1986,
	title = {Uniforms and Nonuniforms: Communication Through Clothing},
	isbn = {0313251959},
	shorttitle = {Uniforms and Nonuniforms},
	publisher = {Greenwood Press},
	author = {Nathan Joseph},
	month = nov,
	year = {1986}
},

@article{rosenfeld_clothing_1977,
	title = {Clothing as communication},
	volume = {27},
	journal = {Journal of Communication},
	author = {T. L. Rosenfeld and T. G. Plax},
	pages = {24--31}
	year = {1977}
},

@book{sanders_customizing_2008,
	title = {Customizing the Body: The Art and Culture of Tattooing},
	isbn = {1592138888},
	shorttitle = {Customizing the Body},
	publisher = {Temple University Press},
	author = {Clinton Sanders and D Angus Vail},
	month = mar,
	year = {2008}
},

@book{krupat_people_1985,
	title = {People in Cities: The Urban Environment and its Effects},
	isbn = {0521319463},
	shorttitle = {People in Cities},
	publisher = {Cambridge University Press},
	author = {Edward Krupat},
	month = sep,
	year = {1985}
},

@book{sommer_personal_1969,
	edition = {6th Printing},
	title = {Personal Space: The Behavioral Basis of Design},
	isbn = {0136575773},
	shorttitle = {Personal Space},
	publisher = {Prentice Hall Trade},
	author = {Robert Sommer},
	month = jun,
	year = {1969}
},

@book{sommer_tight_1974,
	title = {Tight spaces; hard architecture and how to humanize it},
	isbn = {0139213384},
	publisher = {{Prentice-Hall}},
	author = {Robert Sommer},
	year = {1974}
},

@article{schauss_psysiological_1985,
	title = {The psysiological effect of color on the suppression of human aggression},
	volume = {7},
	journal = {International Journal of Biosocial Research},
	author = {{A.G.} Schauss},
	year = {1985},
	pages = {55--64}
},

@article{bottomley_interactive_2006,
	title = {The interactive effects of colors and products on perceptions of brand logo appropriateness},
	volume = {6},
	url = {http://mtq.sagepub.com/cgi/content/abstract/6/1/63},
	doi = {10.1177/1470593106061263},
	abstract = {This article explores the role that color can play in building brand meaning with two experiments. Without prior conditioning, we demonstrate how an appropriately chosen color for a brand name (logo) can bring inherent and immediate value to a brand. Experiment 1 explores the notion of congruity, showing that it is more appropriate for functional products to be presented in functional colors, and sensorysocial products in sensory-social colors. Experiment 2 examines the effect of red and blue on brands of products that can be classified as both functional and sensory-social, and the ability of color to enhance a brand's desired image. When people know how brands are attempting to position themselves, people consider colors congruent with those positions to be more appropriate.},
	number = {1},
	journal = {Marketing Theory},
	author = {Paul A. Bottomley and John R. Doyle},
	month = mar,
	year = {2006},
	pages = {63--83}
},

@book{manusov_attribution_2001,
	edition = {1},
	title = {Attribution, Communication Behavior, and Close Relationships},
	isbn = {0521770890},
	publisher = {Cambridge University Press},
	author = {Valerie Manusov and John H. Harvey},
	month = jan,
	year = {2001}
},

@article{north_-store_1997,
	title = {In-store music affects product choice},
	volume = {390},
	issn = {0028-0836},
	url = {http://dx.doi.org/10.1038/36484},
	doi = {10.1038/36484},
	number = {6656},
	journal = {Nature},
	author = {Adrian C. North and David J. Hargreaves and Jennifer {McKendrick}},
	month = nov,
	year = {1997},
	pages = {132}
},

@article{meer_light_1985,
	title = {The light touch},
	volume = {19},
	journal = {Psychology Today},
	author = {J. Meer},
	year = {1985},
	pages = {60--67}
},

@book{moos_human_1985,
	title = {The Human Context: Environmental Determinants of Behavior},
	isbn = {0898746795},
	shorttitle = {The Human Context},
	publisher = {Krieger Pub Co},
	author = {Rudolf H. Moos},
	month = jun,
	year = {1985}
},

@article{farrenkopf_university_1980,
	title = {The University Faculty Office as an Environment.},
	volume = {12},
	number = {4},
	journal = {Environment and Behavior},
	author = {Toni Farrenkopf and Vicki Roth},
	month = dec,
	year = {1980},
	keywords = {California State Polytechnic University Pomona, Design Research, Post Occupancy Evaluation},
	pages = {467--77},
	annote = {Describes results of a study of the work activities, priorities, and degree of satisfaction with the office environment of faculty at a California university. Implications for office design are discussed. {(Author/WB)}}
},

@misc{_chronomics_2005,
	type = {Text},
	title = {Chronomics of autism and suicide},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2576472/},
	month = oct,
	year = {2005},
	howpublished = {{http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2576472/}}
},

@misc{ekman_nonverbal_1976,
	title = {Nonverbal Communication: Movements with Precise Meanings},
	shorttitle = {Nonverbal Communication},
	author = {Paul Ekman},
	year = {1976},
	keywords = {Emblems},
	annote = {Defines emblems or symbolic gestures, distinguishes them from illustrators and discusses a recently developed procedure for surveying the emblem repertoire in various cultures. {(MH)}}
},

@book{wagner_field_2003,
	title = {Field Guide to Gestures: How to Identify and Interpret Virtually Every Gesture Known to Man},
	isbn = {1931686203},
	shorttitle = {Field Guide to Gestures},
	publisher = {Quirk Books},
	author = {Melissa Wagner and Nancy Armstrong},
	month = jul,
	year = {2003}
},

@book{efron_gesture_1972,
	title = {Gesture, Race and Culture},
	isbn = {9027921121},
	publisher = {Walter de Gruyter, Inc.},
	author = {David Efron},
	month = oct,
	year = {1972}
},

@article{weisfeld_erectness_1982,
	title = {Erectness of posture as an indicator of dominance or success in humans},
	volume = {6},
	url = {http://dx.doi.org/10.1007/BF00992459},
	doi = {10.1007/BF00992459},
	abstract = {Darwin and others have perceived parallels between the erect bearing of proud, successful humans and the expansive demeanor of dominant animals. Various additional parallels between man and other primates in the characteristics of dominance hierarchies, the facial and postural expression of dominance, and its possible neural mediation are described. In the present study, success by human subjects in various situations was found to be reflected by erect posture. In a longitudinal study, boys who had been ranked by peers as “tough,” or dominant in agonistic encounters, in early grade school were observed to have erect posture in high school. Further, high school students who were judged by peers as successful by group standards tended to have erect posture. Finally, erectness of posture was related to performance on a college examination, with students' posture changing in erectness upon their receiving their grade. These results are consistent with the hypothesis that human competition for social success is based upon a biological capacity for dominance hierarchization.},
	number = {2},
	journal = {Motivation and Emotion},
	author = {Glenn E. Weisfeld and Jody M. Beresford},
	month = jun,
	year = {1982},
	pages = {113--131}
},

@article{grant_comparison_1963,
	title = {A Comparison of the Social Postures of Some Common Laboratory Rodents},
	volume = {21},
	issn = {00057959},
	url = {http://www.jstor.org/stable/4533055},
	abstract = {This paper describes elements in the social behaviour of the laboratory rat, mouse, hamster and Guinea-pig. These elements are divided into postures, which are static, and acts, which involve movement. A total of 45 of these elements are mentioned, most of which are common, with only slight modification, to all four species. Apart from these the guinea pig differs in not having a true Upright Posture and also in showing a male sexual display {"Rumba".} The postures are classified under broad motivational headings. A number of general concepts are discussed, for example the relation of convulsions to flight behaviour, the reduction of incoming aggressive stimuli in submissive postures, {"Cut-Off",} and the inhibition of biting in the more social species. /// In dieser Arbeit werden Elemente des sozialen Verhaltens der Laboratoriumsratte, -maus, vom Goldhamster und Meerschweinchen beschrieben, einerseits Haltungen, andererseits Handlungen. Insgesamt sind 45 Elemente erwähnt, die fast alle, mit nur geringen Unterschieden, allen 4 Arten gemeinsam sind. Nur hat das Meerschweinchen keine echte Aufrechthaltung, dafür aber den männlichen Werberitus {"Rumba".} Die Handlungen werden nach den zugrunde liegenden Stimmungen geordnet. Es werden einige allgemeinere Gesichtspunkte erörtert, {z.B.} die Beziehung von Krämpfen zum Fluchtverhalten, erhöhte Schwellen für Angriffsreize in der Demuthaltung (s.g. "cut off" oder {"Abbrechen")} und die Beisshemmung der Arten mit ausgeprägterem Sozialverhalten.},
	number = {3/4},
	journal = {Behaviour},
	author = {E. C. Grant and J. H. Mackintosh},
	year = {1963},
	note = {{ArticleType:} primary\_article / Full publication date: 1963 / Copyright © 1963 {BRILL}},
	pages = {246--259}
},

@article{kleinsmith_cross-cultural_2006,
	title = {Cross-cultural differences in recognizing affect from body posture},
	volume = {18},
	url = {http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6V0D-4K8S5P3-1&_user=10&_rdoc=1&_fmt=&_orig=search&_sort=d&_docanchor=&view=c&_searchStrId=1092987908&_rerunOrigin=google&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=dd14f60be26bc7b1fd72eafe1bb7f399},
	doi = {10.1016/j.intcom.2006.04.003},
	number = {6},
	journal = {Interacting with Computers},
	author = {Andrea Kleinsmith and P. Ravindra De Silva and Nadia {Bianchi-Berthouze}},
	month = dec,
	year = {2006},
	keywords = {Affective body postures, Affective communication, Embodied animated agents, Emotion nuances, Intercultural differences},
	pages = {1371--1389}
},

@article{afifi_use_1999,
	title = {The Use and Interpretation of Tie Signs in a Public Setting: Relationship and Sex Differences},
	volume = {16},
	shorttitle = {The Use and Interpretation of Tie Signs in a Public Setting},
	url = {http://spr.sagepub.com/cgi/content/abstract/16/1/9},
	doi = {10.1177/0265407599161002},
	abstract = {This investigation increases our understanding of the uses of nonverbal tie signs in cross-sex friendships and dating relationships, and addresses the role that these displays play in the experience of these two relationships. Specifically, we (i) observed the behavior of cross-sex friends and daters in college bars (study 1), (ii) asked individuals to assess the reasons why they used affection displays in college bars with either a friend of the opposite sex or a dating partner (study 2), and (iii) asked participants for their perceptions of why their dating partner or cross-sex friend uses affection displays with them when at a college bar (study 3). We examined the influence of sex and relationship type in each of these domains. The results significantly extend past research on tie signs and cross-sex friendships by incorporating observational data of tie signs with sender and receiver perceptions of the functions underlying these observed behaviors.},
	number = {1},
	journal = {Journal of Social and Personal Relationships},
	author = {Walid A. Afifi and Michelle L. Johnson},
	month = feb,
	year = {1999},
	pages = {9--38}
},

@book{montagu_touching:_1986,
	edition = {3},
	title = {Touching: The Human Significance of the Skin},
	isbn = {0060960280},
	shorttitle = {Touching},
	publisher = {Harper Paperbacks},
	author = {Ashley Montagu},
	month = sep,
	year = {1986}
},

@article{hertenstein_communicative_2006,
	title = {The communicative functions of touch in humans, nonhuman primates, and rats: a review and synthesis of the empirical research},
	volume = {132},
	issn = {8756-7547},
	shorttitle = {The communicative functions of touch in humans, nonhuman primates, and rats},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/17345871},
	abstract = {Although touch is one of the most neglected modalities of communication, several lines of research bear on the important communicative functions served by the modality. The authors highlighted the importance of touch by reviewing and synthesizing the literatures pertaining to the communicative functions served by touch among humans, nonhuman primates, and rats. In humans, the authors focused on the role that touch plays in emotional communication, attachment, bonding, compliance, power, intimacy, hedonics, and liking. In nonhuman primates, the authors examined the relations among touch and status, stress, reconciliation, sexual relations, and attachment. In rats, the authors focused on the role that touch plays in emotion, learning and memory, novelty seeking, stress, and attachment. The authors also highlighted the potential phylogenetic and ontogenetic continuities and discussed suggestions for future research.},
	number = {1},
	journal = {Genetic, Social, and General Psychology Monographs},
	author = {Matthew J Hertenstein and Julie M Verkamp and Alyssa M Kerestes and Rachel M Holmes},
	month = feb,
	year = {2006},
	note = {{PMID:} 17345871},
	keywords = {Adolescent, Adult, Animal Communication, Animals, Child, Child, Preschool, Emotions, Evolution, Female, Humans, Infant, Male, Middle Aged, Nonverbal Communication, Object Attachment, Phylogeny, Primates, Rats, Species Specificity, Touch},
	pages = {5--94}
}

@incollection{robles-de-la-torre_principles_2008,
	title = {Principles of haptic perception in virtual environments},
	url = {http://dx.doi.org/10.1007/978-3-7643-7612-3_30},
	abstract = {During haptic interaction with everyday environments, haptic perception relies on sensory signals arising from mechanical signals such as contact forces,
torques, movement of objects and limbs, mass or weight of objects, stiffness of materials, geometry of objects, etc. {(Fig.} 1a). In contrast, haptic perception in Virtual Environments {(VEs)} relies on sensory signals arising from computer-controlled mechanical signals
produced by haptic interfaces (see Fig. 1b, the online animation [1] under Selected Readings and Websites, and [1, 2]). Haptic interfaces are programmable systems, which can reproduce mechanical signals that are normally experienced when
haptically exploring real, everyday environments. Perhaps more importantly, haptic interfaces can create combinations of mechanical
signals that do not have counterparts in real environments. This allows creating haptic {VEs} in which entirely new haptic sensory experiences are possible.},
	booktitle = {Human Haptic Perception: Basics and Applications},
	author = {Gabriel {Robles-De-La-Torre}},
	year = {2008},
	pages = {363--379}
},

@article{hertenstein_touch_2006,
	title = {Touch communicates distinct emotions},
	volume = {6},
	number = {3},
	journal = {Emotion},
	author = {M. J. Hertenstein and D. Keltner and B. App and A. B. Bulleit and R. Jaskolta},
	year = {2006},
	pages = {528--533}
},

@book{brent_d._ruben_human_1975,
	address = {{(Rochelle} Park, {N.J)}},
	title = {Human communication handbook},
	isbn = {74023696},
	url = {http://openlibrary.org/b/OL5059881M/Human_communication_handbook},
	abstract = {simulations and games},
	publisher = {Hayden Book Co.},
	author = {Brent D. Ruben},
	year = {1975},
	keywords = {Abebooks, Alibris, Amazon, Barnes and Noble, {BookMooch,} Borrow, Browse, Buy, Data comes from Amazon,, Library of Congress, and users like you., Not available for this book., Powells, Title Trader, Your local library}
},

@book{brown_social_1986,
	address = {New York, {NY}},
	title = {Social Psychology},
	publisher = {Free Press},
	author = {R. Brown},
	year = {1986}
}?
@inproceedings{russell_british_2007,
	title = {British Machine Vision Conference},
	author = {David Russell and Shaogang Gong},
	year = {2007}
},

@article{yeasin_recognition_2006,
	title = {Recognition of facial expressions and measurement of levels of interest from video},
	volume = {8},
	issn = {1520-9210},
	doi = {10.1109/TMM.2006.870737},
	abstract = {This paper presents a spatio-temporal approach in recognizing six universal facial expressions from visual data and using them to compute levels of interest. The classification approach relies on a two-step strategy on the top of projected facial motion vectors obtained from video sequences of facial expressions. First a linear classification bank was applied on projected optical flow vectors and decisions made by the linear classifiers were coalesced to produce a characteristic signature for each universal facial expression. The signatures thus computed from the training data set were used to train discrete hidden Markov models {(HMMs)} to learn the underlying model for each facial expression. The performances of the proposed facial expressions recognition were computed using five fold cross-validation on {Cohn-Kanade} facial expressions database consisting of 488 video sequences that includes 97 subjects. The proposed approach achieved an average recognition rate of 90.9\% on {Cohn-Kanade} facial expressions database. Recognized facial expressions were mapped to levels of interest using the affect space and the intensity of motion around apex frame. Computed level of interest was subjectively analyzed and was found to be consistent with "ground truth" information in most of the cases. To further illustrate the efficacy of the proposed approach, and also to better understand the effects of a number of factors that are detrimental to the facial expression recognition, a number of experiments were conducted. The first empirical analysis was conducted on a database consisting of 108 facial expressions collected from {TV} broadcasts and labeled by human coders for subsequent analysis. The second experiment (emotion elicitation) was conducted on facial expressions obtained from 21 subjects by showing the subjects six different movies clips chosen in a manner to arouse spontaneous emotional reactions that would produce natural facial expressions.},
	number = {3},
	journal = {Multimedia, {IEEE} Transactions on},
	author = {M. Yeasin and B. Bullot and R. Sharma},
	year = {2006},
	keywords = {{Cohn-Kanade} facial expressions database, discrete hidden Markov model training, emotion recognition, Emotions, empirical analysis, face detection, face recognition, facial motion vector projection, hidden Markov models, hidden Markov models {(HMMs),} {HMM,} image motion analysis, image sequences, learning (artificial intelligence), levels of interest, levels of interest measurement, linear classification bank, machine learning, optical flow vector projection, pattern classification, spatio-temporal approach, universal facial expression recognition, universal facial expressions, video sequences, visual databases},
	pages = {500--508}
},

@inproceedings{ozden_background_2005,
	title = {Background Recognition in Dynamic Scenes with Motion Constraints},
	isbn = {0-7695-2372-2},
	url = {http://portal.acm.org/citation.cfm?id=1068922},
	abstract = {Consider a monocular image sequence which contains independently moving objects and assume it is already segmented. In order to get a realistic {3D} reconstruction of such a scene, we have to solve the relative scale ambiguity between the reconstructions of different moving objects. Recently, we demonstrated the usefulness of the so-called ýnon-accidentalness and independence constraintsý to disambiguate the mentioned unknown relative scale. However, this technique requires that the video segment which corresponds to the background is known beforehand. In this paper, we analyze the background detection problem in the vein of the aforementioned constraints and show that the background is not just another moving object but the one which results in the simplest overall scene interpretation.},
	publisher = {{IEEE} Computer Society},
	author = {K. E. Ozden and L. Van Gool},
	year = {2005},
	pages = {250--255}
},

@inproceedings{collins_automatic_2002,
	title = {Automatic Face and Gesture Recognition, 2002. Proceedings. Fifth {IEEE} International Conference on},
	doi = {10.1109/AFGR.2002.1004181},
	abstract = {Our goal is to establish a simple baseline method for human identification based on body shape and gait. This baseline recognition method provides a lower bound against which to evaluate more complicated procedures. We present a viewpoint-dependent technique based on template matching of body silhouettes. Cyclic gait analysis is performed to extract key frames from a test sequence. These frames are compared to training frames using normalized correlation, and subject classification is performed by nearest-neighbor matching among correlation scores. The approach implicitly captures biometric shape cues such as body height, width, and body-part proportions, as well as gait cues such as stride length and amount of arm swing. We evaluate the method on four databases with varying viewing angles, background conditions (indoors and outdoors), walking styles and pixels on target.},
	author = {{R.T.} Collins and R. Gross and Jianbo Shi},
	year = {2002},
	keywords = {arm swing, background conditions, baseline recognition method, biometric shape cues, biometrics (access control), body height, body shape, body width, body-part proportions, computer vision, correlation methods, correlation scores, cyclic gait analysis, databases, gait analysis, gait cues, image classification, image matching, image motion analysis, image sequences, indoor conditions, key frame extraction, lower bound, nearest-neighbor matching, normalized correlation, outdoor conditions, pixels, shape measurement, silhouette-based human identification, stride length, subject classification, template matching, test image sequence, training frames, video databases, viewing angles, viewpoint-dependent technique, walking styles},
	pages = {366--371}
},

@inproceedings{yijun_xiao_topological_2003,
	title = {A topological approach for segmenting human body shape},
	doi = {10.1109/ICIAP.2003.1234029},
	abstract = {Segmentation of a {3D} human body, is a very challenging problem in applications exploiting human scan data. To tackle this problem, the paper proposes a topological approach based on the discrete Reeb graph {(DRG)} which is an extension of the classical Reeb graph to handle unorganized clouds of {3D} points. The essence of the approach concerns detecting critical nodes in the {DRG,} thereby permitting the extraction of branches that represent parts of the body. Because the human body shape representation is built upon global topological features that are preserved so long as the whole structure of the human body does not change, our approach is quite robust against noise, holes, irregular sampling, frame change and posture variation. Experimental results performed on real scan data demonstrate the validity of our method.},
	author = {Yijun Xiao and N. Werghi and P. Siebert},
	year = {2003},
	keywords = {{3D} human body image segmentation, discrete Reeb graph, frame change, global topological features, graph theory, holes, human body shape segmentation, human scan data, image sampling, image segmentation, imaging, irregular sampling, noise, posture variation},
	pages = {82--87}
},

@inproceedings{hahnel_color_2004,
	title = {Color and texture features for person recognition},
	volume = {1},
	isbn = {1098-7576},
	abstract = {The need for automatic visual surveillance is increasing and the research on person recognition systems is more and more supported. As many biometric recognition methods, e.g. face recognition, are based on quite high camera resolutions which are not available in many situations, we examine features as well as classifier techniques for full body recognition. We present our experiments with color and texture features in the application of full body person recognition. On a database of 53 individuals we tested approved features for object recognition as well as {MPEG7} color and texture descriptors on a person recognition task. For comparison, we used an {RBF} network classifier as well as a nearest-neighbor classifier. Our experiments showed that color as well as texture information is important for a person recognition system. Additionally, a combination of these two kind of features results in a performance improvement.},
	author = {M. Hahnel and D. Klunder and {K.-F.} Kraiss},
	year = {2004},
	keywords = {automatic visual surveillance, biometric recognition method, classifier technique, color feature, full body person recognition, image colour analysis, image recognition, image texture, nearest-neighbor classifier, object recognition, person recognition system, radial basis function, radial basis function networks, {RBF} network classifier, surveillance, texture feature},
	pages = {652}
},

@inproceedings{srinivasan_bottom-up_2007,
	title = {Bottom-up Recognition and Parsing of the Human Body},
	doi = {10.1109/CVPR.2007.383301},
	abstract = {Recognizing humans, estimating their pose and segmenting their body parts are key to high-level image understanding. Because humans are highly articulated, the range of deformations they undergo makes this task extremely challenging. Previous methods have focused largely on heuristics or pairwise part models in approaching this problem. We propose a bottom-up parsing of increasingly more complete partial body masks guided by a parse tree. At each level of the parsing process, we evaluate the partial body masks directly via shape matching with exemplars, without regard to how the parses are formed. The body is evaluated as a whole, not the sum of its constituent parses, unlike previous approaches. Multiple image segmentations are included at each of the levels of the parsing, to augment existing parses or to introduce ones. Our method yields both a pose estimate as well as a segmentation of the human. We demonstrate competitive results on this challenging task with relatively few training examples on a dataset of baseball players with wide pose variation. Our method is comparatively simple and could be easily extended to other objects.},
	author = {P. Srinivasan and Jianbo Shi},
	year = {2007},
	keywords = {body parts segmentation, bottom-up recognition, gesture recognition, human body parsing, image matching, image segmentation, multiple image segmentations, parsing process},
	pages = {1--8}
},

@inproceedings{todorovic_detection_2004,
	title = {Detection of Artificial Structures in {Natural-Scene} Images Using Dynamic Trees},
	isbn = {0-7695-2128-2},
	url = {http://portal.acm.org/citation.cfm?id=1020433},
	abstract = {We seek a framework that addresses localization, detection and recognition of man-made objects in natural-scene images in a unified manner. We propose to model artificial structures by dynamic tree-structured belief networks {(DTS-BNs).} {DTSBNs} provide for a distribution over tree structures that we learn using our Structured Approximation {(SVA)} inference algorithm. Furthermore, we propose multi-scale linear-discriminant analysis {(MLDA)} as a feature extraction method, which appears well suited for our goals, as we assume that man-made objects are characterized primarily by geometric regularities and by patches of uniform color. {MLDA} extracts edges over a finite range of locations, orientations and scales, decomposing an image into dyadic squares. Both the color of dyadic squares and the geometric properties of extracted edges represent observable input to our {DTSBNs.} Experimental results demonstrate that {DTS-BNs,} trained on {MLDA} features, offer a viable solution for detection of artificial structures in natural-scene images.},
	publisher = {{IEEE} Computer Society},
	author = {Sinisa Todorovic and Michael C. Nechyba},
	year = {2004},
	pages = {35--39}
},

@misc{_ultrasonic_????,
	title = {Ultrasonic range finder},
	url = {http://www.freepatentsonline.com/7330398.html}
},

@article{ren_statistical_2003,
	title = {Statistical background modeling for non-stationary camera},
	volume = {24},
	url = {http://portal.acm.org/citation.cfm?id=641699},
	abstract = {A new background subtraction method is proposed in this paper for the foreground detection from a non-stationary background. Usually, motion compensation is required when applying background subtraction to a non-stationary background. In practice, it is difficult to realize this to sufficient pixel accuracy. The problem is further complicated when the moving objects to be detected/tracked are small, since the pixel error in motion compensating the background will hide the small targets. A spatial distribution of Gaussians model is proposed to deal with moving object detection where the motion compensation is not exact but approximated. The distribution of each background pixel is temporally and spatially modeled. Based on this statistical model, a pixel in the current frame is classified as belonging to the foreground or background. In addition, a new background restoration and adaptation algorithm is developed for the non-stationary background over an extended period of time. Test cases involving a surveillance system to detect small moving objects (human and car) within a highly textured background and a pan-tilt human tracking system are demonstrated successfully.},
	number = {1-3},
	journal = {Pattern Recogn. Lett.},
	author = {Ying Ren and {Chin-Seng} Chua and {Yeong-Khing} Ho},
	year = {2003},
	keywords = {background restoration, foreground detection, non-stationary background, object tracking, sdg model},
	pages = {183--196}
},

@inproceedings{hwang_automatic_2006,
	title = {An automatic three-dimensional human behavior analysis system for video surveillance applications},
	doi = {10.1109/ISCAS.2006.1692632},
	abstract = {We developed a system for automatic {3-D} human body motion in video surveillance sequences. As the first step, the human body that will be used for the analysis is extracted and tracked based on video object {(VO)} segmentation technique. Once human body is extracted, this {2-D} object, which consists of the merging of several regions, is then analyzed for activity and body part interpretation. The specific {2-D} human body shape along with the information derived from maximum probable explanation {(MPE)} of dynamic Bayesian modeling is then effectively utilized to obtain {3-D} body model frame-by-frame so that the {3-D} motion trajectory of the human object can be derived for any future event analysis, which is independent of the viewing perspective of the {2-D} video.},
	author = {{J.-N.} Hwang and I. Karliga and {H.-Y.} Cheng},
	year = {2006},
	keywords = {{2D} human body shape, {2D} video, {3D} motion trajectory, automatic {3D} human body motion, Bayes methods, dynamic Bayesian modeling, feature extracted, image motion analysis, image segmentation, image sequences, maximum probable explanation, surveillance, video object segmentation technique, video surveillance sequences},
	pages = {4 pp.}
},

@inproceedings{zhang_distributed_2008,
	title = {Distributed Smart Cameras, 2008. {ICDSC} 2008. Second {ACM/IEEE} International Conference on},
	doi = {10.1109/ICDSC.2008.4635727},
	abstract = {In this paper, we present a clothing recognition system that augments clothes recommendation and fashion exploration using the intelligent multi-view vision technology of the Responsive Mirror, an implicitly controlled human-computer interaction system for clothes fitting rooms. The Responsive Mirror provides shoppers with real-time “self” and “social” clothes comparisons. The system recommends clothing that is “similar” and “different” than the clothing that the person is trying on in the mirror. The goal of the research in this paper is to create a recommendation system that uses a definition of “similar” and “different” that matches human perception. We address the social nature of the recognition problem by conducting a user study to identify the salient clothes factors that people use to determine clothes similarity. We describe the computer vision and machine learning techniques employed to recognize the factors that human eyes perceive in term of clothing similarity from frontal-view outfit images. We describe the key components of the motion-tracking and clothes-recognition systems and evaluate their performance by user study and experiments on a simulated clothes fitting image dataset. The approach and results presented here will benefit designers and developers of similar applications in the future.},
	author = {Wei Zhang and Bo Begole and Maurice Chu and Juan Liu and Nicholas Yee},
	year = {2008},
	keywords = {clothes recognition, fashion, human-computer interaction, motion tracking, Multi-view vision},
	pages = {1--10}
},

@inproceedings{jaewon_sung_combining_2007,
	title = {Combining Local and Global Motion Estimators for Robust Face Tracking},
	doi = {10.1109/ROMAN.2007.4415107},
	abstract = {Faces show both global and local motions, where the former represents rigid head movements due to {3D} translation and rotation and the local motion represents non-rigid deformation due to speech, or facial expressions. Although non- rigid face models can represent both types of the facial motions, they are not enough to track the facial motions correctly. The non-rigid face models have large number of model parameters to explain various deformation of the face and the high dimensionality of their model parameter space make them sensitive to initial model parameters, apt to be stuck to local minimum, and difficult to be recovered (re-initialized) from failure when iterative gradient descent optimization techniques are used. To alleviate these problems, we propose to use two types of face trackers that are suitable for estimating the global and local motions, respectively. In the proposed algorithm, the global motion estimator is applied at first and the estimated global motion is used to compute proper initial model parameters of the local motion estimator to make it converge correctly. In this paper, we used active appearance model {(MM)} and cylinder head model {(CHM)} as the representative examples of the non- rigid and rigid face models. Experimental results showed that face tracking combining {AAMs} and {CHMs} improved the face tracking performance than that of {AAMs} in terms of 170\% higher tracking rate and the 115\% wider pose coverage.},
	author = {Jaewon Sung and Daijin Kim},
	year = {2007},
	keywords = {{3D} rotation, {3D} translation, active appearance model, cylinder head model, face recognition, facial motion, global motion estimator, gradient methods, iterative gradient descent optimization techniques, motion estimation, optical tracking, optimisation, robust face tracking},
	pages = {345--350}
},

@inproceedings{lee_biometrics_2008,
	title = {Biometrics Symposium, 2008. {BYSYM} '08},
	shorttitle = {Scars, marks and tattoos {(SMT)}},
	doi = {10.1109/BSYM.2008.4655515},
	abstract = {Scars, marks and tattoos {(SMT)} are being increasingly used for suspect and victim identification in forensics and law enforcement agencies. Tattoos, in particular, are getting serious attention because of their visual and demographic characteristics as well as their increasing prevalence. However, current tattoo matching procedure requires human-assigned class labels in the {ANSI/NIST} {ITL} 1–2000 standard which makes it time consuming and subjective with limited retrieval performance. Further, tattoo images are complex and often contain multiple objects with large intra-class variability, making it very difficult to assign a single category in the {ANSI/NIST} standard. We describe a content-based image retrieval {(CBIR)} system for matching and retrieving tattoo images. Based on Scale Invariant Feature Transform {(SIFT)} features extracted from tattoo images and optional accompanying demographical information, our system computes feature-based similarity between the query tattoo image and tattoos in the criminal database. Experimental results on two different tattoo databases show encouraging results.},
	author = {{Jung-Eun} Lee and Anil K. Jain and Rong Jin},
	year = {2008},
	pages = {1--8}
},

@inproceedings{matta_behavioural_2006,
	title = {A Behavioural Approach to Person Recognition},
	doi = {10.1109/ICME.2006.262817},
	abstract = {This paper describes a new approach for identity recognition using video sequences. While most image and video recognition systems discriminate identities using physical information only, our approach exploits the behavioral information of head dynamics; in particular the displacement signals of few head features directly extracted in the image plane. Due to the lack of standard video database, identification and verification scores have been obtained using a small collection of video sequences: the results for this new approach are nevertheless promising},
	author = {F. Matta and {J.-L.} Dugelay},
	year = {2006},
	keywords = {feature extraction, head feature extraction, image recognition, image sequences, person recognition, video sequence, video signal processing},
	pages = {1461--1464}
},

@inproceedings{adam_aggregated_2006,
	title = {Aggregated Dynamic Background Modeling},
	isbn = {1522-4880},
	doi = {10.1109/ICIP.2006.312881},
	abstract = {Standard practices in background modeling learn a separate model for every pixel in the image. However, in dynamic scenes the connection between an observation and the place where it was observed is much less important and is usually random. For example, a wave observed in an ocean scene could easily have been observed at another place in the image. Moreover, during a limited learning period, we cannot expect to observe at every pixel all the possible background behaviors. We therefore develop in this paper a background model in which observations are decoupled from the place in the image where they were observed. A single non-parametric model is used to describe the dynamic region of the scene, aggregating the observations from the whole region. Using high-order features, we demonstrate the feasibility of our approach on challenging ocean scenes using only grayscale information},
	author = {A. Adam and E. Rivlin and I. Shimshoni},
	year = {2006},
	keywords = {background modeling, dynamic background modeling, dynamic backgrounds, feature extraction, grayscale information, image processing, nonparametric model, observation aggregation, ocean scene, video signal processing, video surveillance},
	pages = {3313--3316}
},

@inproceedings{fawky_eye_2007,
	title = {Eye detection to assist drowsy drivers},
	doi = {10.1109/ITICT.2007.4475632},
	abstract = {This paper proposes a new approach to detect the eyes opening state for the purpose of alarming drowsy drivers on highroads. The input image frame is processed in successive steps using a combination of algorithms, namely wavelets transform, edge detection and {YCbCr} transform. The algorithm performs well, regardless the size of the image and is immune to distracting noises, high luminance, and background lighting. The proposed algorithm was able to attain an average eye detection accuracy of 80\% when tested on different data bases and different conditions.},
	author = {A. Fawky and S. Khalil and M. Elsabrouty},
	year = {2007},
	keywords = {drowsy drivers, edge detection, eye detection, object detection, Skin Filter, wavelet transforms, Wavelets Transform, {YCbCr,} {YCbCr} transform},
	pages = {131--134}
},

@inproceedings{haibin_ling_study_2007,
	title = {A Study of Face Recognition as People Age},
	isbn = {1550-5499},
	doi = {10.1109/ICCV.2007.4409069},
	abstract = {In this paper we study face recognition across ages within a real passport photo verification task. First, we propose using the gradient orientation pyramid for this task. Discarding the gradient magnitude and utilizing hierarchical techniques, we found that the new descriptor yields a robust and discriminative representation. With the proposed descriptor, we model face verification as a two-class problem and use a support vector machine as a classifier. The approach is applied to two passport data sets containing more than 1,800 image pairs from each person with large age differences. Although simple, our approach outperforms previously tested Bayesian technique and other descriptors, including the intensity difference and gradient with magnitude. In addition, it works as well as two commercial systems. Second, for the first time, we empirically study how age differences affect recognition performance. Our experiments show that, although the aging process adds difficulty to the recognition task, it does not surpass illumination or expression as a confounding factor.},
	author = {Haibin Ling and S. Soatto and N. Ramanathan and {D.W.} Jacobs},
	year = {2007},
	keywords = {Bayesian technique, discriminative representation, face recognition, face verification, gradient magnitude, gradient orientation pyramid, hierarchical techniques, intensity difference, passport photo verification task, support vector machine, support vector machines},
	pages = {1--8}
},

@article{oliver_bayesian_2000,
	title = {A Bayesian computer vision system for modeling human interactions},
	volume = {22},
	issn = {0162-8828},
	doi = {10.1109/34.868684},
	abstract = {We describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task. The system deals in particularly with detecting when interactions between people occur and classifying the type of interaction. Examples of interesting interaction behaviors include following another person, altering one's path to meet another, and so forth. Our system combines top-down with bottom-up information in a closed feedback loop, with both components employing a statistical Bayesian approach. We propose and compare two different state-based learning architectures, namely, {HMMs} and {CHMMs} for modeling behaviors and interactions. Finally, a synthetic {“Alife-style”} training system is used to develop flexible prior models for recognizing human interactions. We demonstrate the ability to use these a priori models to accurately classify real human behaviors and interactions with no additional tuning or training},
	number = {8},
	journal = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {{N.M.} Oliver and B. Rosario and {A.P.} Pentland},
	year = {2000},
	keywords = {Bayes method, Bayes methods, computer vision, hidden Markov model, hidden Markov models, human behavior recognition, image segmentation, learning systems, machine learning, object recognition, pattern recognition, people detection, real-time systems, surveillance, visual surveillance},
	pages = {831--843}
},

@inproceedings{kulic_combining_2008,
	title = {Combining automated on-line segmentation and incremental clustering for whole body motions},
	isbn = {1050-4729},
	doi = {10.1109/ROBOT.2008.4543603},
	abstract = {This paper describes a novel approach for incremental learning of human motion pattern primitives through on-line observation of human motion. The observed motion time series data stream is first stochastically segmented into potential motion primitive segments, based on the assumption that data belonging to the same motion primitive will have the same underlying distribution. The motion segments are then abstracted into a stochastic model representation, and automatically clustered and organized. As new motion patterns are observed, they are incrementally grouped together based on their relative distance in the model space. The resulting representation of the knowledge domain is a tree structure, with specialized motions at the tree leaves, and generalized motions closer to the root. The tree leaves, which represent the most specialized learned motion primitives, are then passed back to the segmentation algorithm, so that as the number of known motion primitives increases, the accuracy of the segmentation can also be improved. The combined algorithm is tested on a sequence of continuous human motion data obtained through motion capture, and demonstrates the performance of the proposed approach.},
	author = {D. Kulic and W. Takano and Y. Nakamura},
	year = {2008},
	keywords = {automated online segmentation, humanoid robots, image motion analysis, image segmentation, incremental clustering, incremental learning, intelligent robots, learning (artificial intelligence), motion segments, segmentation algorithm, tree structure, whole body motions},
	pages = {2591--2598}
},

@article{ming-hsuan_yang_detecting_2002,
	title = {Detecting faces in images: a survey},
	volume = {24},
	issn = {0162-8828},
	shorttitle = {Detecting faces in images},
	doi = {10.1109/34.982883},
	abstract = {Images containing faces are essential to intelligent vision-based human-computer interaction, and research efforts in face processing include face recognition, face tracking, pose estimation and expression recognition. However, many reported methods assume that the faces in an image or an image sequence have been identified and localized. To build fully automated systems that analyze the information contained in face images, robust and efficient face detection algorithms are required. Given a single image, the goal of face detection is to identify all image regions which contain a face, regardless of its {3D} position, orientation and lighting conditions. Such a problem is challenging because faces are non-rigid and have a high degree of variability in size, shape, color and texture. Numerous techniques have been developed to detect faces in a single image, and the purpose of this paper is to categorize and evaluate these algorithms. We also discuss relevant issues such as data collection, evaluation metrics and benchmarking. After analyzing these algorithms and identifying their limitations, we conclude with several promising directions for future research},
	number = {1},
	journal = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {{Ming-Hsuan} Yang and {D.J.} Kriegman and N. Ahuja},
	year = {2002},
	keywords = {{3D} position, benchmarking, computer vision, data collection, evaluation metrics, expression recognition, face color, face detection algorithms, face images, face orientation, face recognition, face shape, face size, face texture, face tracking, feature extraction, fully automated systems, image sequence, intelligent vision-based human-computer interaction, lighting conditions, machine learning, object detection, object recognition, pose estimation, reviews, statistical pattern recognition, survey, view-based recognition},
	pages = {34--58}
},

@article{viola_robust_2001,
	title = {Robust Real-time Object Detection},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.2751},
	doi = {10.1.1.23.2751},
	journal = {International Journal of Computer Vision},
	author = {Paul Viola and Michael Jones},
	year = {2001}
},

@inproceedings{bhattacharya_evaluation_2008,
	title = {An evaluation of the Tight Optical Integration {(TOI)} algorithm sensitivity to inertial and camera errors},
	doi = {10.1109/PLANS.2008.4570061},
	abstract = {The objective of this paper is to demonstrate the sensitivity of the {GPS-camera} tight optical integration {(TOI)} algorithm to errors stemming from two sources; (a) inertial measurement errors and, (b) camera measurement errors. The {TOI} algorithm has been described by the authors in a recent paper, {"An} algorithm for {GPS} tight optical integration {(TOI)".} In the {TOI} algorithm, the inertial measurements are primarily used to derive attitude information to transform the camera measurements from the body frame to the navigation frame of reference. Clearly, errors on these inertial measurements contribute directly to the angular information from the camera used for a final position solution. Also, an initial position estimate is required to transform the measurements from the navigation frame of reference to an earth reference frame. As a result, an error in the initial position estimate will introduce some error in the position solution, which is included as an equivalent inertial error in the overall error analysis. The function of the camera is to make angular measurements from a pre-defined "marker". The errors in these measurements are sensitive to the range of the marker from the camera, i.e. marker range. Although the marker range is not a measurement used in the algorithm, it is necessary to investigate the sensitivity of the algorithm to variations in the marker range. It is shown that the sensitivity of the error in the final position estimate increases as this marker range increases. Moreover, the camera needs to identify the marker and hence, an error in the pixel selection in the image or a marker position error will affect the accuracy of the final position estimates. The sensitivity of the algorithm to these errors contributed by the camera is also analyzed. The {TOI} algorithm performance is shown to be similar to {GPS} performance levels, in the presence of the error sources discussed above. This makes it a viable solution for navigation capabilities in environ- - ments where sufficient satellites for a {GPS} only solution are not available, e.g. an urban canyon.},
	author = {S. Bhattacharya and T. Arthur and M. Uijt de Haag and Z. Zhu and K. Scheff},
	year = {2008},
	keywords = {angular measurement, angular measurements, camera measurement errors, inertial measurement errors, measurement errors, navigation, position measurement, tight optical integration},
	pages = {533--540}
},

@inproceedings{sano_human_2004,
	title = {Human body shape imaging for Japanese kimono design},
	volume = {2},
	isbn = {1091-5281},
	doi = {10.1109/IMTC.2004.1351260},
	abstract = {A yukata is a type of traditional Japanese kimono. An alignment of its texture pattern is an important factor of the yukata design. The wearing condition of the yukata is affected by the wearer's body shape and the way of wearing the yukata. Accordingly, a three dimensional display of the yukata is necessary for designing the yukata. In this paper, we developed a human body shape imaging system for yukata design. Firstly, we developed an algorithm to measure the wearer's upper half of the body, which is important to display the wearing condition of the yukata. Secondly, we developed an algorithm to map the texture pattern of the kimono cloth on the wearer's body shape. The designer and the wearer can make sure of the condition of the texture alignment exactly because the yukata is displayed three dimensionally on the wearer's body shape.},
	author = {T. Sano and H. Yamamoto},
	year = {2004},
	keywords = {{CAD,} {CAD} system, clothing, human body shape imaging, image texture, Japanese kimono design, kimono cloth texture pattern mapping, shape measurement, texture pattern alignment, three-dimensional displays, yukata {3D} display, yukata wearing condition},
	pages = {1120--1123 Vol.2}
},

@article{xuelong_li_gait_2008,
	title = {Gait Components and Their Application to Gender Recognition},
	volume = {38},
	issn = {1094-6977},
	doi = {10.1109/TSMCC.2007.913886},
	abstract = {Human gait is a promising biometrics resource. In this paper, the information about gait is obtained from the motions of the different parts of the silhouette. The human silhouette is segmented into seven components, namely head, arm, trunk, thigh, front-leg, back-leg, and feet. The leg silhouettes for the front-leg and the back-leg are considered separately because, during walking, the left leg and the right leg are in front or at the back by turns. Each of the seven components and a number of combinations of the components are then studied with regard to two useful applications: human identification {(ID)} recognition and gender recognition. More than 500 different experiments on human {ID} and gender recognition are carried out under a wide range of circumstances. The effectiveness of the seven human gait components for {ID} and gender recognition is analyzed.},
	number = {2},
	journal = {Systems, Man, and Cybernetics, Part C: Applications and Reviews, {IEEE} Transactions on},
	author = {Xuelong Li and {S.J.} Maybank and Shuicheng Yan and Dacheng Tao and Dong Xu},
	year = {2008},
	keywords = {biometrics, biometrics (access control), gait analysis, gait components, gender recognition, human gait, human gait recognition, Human identification, human silhouette, image motion analysis, image recognition, image segmentation, visual surveillance},
	pages = {145--155}
},

@article{villanueva_novel_2008,
	title = {A Novel Gaze Estimation System With One Calibration Point},
	volume = {38},
	issn = {1083-4419},
	doi = {10.1109/TSMCB.2008.926606},
	abstract = {The design of robust and high-performance gaze-tracking systems is one of the most important objectives of the eye-tracking community. In general, a subject calibration procedure is needed to learn system parameters and be able to estimate the gaze direction accurately. In this paper, we attempt to determine if subject calibration can be eliminated. A geometric analysis of a gaze-tracking system is conducted to determine user calibration requirements. The eye model used considers the offset between optical and visual axes, the refraction of the cornea, and Donder's law. This paper demonstrates the minimal number of cameras, light sources, and user calibration points needed to solve for gaze estimation. The underlying geometric model is based on glint positions and pupil ellipse in the image, and the minimal hardware needed for this model is one camera and multiple light-emitting diodes. This paper proves that subject calibration is compulsory for correct gaze estimation and proposes a model based on a single point for subject calibration. The experiments carried out show that, although two glints and one calibration point are sufficient to perform gaze estimation (error {\textasciitilde} 1deg), using more light sources and calibration points can result in lower average errors.},
	number = {4},
	journal = {Systems, Man, and Cybernetics, Part B, {IEEE} Transactions on},
	author = {A. Villanueva and R. Cabeza},
	year = {2008},
	keywords = {calibration, eye model, Eye tracking, gaze direction estimation, gaze estimation, gaze tracking system, geometric analysis, geometric model, glint position, graphical user interfaces, line of sight {(LOS),} point of regard {(POR),} pupil ellipse, subject calibration, user calibration point, user calibration requirement, video oculography, video signal processing},
	pages = {1123--1138}
},

@inproceedings{bolder_robotics_2007,
	title = {Robotics and Automation, 2007 {IEEE} International Conference on},
	isbn = {1050-4729},
	doi = {10.1109/ROBOT.2007.363936},
	abstract = {We describe a system for visual interaction developed for humanoid robots. It enables the robot to interact with its environment using a smooth whole body motion control driven by stabilized visual targets. Targets are defined as visually extracted "proto-objects" and behavior-relevant object hypotheses and are stabilized by means of a short-term sensory memory. Selection mechanisms are used to switch between behavior alternatives for searching or tracking objects as well as different whole body motion strategies for reaching. The decision between different motion strategies like reaching with right or left hand or with and without walking is made based on internal predictions that use copies of the whole-body control algorithm. The results show robust object tracking and a smooth interaction behavior that includes a large variety of whole-body postures.},
	author = {B. Bolder and M. Dunn and M. Gienger and H. Janssen and H. Sugiura and C. Goerick},
	year = {2007},
	keywords = {humanoid robots, mobile robots, motion control, object tracking, robot vision, selection mechanisms, short-term sensory memory, visually guided whole body interaction},
	pages = {3054--3061}
},

@inproceedings{yinggang_xie_control_2006,
	title = {Control Conference, 2006. {CCC} 2006. Chinese},
	doi = {10.1109/CHICC.2006.280890},
	abstract = {Affective computing is becoming a new research hotspot. An effective method of facial and eye detection is presented in this paper, which uses skin-color model and key facial feature points, based on image processing and expression recognition. Finally the realization is introduced that the solution has been applied to the e-learning system to cope with the emotion and cognition of the student interest},
	author = {Yinggang Xie and Zhiliang Wang and Ning Cheng and Guojiang Wang and M. Nagai},
	year = {2006},
	keywords = {affective computing, affective recognition, Artificial Psychology, e-learning system, expression recognition, Eye, eye detection, face recognition, Facial Detection, image colour analysis, image processing, object detection, skin-color model},
	pages = {1942--1946}
},

@article{yacoob_detection_2006,
	title = {Detection and analysis of hair},
	volume = {28},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2006.139},
	abstract = {We develop computational models for measuring hair appearance for comparing different people. The models and methods developed have applications to person recognition and image indexing. An automatic hair detection algorithm is described and results reported. A multidimensional representation of hair appearance is presented and computational algorithms are described. Results on a data set of 524 subjects are reported. Identification of people using hair attributes is compared to eigenface-based recognition along with a joint, eigenface-hair-based identification.},
	number = {7},
	journal = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {Y. Yacoob and {L.S.} Davis},
	year = {2006},
	keywords = {automatic hair detection, eigenfaces, face recognition, feature extraction, hair analysis, hair appearance, hair detection., Human identification, image indexing, image representation, multidimensional representation, object detection, person recognition},
	pages = {1164--1169}
},

@inproceedings{davis_computer_2004,
	title = {Computer Vision and Pattern Recognition Workshop, 2004 Conference on},
	doi = {10.1109/CVPR.2004.80},
	abstract = {We present an adaptive three-mode {PCA} framework for recognizing gender from walking movements. Prototype female and male walkers are initially decomposed into a sub-space of their three-mode components (posture, time, gender). We then assign an importance weight to each motion trajectory in the sub-space and have the model automatically learn the weight values (key features) from labeled training data. We present experiments of recognizing physical (actual) and perceived (from perceptual experiments) gender for 40 walkers. The model demonstrates greater than 90\% recognition for both contexts and shows greater flexibility than standard {PCA.}},
	author = {{J.W.} Davis and Hui Gao},
	year = {2004},
	pages = {9}
},

@article{bacivarov_statistical_2008,
	title = {Statistical models of appearance for eye tracking and eye-blink detection and measurement},
	volume = {54},
	issn = {0098-3063},
	doi = {10.1109/TCE.2008.4637622},
	abstract = {A statistical Active Appearance Model {(AAM)} is developed to track and detect eye blinking. The model has been designed to be robust to variations of head pose or gaze. In particular we analyze and determine the model parameters which encode the variations caused by blinking. This global model is further extended using a series of sub-models to enable independent modeling and tracking of the two eye regions. Several methods to enable measurement and detection of eye-blink are proposed and evaluated. The results of various tests on different image databases are presented to validate each model.},
	number = {3},
	journal = {Consumer Electronics, {IEEE} Transactions on},
	author = {I. Bacivarov and M. Ionita and P. Corcoran},
	year = {2008},
	keywords = {eye blinking, eye tracking, active appearance model, digital imaging},
	pages = {1312--1320}
},

@inproceedings{kanaujia_computer_2006,
	title = {Computer Vision and Pattern Recognition Workshop, 2006 Conference on},
	doi = {10.1109/CVPRW.2006.69},
	abstract = {Tracking facial features across large head rotations is a challenging research problem. Both {2D} and {3D} model based approaches have been proposed for feature analysis from multiple views. Accurate feature tracking enables useful video processing applications like emblem detection(an event or movement that symbolizes an idea), facial expressions recognition, morphing and synthesis. A crucial requirement is generalizability of the tracking framework across appearance variations, presence of facial hair and illumination changes. We propose a framework to detect emblems that combines active shape model with a predictive face aspect model to track features across large head movements and runs close to real time. Active Shape {Model(ASM)} is a deformable model for shape registration that detect facial features by combining prior shape information with the observed image data. Our view based framework represents various head poses by multiple {2D} shape models and accounts for large head rotations by dynamically switching between them. Our switching variable (the current model to use) is discriminatively predicted from the {SIFT} descriptors computed over the bounding box of low resolution face image. We demonstrate the use of tracking framework to recognize high level events like head nodding, shaking and eye blinking.},
	author = {A. Kanaujia and Yuchi Huang and D. Metaxas},
	year = {2006},
	pages = {108}
},

@article{rajashekar_gaffe:_2008,
	title = {{GAFFE:} A {Gaze-Attentive} Fixation Finding Engine},
	volume = {17},
	issn = {1057-7149},
	shorttitle = {{GAFFE}},
	doi = {10.1109/TIP.2008.917218},
	abstract = {The ability to automatically detect visually interesting regions in images has many practical applications, especially in the design of active machine vision and automatic visual surveillance systems. Analysis of the statistics of image features at observers' gaze can provide insights into the mechanisms of fixation selection in humans. Using a foveated analysis framework, we studied the statistics of four low-level local image features: luminance, contrast, and bandpass outputs of both luminance and contrast, and discovered that image patches around human fixations had, on average, higher values of each of these features than image patches selected at random. Contrast-bandpass showed the greatest difference between human and random fixations, followed by luminance-bandpass, {RMS} contrast, and luminance. Using these measurements, we present a new algorithm that selects image regions as likely candidates for fixation. These regions are shown to correlate well with fixations recorded from human observers.},
	number = {4},
	journal = {Image Processing, {IEEE} Transactions on},
	author = {U. Rajashekar and I. van der Linde and {A.C.} Bovik and {L.K.} Cormack},
	year = {2008},
	keywords = {active machine vision, active vision, automatic visual surveillance systems, bandpass outputs, contrast feature, Eye tracking, feature extraction, fixation selection, foveated analysis framework, foveation, {GAFFE} engine, gaze-attentive fixation finding engine, image feature statistics analysis, image patches, luminance feature, object detection, observer gaze, point-of-gaze, statistical analysis, visually interesting region detection},
	pages = {564--573}
},

@inproceedings{balan_computer_2007,
	title = {Computer Vision and Pattern Recognition, 2007. {CVPR} '07. {IEEE} Conference on},
	doi = {10.1109/CVPR.2007.383340},
	abstract = {Much of the research on video-based human motion capture assumes the body shape is known a priori and is represented coarsely (e.g. using cylinders or superquadrics to model limbs). These body models stand in sharp contrast to the richly detailed {3D} body models used by the graphics community. Here we propose a method for recovering such models directly from images. Specifically, we represent the body using a recently proposed triangulated mesh model called {SCAPE} which employs a low-dimensional, but detailed, parametric model of shape and pose-dependent deformations that is learned from a database of range scans of human bodies. Previous work showed that the parameters of the {SCAPE} model could be estimated from marker-based motion capture data. Here we go further to estimate the parameters directly from image data. We define a cost function between image observations and a hypothesized mesh and formulate the problem as optimization over the body shape and pose parameters using stochastic search. Our results show that such rich generative models enable the automatic recovery of detailed human shape and pose from images.},
	author = {{A.O.} Balan and L. Sigal and {M.J.} Black and {J.E.} Davis and {H.W.} Haussecker},
	year = {2007},
	keywords = {automatic recovery, gesture recognition, human pose, human shape, hypothesized mesh, image motion analysis, image observations, image segmentation, marker-based motion capture data, {SCAPE,} stochastic search, triangulated mesh model, video-based human motion capture},
	pages = {1--8}
},

@inproceedings{arthur_demonstration_2008,
	title = {Demonstration of Tight Optical Integration {(TOI)} algorithm using field data},
	doi = {10.1109/PLANS.2008.4570065},
	abstract = {Recently a biologically inspired algorithm, called tight optical integration {(TOI),} was developed for tightly integrating optical sensor with {GPS.} The algorithm involves the integration of a standard camera along with {GPS} range (pseudorange or carrier phase) measurements to form position estimates. Initial simulations showed that {TOI} is capable of providing a position solution with an insufficient number of {GPS} satellites and a visible ldquomarkerrdquo at a known location, with an inertial unit to provide attitude information. This paper demonstrates how a marker is selected from a picture frame and tracked among consecutive frames. {TOI} has the potential to navigate with one known marker and two or three {GPS} satellites. In this work attitude information is derived from the {GPS} velocity estimates assuming zero roll for a terrestrial vehicle. Additionally, the same {TOI} algorithm can auto-locate unknown features when the position of the marker is not available, and navigate by these features when location is lost. The {TOI} algorithm is unique because it relies only on {GPS} range measurements and the pixel data from a camera. No ranging sources such as radar or {LIDAR} are required. It has particular application to scenarios involving a reduced constellation; such a reduced constellation may be due either to an urban canyon or a denied signal environment.},
	author = {T. Arthur and Z. Zhu and S. Bhattacharya and K. Johnson and K. Scheff},
	year = {2008},
	keywords = {attitude information, autolocate unknown features, biologically inspired algorithm, cameras, Global Positioning System, {GPS} satellites, optical sensor, optical sensors, tight optical integration algorithm},
	pages = {744--751}
},

@inproceedings{gallagher_clothing_2008,
	title = {Clothing cosegmentation for recognizing people},
	isbn = {1063-6919},
	doi = {10.1109/CVPR.2008.4587481},
	abstract = {Researches have verified that clothing provides information about the identity of the individual. To extract features from the clothing, the clothing region first must be localized or segmented in the image. At the same time, given multiple images of the same person wearing the same clothing, we expect to improve the effectiveness of clothing segmentation. Therefore, the identity recognition and clothing segmentation problems are inter-twined; a good solution for one aides in the solution for the other. We build on this idea by analyzing the mutual information between pixel locations near the face and the identity of the person to learn a global clothing mask. We segment the clothing region in each image using graph cuts based on a clothing model learned from one or multiple images believed to be the same person wearing the same clothing. We use facial features and clothing features to recognize individuals in other images. The results show that clothing segmentation provides a significant improvement in recognition accuracy for large image collections, and useful clothing masks are simultaneously produced. A further significant contribution is that we introduce a publicly available consumer image collection where each individual is identified. We hope this dataset allows the vision community to more easily compare results for tasks related to recognizing people in consumer image collections.},
	author = {{A.C.} Gallagher and Tsuhan Chen},
	year = {2008},
	keywords = {clothing cosegmentation, consumer image collections, face recognition, feature extraction, global clothing mask, image resolution, image segmentation, people recognition, pixel locations},
	pages = {1--8}
},

@inproceedings{barnard_body_2008,
	title = {Body part segmentation of noisy human silhouette images},
	doi = {10.1109/ICME.2008.4607653},
	abstract = {In this paper we propose a solution to the problem of body part segmentation in noisy silhouette images. In developing this solution we revisit the issue of insufficient labeled training data, by investigating how synthetically generated data can be used to train general statistical models for shape classification. In our proposed solution we produce sequences of synthetically generated images, using three dimensional rendering and motion capture information. Each image in these sequences is labeled automatically as it is generated and this labeling is based on the hand labeling of a single initial {image.We} use shape context features and Hidden Markov Models trained based on this labeled synthetic data. This model is then used to segment silhouettes into four body parts; arms, legs, body and head. Importantly, in all the experiments we conducted the same model is employed with no modification of any parameters after initial training.},
	author = {M. Barnard and M. Matilainen and J. Heikkila},
	year = {2008},
	keywords = {body part recognition, body part segmentation, hidden Markov model, hidden Markov models, image classification, image motion analysis, image segmentation, image sequence, image sequences, labeled synthetic data, motion capture information, noisy human silhouette image, rendering (computer graphics), shape classification, shape context features, silhouette segmentation, statistical model, three dimensional rendering},
	pages = {1189--1192}
},

@inproceedings{ram_people_1998,
	title = {The people sensor: a mobility aid for the visually impaired},
	shorttitle = {The people sensor},
	doi = {10.1109/ISWC.1998.729548},
	abstract = {Electronic Travel Aids, which transform visual environmental cues into another sensory modality, have been proven to help visually impaired people travel with a greater degree of psychological comfort and independence. The People Sensor is an Electronic Travel Aid designed to address two issues of importance to visually impaired people: inadvertent cane contact with other pedestrians and objects, and speaking to a person who is no longer within hearing range. The device uses pyroelectric and ultrasound sensors to locate and differentiate between animate (human) and inanimate (non-human) obstructions in the detection path. The distance between the user and the obstruction along with the nature of the obstruction (human or non-human) is transmitted via modulated vibrotactile feedback. Armed with advance knowledge of the presence and location of objects and people in the environment, users of The People Sensor can travel with increased independence, safety and confidence},
	author = {S. Ram and J. Sharf},
	year = {1998},
	keywords = {Electronic Travel Aid, handicapped aids, modulated vibrotactile feedback, People Sensor, pyroelectric detectors, visual environmental cues, visually impaired people},
	pages = {166--167}
},

@inproceedings{valenti_accurate_2008,
	title = {Accurate eye center location and tracking using isophote curvature},
	isbn = {1063-6919},
	doi = {10.1109/CVPR.2008.4587529},
	abstract = {The ubiquitous application of eye tracking is precluded by the requirement of dedicated and expensive hardware, such as infrared high definition cameras. Therefore, systems based solely on appearance (i.e. not involving active infrared illumination) are being proposed in literature. However, although these systems are able to successfully locate eyes, their accuracy is significantly lower than commercial eye tracking devices. Our aim is to perform very accurate eye center location and tracking, using a simple Web cam. By means of a novel relevance mechanism, the proposed method makes use of isophote properties to gain invariance to linear lighting changes (contrast and brightness), to achieve rotational invariance and to keep low computational costs. In this paper we test our approach for accurate eye location and robustness to changes in illumination and pose, using the {BioIDand} the Yale Face B databases, respectively. We demonstrate that our system can achieve a considerable improvement in accuracy over state of the art techniques.},
	author = {R. Valenti and T. Gevers},
	year = {2008},
	keywords = {{BioID} database, computer vision, eye center location, eye center tracking, infrared high definition cameras, isophote curvature, relevance mechanism, visual databases, Web cam, Yale Face B database},
	pages = {1--8}
},

@article{radke_image_2005,
	title = {Image change detection algorithms: a systematic survey},
	volume = {14},
	issn = {1057-7149},
	shorttitle = {Image change detection algorithms},
	doi = {10.1109/TIP.2004.838698},
	abstract = {Detecting regions of change in multiple images of the same scene taken at different times is of widespread interest due to a large number of applications in diverse disciplines, including remote sensing, surveillance, medical diagnosis and treatment, civil infrastructure, and underwater sensing. This paper presents a systematic survey of the common processing steps and core decision rules in modern change detection algorithms, including significance and hypothesis testing, predictive models, the shading model, and background modeling. We also discuss important preprocessing methods, approaches to enforcing the consistency of the change mask, and principles for evaluating and comparing the performance of change detection algorithms. It is hoped that our classification of algorithms into a relatively small number of categories will provide useful guidance to the algorithm designer.},
	number = {3},
	journal = {Image Processing, {IEEE} Transactions on},
	author = {{R.J.} Radke and S. Andra and O. {Al-Kofahi} and B. Roysam},
	year = {2005},
	keywords = {background modeling, change detection, change mask, hypothesis testing, illumination invariance, image change detection algorithm, image classification, mixture models, predictive model, predictive models, shading model, significance testing, systematic survey},
	pages = {294--307}
},

@inproceedings{sangho_park_motion_2002,
	title = {Motion and Video Computing, 2002. Proceedings. Workshop on},
	doi = {10.1109/MOTION.2002.1182221},
	abstract = {The paper presents a system to segment and track multiple body parts of interacting humans in the presence of mutual occlusion and shadow. The color image sequence is processed at three levels: pixel level, blob level, and object level. A Gaussian mixture model is used at the pixel level to train and classify individual pixel colors. A Markov random field {(MRF)} framework is used at the blob level to merge the pixels into coherent blobs and to register inter-blob relations. A coarse model of the human body is applied at the object level as empirical domain knowledge to resolve ambiguity due to occlusion and to recover from intermittent tracking failures. A two-fold tracking scheme is used which consists of blob to blob matching in consecutive frames and blob to body part association within a frame. The tracking scheme resembles a multi-target, multi-assignment framework. The result is a tracking system that simultaneously segments and tracks multiple body parts of interacting people. Example sequences illustrate the success of the proposed paradigm.},
	author = {Sangho Park and {J.K.} Aggarwal},
	year = {2002},
	keywords = {blob level processing, body part segmentation, body part tracking, color image sequence, Gaussian mixture model, Gaussian processes, hidden feature removal, image classification, image colour analysis, image matching, image registration, image segmentation, image sequences, learning (artificial intelligence), Markov processes, Markov random field, object detection, object level processing, occlusion, optical tracking, pixel level processing, shadowing, surveillance, video signal processing, video surveillance},
	pages = {105--111}
},

@inproceedings{funahashi_control_2007,
	title = {Control, Automation and Systems, 2007. {ICCAS} '07. International Conference on},
	doi = {10.1109/ICCAS.2007.4406545},
	abstract = {We propose a system for extracting eye gaze information, and develop a couple of new human interface media by eye gaze information. For example, the observer wants to know visually how effectively the partner is interesting to him or not even on the net environment. To do this it is usually expected to utilize motion images. But, the observer wants to know exclusively facial image and gaze. So, we paid attention to face tracking from the scene and analyzed the features of eye gaze pattern extracted from the facial images by iris recognition.},
	author = {T. Funahashi and T. Fujiwara and H. Koshimizu},
	year = {2007},
	keywords = {eye gaze information extraction, eye gaze patterns, Eye tracking, face recognition, face tracking, facial image, feature extraction, gaze analysis, Gaze Mark Pattern, human interface media, image motion analysis, iris recognition, motion images},
	pages = {1337--1341}
},

@inproceedings{_person_2002,
	title = {Person Identification Using Automatic Height and Stride Estimation},
	isbn = {{0-7695-1695-X}},
	url = {http://portal.acm.org/citation.cfm?id=846227.848606},
	abstract = {We present a parametric method to automatically identify people in monocular low-resolution video by estimating the height and stride parameters of their gait. Stride parameters (stride length and cadence) are functions of body height, weight, and gender. Previous work has demonstrated effective use of these biometrics for identification and verification of people. In this paper, we show that performance is significantly improved by using height as an additional discriminant feature. Height is estimated by segmenting the person from the background and fitting their apparent height to a time-dependent model. With a database of 45 people and 4samples of each, we show that a person is correctly identified with 49\% probability when using both height and stride parameters, compared with 21\% when using stride parameters only. Height estimates for this configuration are accurate to within s = 3.5cm. This method works with low-resolution images of people, and is robust to changes in lighting, clothing, and tracking errors.},
	publisher = {{IEEE} Computer Society},
	year = {2002},
	pages = {40377}
},

@inproceedings{miyazaki_sice_2007,
	title = {{SICE,} 2007 Annual Conference},
	doi = {10.1109/SICE.2007.4421007},
	abstract = {In order to investigate whether or not checkpoints of features surrounding the eye are suitable for eye tracking, we evaluated the checkpoints using statistical hypothesis testing. By combining the checkpoints of features surrounding the eye, the eye tracking system using the template matching was effective for the eye detection. The parameters of the checkpoints were empirically determined to detect the eye in any individual. The statistical hypothesis testing revealed that the empirical setting values of the checkpoints were significantly different from actual facial features obtained using face images. We investigated suitable parameters of the checkpoints derived from actual facial features obtained with facial images. It was revealed that new checkpoint setting's were effective in drastically reducing the possibility of incorrect eye detection, although the correct detection rate was slightly decreased.},
	author = {S. Miyazaki and H. Takano and K. Nakamura},
	year = {2007},
	keywords = {Checkpoints of features surrounding the eye, Eye, eye detection, Eye tracking, eye tracking system, face recognition, facial features, facial images, feature checkpoints, feature extraction, image matching, object detection, Statistical hypothesis testing, statistical testing, template matching, tracking},
	pages = {356--360}
},

@inproceedings{kawade_micromechatronics_2002,
	title = {Micromechatronics and Human Science, 2002. {MHS} 2002. Proceedings of 2002 International Symposium on},
	doi = {10.1109/MHS.2002.1058006},
	abstract = {When people communicate with each other directly (face to face), the role of the visual information is very essential, especially facial images which include the most important information. If artificial vision systems that can understand human faces are realized, they will enable machines to communicate with people more effectively. For this purpose, we have been continuing the research and development of vision-based face understanding technologies, such as face detection, facial feature point detection, face recognition, face tracking, encoding and transmission of facial motion, face situation estimation, gender and age estimation, etc. This paper introduces these technologies and their applications.},
	author = {M. Kawade},
	year = {2002},
	keywords = {age estimation, computer vision, face detection, face recognition, face tracking, face understanding vision system, facial feature point detection, feature extraction, gender estimation, human faces, race recognition, wavelet transform, wavelet transforms},
	pages = {27--32}
},

@inproceedings{shan_du_robust_2007,
	title = {A Robust Approach for Eye Localization Under Variable Illuminations},
	volume = {1},
	isbn = {1522-4880},
	doi = {10.1109/ICIP.2007.4378970},
	abstract = {Illumination variation is a main obstacle in facial feature detection. This paper presents a novel automated approach that localizes eyes in gray-scale face images and that is robust to illumination changes. The approach does not require prior knowledge about face orientation and illumination strength. Other advantages are that no initialization and training process are needed. Based on an edge map obtained via multi-resolution wavelet transform, this approach first segments an image into different inhomogeneously illuminated regions. The illumination of every region is then adjusted so that the features' details are more pronounced. To locate the different facial features, for every region, Gabor-based image is constructed from the re-lit image. The eyes sub-regions are then identified using the edge map of the re-lit image. This method has been applied successfully to the images of the Yale B face database that have different illuminations.},
	author = {Shan Du and R. Ward},
	year = {2007},
	keywords = {biometrics, edge detection, eye detection, eye localization, face orientation, face recognition, facial feature detection, feature extraction, Gabor-based image, gray-scale face images, illumination strength, image colour analysis, image resolution, lighting, multiresolution wavelet transform, variable illuminations, wavelet transforms, wavelets, Yale B face database},
	pages = {I -- 377-I - 380}
},

@article{mitra_gesture_2007,
	title = {Gesture Recognition: A Survey},
	volume = {37},
	issn = {1094-6977},
	shorttitle = {Gesture Recognition},
	doi = {10.1109/TSMCC.2007.893280},
	abstract = {Gesture recognition pertains to recognizing meaningful expressions of motion by a human, involving the hands, arms, face, head, and/or body. It is of utmost importance in designing an intelligent and efficient human-computer interface. The applications of gesture recognition are manifold, ranging from sign language through medical rehabilitation to virtual reality. In this paper, we provide a survey on gesture recognition with particular emphasis on hand gestures and facial expressions. Applications involving hidden Markov models, particle filtering and condensation, finite-state machines, optical flow, skin color, and connectionist models are discussed in detail. Existing challenges and future research possibilities are also highlighted},
	number = {3},
	journal = {Systems, Man, and Cybernetics, Part C: Applications and Reviews, {IEEE} Transactions on},
	author = {S. Mitra and T. Acharya},
	year = {2007},
	keywords = {condensation, connectionist models, face recognition, facial expressions, finite state machines, finite-state machines, gesture recognition, hand gestures, hidden Markov models, hidden Markov models {(HMMs),} human computer interaction, image color analysis, image sequences, intelligent human-computer interface, optical flow, particle filtering, skin color, soft computing},
	pages = {311--324}
},

@inproceedings{rurainsky_image_2007,
	title = {Image Processing, 2007. {ICIP} 2007. {IEEE} International Conference on},
	volume = {3},
	isbn = {1522-4880},
	doi = {10.1109/ICIP.2007.4379249},
	abstract = {We present our system for the capturing and analysis of {3D} facial motion. A high speed camera is used as capture unit in combination with two surface mirrors. The mirrors provide two additional virtual views of the face without the need of multiple cameras and to avoid synchronization problems. We use this system to capture the motion of a person's face while speaking. Investigations of these facial motions are presented and rigid and non-rigid motion are analyzed. In order to extract only facial deformation independent from head pose, we use a new and simple approach for separating rigid and non-rigid motion named weight-compensated motion estimation {(WCME).} This approach weights the data points according to their influence to the desired motion model. We also present first results of our model-based facial deformation analysis. Such results can be used for facial animations in order to achieve a higher degree of quality.},
	author = {J. Rurainsky and P. Eisert},
	year = {2007},
	keywords = {{3D} facial motion analysis, {3D} modeling \& synthesis, face recognition, facial animation, facial deformation, facial deformation extraction, high speed camera, mirror, mirror-based multiview analysis, mirrors, motion compensation, motion estimation, multiview, nonrigid motion, parametric models for motion estimation, rigid motion, weight-compensated motion estimation},
	pages = {III -- 73-III - 76}
},

@inproceedings{nunamaker_hmm-based_2005,
	title = {{HMM-Based} Deception Recognition from Visual Cues},
	doi = {10.1109/ICME.2005.1521550},
	abstract = {Behavioral indicators of deception and behavioral state are extremely difficult for humans to analyze. This research effort attempts to leverage automated systems to augment humans in detecting deception by analyzing nonverbal behavior on video. By tracking faces and hands of an individual, it is anticipated that objective behavioral indicators of deception can be isolated, extracted and synthesized to create a more accurate means for detecting human deception. Blob analysis, a method for analyzing the movement of the head and hands based on the identification of skin color is presented. A proof-of-concept study is presented that uses Blob analysis to extract visual cues and events, throughout the examined videos. The integration of these cues is done using a hierarchical hidden Markov model to explore behavioral state identification in the detection of deception, mainly involving the detection of agitated and over-controlled behaviors},
	author = {{J.F.} Nunamaker and G. Tsechpenakis and D. Metaxas and M. Adkins and J. Kruse and {J.K.} Burgoon and {M.L.} Jensen and T. Meservy and {D.P.} Twitchell and A. Deokar},
	year = {2005},
	keywords = {automated system, behavioral state identification, Blob analysis, face recognition, face tracking, feature extraction, hand tracking, hidden Markov model, hidden Markov models, {HMM-based} deception recognition, human deception detection, objective behavioral indicator, Skin, skin color identification, tracking, visual cue extraction},
	pages = {824--827}
},

@inproceedings{fukuda_detecting_2008,
	title = {Detecting Emotions and Dangerous Actions for Better {Human-System} Team Working},
	doi = {10.1109/SSIRI.2008.55},
	abstract = {As situations change very often and extensively today, design is moving toward user-centric because designers cannot foresee the operating conditions and it is a user who knows the situations. Thus, what is called for in a system is to help a user understand the situation better and to help him or her to respond better. Thus, human characteristics must be considered more in a system design. But recent brain studies reveal that in an unexpected situation, humanpsilas brain capability decreases considerably, down to the level of about 30\% of the normal state because a human becomes emotionally unstable. Therefore, it will be very important in our future system design to keep a user emotionally stable. We carried out a series of experiments and developed several methods to detect emotions from face, from voice and from body motion and to detect dangerous actions from body motion. Our immediate goal is to apply these techniques to realize safer driving.},
	author = {S. Fukuda},
	year = {2008},
	keywords = {brain study, Dangerous Action, Detection, emotion, emotion detection, emotion recognition, face detection, face recognition, human brain capability, human computer interaction, human-system team working, speech recognition, team working, user centred design, user-centric design},
	pages = {205--206}
},

@article{jingyu_yan_factorization-based_2008,
	title = {A {Factorization-Based} Approach for Articulated Nonrigid Shape, Motion and Kinematic Chain Recovery From Video},
	volume = {30},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2007.70739},
	abstract = {Recovering articulated shape and motion, especially human body motion, from video is a challenging problem with a wide range of applications in medical study, sport analysis, animation, and so forth. Previous work on articulated motion recovery generally requires prior knowledge of the kinematic chain and usually does not concern the recovery of the articulated shape. The nonrigidity of some articulated part, for example, human body motion with nonrigid facial motion, is completely ignored. We propose a factorization-based approach to recover the shape, motion, and kinematic chain of an articulated object with nonrigid parts altogether directly from video sequences under a unified framework. The proposed approach is based on our modeling of the articulated nonrigid motion as a set of intersecting motion subspaces. A motion subspace is the linear subspace of the trajectories of an object. It can model a rigid or nonrigid motion. The intersection of two motion subspaces of linked parts models the motion of an articulated joint or axis. Our approach consists of algorithms for motion segmentation, kinematic chain building, and shape recovery. It handles outliers and can be automated. We test our approach through synthetic and real experiments and demonstrate how to recover an articulated structure with nonrigid parts via a single-view camera without prior knowledge of its kinematic chain.},
	number = {5},
	journal = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {Jingyu Yan and M. Pollefeys},
	year = {2008},
	keywords = {{3D} scene analysis, articulated, articulated nonrigid shape recovery, computer vision, factorization method, factorization-based approach, image motion analysis, image segmentation, image sequences, kinematic chain, kinematic chain recovery, matrix decomposition, motion, motion recovery, motion segmentation, non-rigid, shape, video sequences, video signal processing},
	pages = {865--877}
},

@inproceedings{usman_saeed_multimedia_2006,
	title = {Multimedia Signal Processing, 2006 {IEEE} 8th Workshop on},
	doi = {10.1109/MMSP.2006.285262},
	abstract = {Face is considered as an attractive biometric but because of multiple sources of variabilities, the associated recognition rate is not high enough, when working on appearance only, for most of real applications. Considering that most available visual data are videos and not still images, we investigate in this article the possible contribution of some dynamic parameters (head displacements and mouth motion) in person recognition. Some preliminary results tend to validate this original proposal that opens some new perspectives in the possible design of future hybrid and efficient system combining appearance and dynamics of faces},
	author = {Usman Saeed and Federico Matta and {Jean-Luc} Dugelay},
	year = {2006},
	keywords = {biometric, biometrics (access control), dynamic parameters person recognition, face recognition, head-mouth dynamics},
	pages = {29--32}
},

@inproceedings{huchuan_lu_gender_2007,
	title = {Gender Recognition using Adaboosted Feature},
	volume = {2},
	doi = {10.1109/ICNC.2007.396},
	abstract = {In this paper, a novel approach for gender recognition combining the ellipse face images, Gabor filters, Adaboost learning and {SVM} classifier is proposed. Face representation based on Harr-like feature, Gabor feature or {ICA} is an effective method to extract facial appearance information. So we compare these three kinds of features selected by Adaboost method using {FERET} database. In the first experiment, several different preprocessing methods (face detector, warp face images and ellipse face images) have been compared, meanwhile comparing different feature extraction methods {(Gabor} wavelets, Haar-like wavelets, {PCA,} {ICA).The} experimental results show that our proposed approach (combination of ellipse face images, Gabor wavelets and {Ada+SVM} classifier) achieves better performance. The second experiment is tested on {PCA} and {ICA} feature extraction method with different explanation. It is shown that {ICA} is much steadier than {PCA} method when the explanation changed.},
	author = {Huchuan Lu and Hui Lin},
	year = {2007},
	keywords = {Adaboost learning, Adaboost method, ellipse face images, face recognition, face representation, {FERET} database, Gabor filters, gender recognition, Harr-like feature, principal component analysis, support vector machines, support vector machines classifier, visual databases},
	pages = {646--650}
},

@article{meservy_deception_2005,
	title = {Deception detection through automatic, unobtrusive analysis of nonverbal behavior},
	volume = {20},
	issn = {1541-1672},
	doi = {10.1109/MIS.2005.85},
	abstract = {Every day, hundreds of thousands of people pass through airport security checkpoints, border crossing stations, or other security screening measures. Security professionals must sift through countless interactions and ferret out high-risk individuals who represent a danger to other citizens. During each interaction, the security professional must decide whether the individual is being forthright or deceptive. This task is difficult because of the limits of human vigilance and perception and the small percentage of individuals who actually harbor hostile intent. Our research initiative is based on a behavioral approach to deception detection. We attempted to build an automated system that can infer deception or truthfulness from a set of features extracted from head and hands movements in a video. A validated and reliable behaviorally based deception analysis system could potentially have great impacts in augmenting humans' abilities to assess credibility. An automated, unobtrusive system identifies behavioral patterns that indicate deception from nonverbal behavioral cues and classifies deception and truth more accurately than many humans.},
	number = {5},
	journal = {Intelligent Systems, {IEEE}},
	author = {{T.O.} Meservy and {M.L.} Jensen and J. Kruse and {J.K.} Burgoon and {J.F.} Nunamaker and {D.P.} Twitchell and G. Tsechpenakis and {D.N.} Metaxas},
	year = {2005},
	keywords = {airport security checkpoints, automated deception detection system, behavioral pattern identification, behaviorally based deception analysis system, border crossing stations, decision support, face and gesture recognition, feature representation, nonverbal behavior automatic unobtrusive analysis, nonverbal behavioral cues, public administration, security, security professional, terrorism, video analysis},
	pages = {36--43}
},

@inproceedings{xuelong_li_gender_2007,
	title = {Gender recognition based on local body motions},
	doi = {10.1109/ICSMC.2007.4413898},
	abstract = {Human body motions, including gait information, are a promising biometrics resource. In this paper, the human silhouette is segmented into seven components for visual surveillance applications, namely, head, arm, body, thigh, front-leg, back-leg, and feet. The legs are classified as front-leg or back-leg because of the bipedal walking style: during walking, the left-leg and the right-leg are in front or at the back in turn. The motions of the individual components and of a number of combinations of components are then studied for gender recognition. For {HumanID} recognition under different cases, the performances of and underlying links amongst the seven human gait components are analyzed.},
	author = {Xuelong Li and S. Maybank and Dacheng Tao},
	year = {2007},
	keywords = {biometrics, biometrics (access control), bipedal walking, gait analysis, gait information, gender recognition, human body motion, human gait analysis, human silhouette segmentation, {HumanID} recognition, image classification, image motion analysis, image segmentation, local body motion, surveillance, visual surveillance application},
	pages = {3881--3886}
},

@inproceedings{poel_automatic_2008,
	title = {Automatic Face \& Gesture Recognition, 2008. {FG} '08. 8th {IEEE} International Conference on},
	shorttitle = {Meeting behavior detection in smart environments},
	abstract = {Unobtrusive and multiple-sensor interfaces enable observation of natural human behavior in smart environments. Being able to detect, analyze and interpret this activity allows for the implementation of various applications, including real-time surveillance and real-time support in smart home and office environments. We focus on smart meeting environments, where nonverbal behavioral cues sometimes tell more about issues such as discussion participation, involvement and contribution, than information obtained from verbal contributions. An important aspect of this behavior is the interaction between meeting participants. We regard the special case where some of the participants are at physically different locations, which hinders natural interaction. We discuss how we can exploit the ability of a sensor-equipped environment to detect nonverbal interaction cues and use these to allow and improve natural interaction between collaborating participants at distributed locations. We focus on research efforts to detect nonverbal interaction cues by looking at various modalities. Also, we discuss how information obtained from the fusion of these modalities allows us to generate and display behavioral cues that allow remote participants to take part in a distributed meeting in a natural way.},
	author = {M. Poel and R. Poppe and A. Nijholt},
	year = {2008},
	keywords = {behavior detection, multiple-sensor interfaces, natural interaction, nonverbal cues, nonverbal interaction cues, real-time support, real-time surveillance, sensor fusion, sensor-equipped environment, smart home, smart meeting environments, smart office, user interfaces},
	pages = {1--6}
},

@inproceedings{kanaujia_emblem_2006,
	title = {Emblem Detections by Tracking Facial Features},
	doi = {10.1109/CVPRW.2006.69},
	abstract = {Tracking facial features across large head rotations is a challenging research problem. Both {2D} and {3D} model based approaches have been proposed for feature analysis from multiple views. Accurate feature tracking enables useful video processing applications like emblem detection(an event or movement that symbolizes an idea), facial expressions recognition, morphing and synthesis. A crucial requirement is generalizability of the tracking framework across appearance variations, presence of facial hair and illumination changes. We propose a framework to detect emblems that combines active shape model with a predictive face aspect model to track features across large head movements and runs close to real time. Active Shape {Model(ASM)} is a deformable model for shape registration that detect facial features by combining prior shape information with the observed image data. Our view based framework represents various head poses by multiple {2D} shape models and accounts for large head rotations by dynamically switching between them. Our switching variable (the current model to use) is discriminatively predicted from the {SIFT} descriptors computed over the bounding box of low resolution face image. We demonstrate the use of tracking framework to recognize high level events like head nodding, shaking and eye blinking.},
	booktitle = {Computer Vision and Pattern Recognition Workshop, 2006. {CVPRW} '06. Conference on},
	author = {A. Kanaujia and Yuchi Huang and D. Metaxas},
	year = {2006},
	pages = {108}
},

@inproceedings{jung-eun_lee_scars_2008,
	title = {Scars, marks and tattoos {(SMT):} Soft biometric for suspect and victim identification},
	shorttitle = {Scars, marks and tattoos {(SMT)}},
	doi = {10.1109/BSYM.2008.4655515},
	abstract = {Scars, marks and tattoos {(SMT)} are being increasingly used for suspect and victim identification in forensics and law enforcement agencies. Tattoos, in particular, are getting serious attention because of their visual and demographic characteristics as well as their increasing prevalence. However, current tattoo matching procedure requires human-assigned class labels in the {ANSI/NIST} {ITL} 1-2000 standard which makes it time consuming and subjective with limited retrieval performance. Further, tattoo images are complex and often contain multiple objects with large intra-class variability, making it very difficult to assign a single category in the {ANSI/NIST} standard. We describe a content-based image retrieval {(CBIR)} system for matching and retrieving tattoo images. Based on scale invariant feature transform {(SIFT)} features extracted from tattoo images and optional accompanying demographical information, our system computes feature-based similarity between the query tattoo image and tattoos in the criminal database. Experimental results on two different tattoo databases show encouraging results.},
	booktitle = {Biometrics Symposium, 2008. {BSYM} '08},
	author = {{Jung-Eun} Lee and {A.K.} Jain and Rong Jin},
	year = {2008},
	keywords = {{ANSI/NIST} {ITL} 1-2000 standard, biometrics (access control), content-based image retrieval, content-based retrieval, criminal database, demographical information, demography, feature extraction, feature-based similarity, forensics, image matching, image retrieval, law administration, law enforcement agencies, police data processing, scale invariant feature transform, scars, marks and tattoos, soft biometric, suspect-victim identification, tattoo databases, tattoo matching procedure, transforms, visual databases},
	pages = {1--8}
},

@inproceedings{sangho_park_segmentation_2002,
	title = {Segmentation and tracking of interacting human body parts under occlusion and shadowing},
	doi = {10.1109/MOTION.2002.1182221},
	abstract = {The paper presents a system to segment and track multiple body parts of interacting humans in the presence of mutual occlusion and shadow. The color image sequence is processed at three levels: pixel level, blob level, and object level. A Gaussian mixture model is used at the pixel level to train and classify individual pixel colors. A Markov random field {(MRF)} framework is used at the blob level to merge the pixels into coherent blobs and to register inter-blob relations. A coarse model of the human body is applied at the object level as empirical domain knowledge to resolve ambiguity due to occlusion and to recover from intermittent tracking failures. A two-fold tracking scheme is used which consists of blob to blob matching in consecutive frames and blob to body part association within a frame. The tracking scheme resembles a multi-target, multi-assignment framework. The result is a tracking system that simultaneously segments and tracks multiple body parts of interacting people. Example sequences illustrate the success of the proposed paradigm.},
	booktitle = {Motion and Video Computing, 2002. Proceedings. Workshop on},
	author = {Sangho Park and {J.K.} Aggarwal},
	year = {2002},
	keywords = {blob level processing, body part segmentation, body part tracking, color image sequence, Gaussian mixture model, Gaussian processes, hidden feature removal, image classification, image colour analysis, image matching, image registration, image segmentation, image sequences, learning (artificial intelligence), Markov processes, Markov random field, object detection, object level processing, occlusion, optical tracking, pixel level processing, shadowing, surveillance, video signal processing, video surveillance},
	pages = {105--111}
},

@inproceedings{collins_silhouette-based_2002,
	title = {Silhouette-based human identification from body shape and gait},
	doi = {10.1109/AFGR.2002.1004181},
	abstract = {Our goal is to establish a simple baseline method for human identification based on body shape and gait. This baseline recognition method provides a lower bound against which to evaluate more complicated procedures. We present a viewpoint-dependent technique based on template matching of body silhouettes. Cyclic gait analysis is performed to extract key frames from a test sequence. These frames are compared to training frames using normalized correlation, and subject classification is performed by nearest-neighbor matching among correlation scores. The approach implicitly captures biometric shape cues such as body height, width, and body-part proportions, as well as gait cues such as stride length and amount of arm swing. We evaluate the method on four databases with varying viewing angles, background conditions (indoors and outdoors), walking styles and pixels on target.},
	booktitle = {Automatic Face and Gesture Recognition, 2002. Proceedings. Fifth {IEEE} International Conference on},
	author = {{R.T.} Collins and R. Gross and Jianbo Shi},
	year = {2002},
	keywords = {arm swing, background conditions, baseline recognition method, biometric shape cues, biometrics (access control), body height, body shape, body width, body-part proportions, computer vision, correlation methods, correlation scores, cyclic gait analysis, databases, gait analysis, gait cues, image classification, image matching, image motion analysis, image sequences, indoor conditions, key frame extraction, lower bound, nearest-neighbor matching, normalized correlation, outdoor conditions, pixels, shape measurement, silhouette-based human identification, stride length, subject classification, template matching, test image sequence, training frames, video databases, viewing angles, viewpoint-dependent technique, walking styles},
	pages = {366--371}
},

@inproceedings{kawade_vision-based_2002,
	title = {Vision-based face understanding technologies and applications},
	doi = {10.1109/MHS.2002.1058006},
	abstract = {When people communicate with each other directly (face to face), the role of the visual information is very essential, especially facial images which include the most important information. If artificial vision systems that can understand human faces are realized, they will enable machines to communicate with people more effectively. For this purpose, we have been continuing the research and development of vision-based face understanding technologies, such as face detection, facial feature point detection, face recognition, face tracking, encoding and transmission of facial motion, face situation estimation, gender and age estimation, etc. This paper introduces these technologies and their applications.},
	booktitle = {Micromechatronics and Human Science, 2002. {MHS} 2002. Proceedings of 2002 International Symposium on},
	author = {M. Kawade},
	year = {2002},
	keywords = {age estimation, computer vision, face detection, face recognition, face tracking, face understanding vision system, facial feature point detection, feature extraction, gender estimation, human faces, race recognition, wavelet transform, wavelet transforms},
	pages = {27--32}
},

@inproceedings{funahashi_face_2007,
	title = {Face and eye tracking for gaze analysis},
	doi = {10.1109/ICCAS.2007.4406545},
	abstract = {We propose a system for extracting eye gaze information, and develop a couple of new human interface media by eye gaze information. For example, the observer wants to know visually how effectively the partner is interesting to him or not even on the net environment. To do this it is usually expected to utilize motion images. But, the observer wants to know exclusively facial image and gaze. So, we paid attention to face tracking from the scene and analyzed the features of eye gaze pattern extracted from the facial images by iris recognition.},
	booktitle = {Control, Automation and Systems, 2007. {ICCAS} '07. International Conference on},
	author = {T. Funahashi and T. Fujiwara and H. Koshimizu},
	year = {2007},
	keywords = {eye gaze information extraction, eye gaze patterns, Eye tracking, face recognition, face tracking, facial image, feature extraction, gaze analysis, Gaze Mark Pattern, human interface media, image motion analysis, iris recognition, motion images},
	pages = {1337--1341}
},

@inproceedings{yinggang_xie_facial_2006,
	title = {Facial and Eye Detection and Application in Affective Recognition},
	doi = {10.1109/CHICC.2006.280890},
	abstract = {Affective computing is becoming a new research hotspot. An effective method of facial and eye detection is presented in this paper, which uses skin-color model and key facial feature points, based on image processing and expression recognition. Finally the realization is introduced that the solution has been applied to the e-learning system to cope with the emotion and cognition of the student interest},
	booktitle = {Control Conference, 2006. {CCC} 2006. Chinese},
	author = {Yinggang Xie and Zhiliang Wang and Ning Cheng and Guojiang Wang and M. Nagai},
	year = {2006},
	keywords = {affective computing, affective recognition, Artificial Psychology, e-learning system, expression recognition, Eye, eye detection, face recognition, Facial Detection, image colour analysis, image processing, object detection, skin-color model},
	pages = {1942--1946}
},

@inproceedings{rurainsky_mirror-based_2007,
	title = {{Mirror-Based} {Multi-View} Analysis of Facial Motions},
	volume = {3},
	isbn = {1522-4880},
	doi = {10.1109/ICIP.2007.4379249},
	abstract = {We present our system for the capturing and analysis of {3D} facial motion. A high speed camera is used as capture unit in combination with two surface mirrors. The mirrors provide two additional virtual views of the face without the need of multiple cameras and to avoid synchronization problems. We use this system to capture the motion of a person's face while speaking. Investigations of these facial motions are presented and rigid and non-rigid motion are analyzed. In order to extract only facial deformation independent from head pose, we use a new and simple approach for separating rigid and non-rigid motion named weight-compensated motion estimation {(WCME).} This approach weights the data points according to their influence to the desired motion model. We also present first results of our model-based facial deformation analysis. Such results can be used for facial animations in order to achieve a higher degree of quality.},
	booktitle = {Image Processing, 2007. {ICIP} 2007. {IEEE} International Conference on},
	author = {J. Rurainsky and P. Eisert},
	year = {2007},
	keywords = {{3D} facial motion analysis, {3D} modeling \& synthesis, face recognition, facial animation, facial deformation, facial deformation extraction, high speed camera, mirror, mirror-based multiview analysis, mirrors, motion compensation, motion estimation, multiview, nonrigid motion, parametric models for motion estimation, rigid motion, weight-compensated motion estimation},
	pages = {III -- 73-III - 76}
},

@article{yeasin_recognition_2006-1,
	title = {Recognition of facial expressions and measurement of levels of interest from video},
	volume = {8},
	issn = {1520-9210},
	doi = {10.1109/TMM.2006.870737},
	abstract = {This paper presents a spatio-temporal approach in recognizing six universal facial expressions from visual data and using them to compute levels of interest. The classification approach relies on a two-step strategy on the top of projected facial motion vectors obtained from video sequences of facial expressions. First a linear classification bank was applied on projected optical flow vectors and decisions made by the linear classifiers were coalesced to produce a characteristic signature for each universal facial expression. The signatures thus computed from the training data set were used to train discrete hidden Markov models {(HMMs)} to learn the underlying model for each facial expression. The performances of the proposed facial expressions recognition were computed using five fold cross-validation on {Cohn-Kanade} facial expressions database consisting of 488 video sequences that includes 97 subjects. The proposed approach achieved an average recognition rate of 90.9\% on {Cohn-Kanade} facial expressions database. Recognized facial expressions were mapped to levels of interest using the affect space and the intensity of motion around apex frame. Computed level of interest was subjectively analyzed and was found to be consistent with "ground truth" information in most of the cases. To further illustrate the efficacy of the proposed approach, and also to better understand the effects of a number of factors that are detrimental to the facial expression recognition, a number of experiments were conducted. The first empirical analysis was conducted on a database consisting of 108 facial expressions collected from {TV} broadcasts and labeled by human coders for subsequent analysis. The second experiment (emotion elicitation) was conducted on facial expressions obtained from 21 subjects by showing the subjects six different movies clips chosen in a manner to arouse spontaneous emotional reactions that would produce natural facial expressions.},
	number = {3},
	journal = {Multimedia, {IEEE} Transactions on},
	author = {M. Yeasin and B. Bullot and R. Sharma},
	year = {2006},
	keywords = {{Cohn-Kanade} facial expressions database, discrete hidden Markov model training, emotion recognition, Emotions, empirical analysis, face detection, face recognition, facial motion vector projection, hidden Markov models, hidden Markov models {(HMMs),} {HMM,} image motion analysis, image sequences, learning (artificial intelligence), levels of interest, levels of interest measurement, linear classification bank, machine learning, optical flow vector projection, pattern classification, spatio-temporal approach, universal facial expression recognition, universal facial expressions, video sequences, visual databases},
	pages = {500--508}
}?
@inproceedings{russell_british_2007,
	title = {British Machine Vision Conference},
	author = {David Russell and Shaogang Gong},
	year = {2007}
},

@inproceedings{ozden_background_2005,
	title = {Background Recognition in Dynamic Scenes with Motion Constraints},
	isbn = {0-7695-2372-2},
	url = {http://portal.acm.org/citation.cfm?id=1068922},
	abstract = {Consider a monocular image sequence which contains independently moving objects and assume it is already segmented. In order to get a realistic {3D} reconstruction of such a scene, we have to solve the relative scale ambiguity between the reconstructions of different moving objects. Recently, we demonstrated the usefulness of the so-called ýnon-accidentalness and independence constraintsý to disambiguate the mentioned unknown relative scale. However, this technique requires that the video segment which corresponds to the background is known beforehand. In this paper, we analyze the background detection problem in the vein of the aforementioned constraints and show that the background is not just another moving object but the one which results in the simplest overall scene interpretation.},
	publisher = {{IEEE} Computer Society},
	author = {K. E. Ozden and L. Van Gool},
	year = {2005},
	pages = {250--255}
},

@inproceedings{collins_automatic_2002,
	title = {Automatic Face and Gesture Recognition, 2002. Proceedings. Fifth {IEEE} International Conference on},
	doi = {10.1109/AFGR.2002.1004181},
	abstract = {Our goal is to establish a simple baseline method for human identification based on body shape and gait. This baseline recognition method provides a lower bound against which to evaluate more complicated procedures. We present a viewpoint-dependent technique based on template matching of body silhouettes. Cyclic gait analysis is performed to extract key frames from a test sequence. These frames are compared to training frames using normalized correlation, and subject classification is performed by nearest-neighbor matching among correlation scores. The approach implicitly captures biometric shape cues such as body height, width, and body-part proportions, as well as gait cues such as stride length and amount of arm swing. We evaluate the method on four databases with varying viewing angles, background conditions (indoors and outdoors), walking styles and pixels on target.},
	author = {{R.T.} Collins and R. Gross and Jianbo Shi},
	year = {2002},
	keywords = {arm swing, background conditions, baseline recognition method, biometric shape cues, biometrics (access control), body height, body shape, body width, body-part proportions, computer vision, correlation methods, correlation scores, cyclic gait analysis, databases, gait analysis, gait cues, image classification, image matching, image motion analysis, image sequences, indoor conditions, key frame extraction, lower bound, nearest-neighbor matching, normalized correlation, outdoor conditions, pixels, shape measurement, silhouette-based human identification, stride length, subject classification, template matching, test image sequence, training frames, video databases, viewing angles, viewpoint-dependent technique, walking styles},
	pages = {366--371}
},

@inproceedings{yijun_xiao_topological_2003,
	title = {A topological approach for segmenting human body shape},
	doi = {10.1109/ICIAP.2003.1234029},
	abstract = {Segmentation of a {3D} human body, is a very challenging problem in applications exploiting human scan data. To tackle this problem, the paper proposes a topological approach based on the discrete Reeb graph {(DRG)} which is an extension of the classical Reeb graph to handle unorganized clouds of {3D} points. The essence of the approach concerns detecting critical nodes in the {DRG,} thereby permitting the extraction of branches that represent parts of the body. Because the human body shape representation is built upon global topological features that are preserved so long as the whole structure of the human body does not change, our approach is quite robust against noise, holes, irregular sampling, frame change and posture variation. Experimental results performed on real scan data demonstrate the validity of our method.},
	author = {Yijun Xiao and N. Werghi and P. Siebert},
	year = {2003},
	keywords = {{3D} human body image segmentation, discrete Reeb graph, frame change, global topological features, graph theory, holes, human body shape segmentation, human scan data, image sampling, image segmentation, imaging, irregular sampling, noise, posture variation},
	pages = {82--87}
},

@inproceedings{hahnel_color_2004,
	title = {Color and texture features for person recognition},
	volume = {1},
	isbn = {1098-7576},
	abstract = {The need for automatic visual surveillance is increasing and the research on person recognition systems is more and more supported. As many biometric recognition methods, e.g. face recognition, are based on quite high camera resolutions which are not available in many situations, we examine features as well as classifier techniques for full body recognition. We present our experiments with color and texture features in the application of full body person recognition. On a database of 53 individuals we tested approved features for object recognition as well as {MPEG7} color and texture descriptors on a person recognition task. For comparison, we used an {RBF} network classifier as well as a nearest-neighbor classifier. Our experiments showed that color as well as texture information is important for a person recognition system. Additionally, a combination of these two kind of features results in a performance improvement.},
	author = {M. Hahnel and D. Klunder and {K.-F.} Kraiss},
	year = {2004},
	keywords = {automatic visual surveillance, biometric recognition method, classifier technique, color feature, full body person recognition, image colour analysis, image recognition, image texture, nearest-neighbor classifier, object recognition, person recognition system, radial basis function, radial basis function networks, {RBF} network classifier, surveillance, texture feature},
	pages = {652}
},

@inproceedings{srinivasan_bottom-up_2007,
	title = {Bottom-up Recognition and Parsing of the Human Body},
	doi = {10.1109/CVPR.2007.383301},
	abstract = {Recognizing humans, estimating their pose and segmenting their body parts are key to high-level image understanding. Because humans are highly articulated, the range of deformations they undergo makes this task extremely challenging. Previous methods have focused largely on heuristics or pairwise part models in approaching this problem. We propose a bottom-up parsing of increasingly more complete partial body masks guided by a parse tree. At each level of the parsing process, we evaluate the partial body masks directly via shape matching with exemplars, without regard to how the parses are formed. The body is evaluated as a whole, not the sum of its constituent parses, unlike previous approaches. Multiple image segmentations are included at each of the levels of the parsing, to augment existing parses or to introduce ones. Our method yields both a pose estimate as well as a segmentation of the human. We demonstrate competitive results on this challenging task with relatively few training examples on a dataset of baseball players with wide pose variation. Our method is comparatively simple and could be easily extended to other objects.},
	author = {P. Srinivasan and Jianbo Shi},
	year = {2007},
	keywords = {body parts segmentation, bottom-up recognition, gesture recognition, human body parsing, image matching, image segmentation, multiple image segmentations, parsing process},
	pages = {1--8}
},

@inproceedings{todorovic_detection_2004,
	title = {Detection of Artificial Structures in {Natural-Scene} Images Using Dynamic Trees},
	isbn = {0-7695-2128-2},
	url = {http://portal.acm.org/citation.cfm?id=1020433},
	abstract = {We seek a framework that addresses localization, detection and recognition of man-made objects in natural-scene images in a unified manner. We propose to model artificial structures by dynamic tree-structured belief networks {(DTS-BNs).} {DTSBNs} provide for a distribution over tree structures that we learn using our Structured Approximation {(SVA)} inference algorithm. Furthermore, we propose multi-scale linear-discriminant analysis {(MLDA)} as a feature extraction method, which appears well suited for our goals, as we assume that man-made objects are characterized primarily by geometric regularities and by patches of uniform color. {MLDA} extracts edges over a finite range of locations, orientations and scales, decomposing an image into dyadic squares. Both the color of dyadic squares and the geometric properties of extracted edges represent observable input to our {DTSBNs.} Experimental results demonstrate that {DTS-BNs,} trained on {MLDA} features, offer a viable solution for detection of artificial structures in natural-scene images.},
	publisher = {{IEEE} Computer Society},
	author = {Sinisa Todorovic and Michael C. Nechyba},
	year = {2004},
	pages = {35--39}
},

@misc{_ultrasonic_????,
	title = {Ultrasonic range finder},
	url = {http://www.freepatentsonline.com/7330398.html}
},

@article{ren_statistical_2003,
	title = {Statistical background modeling for non-stationary camera},
	volume = {24},
	url = {http://portal.acm.org/citation.cfm?id=641699},
	abstract = {A new background subtraction method is proposed in this paper for the foreground detection from a non-stationary background. Usually, motion compensation is required when applying background subtraction to a non-stationary background. In practice, it is difficult to realize this to sufficient pixel accuracy. The problem is further complicated when the moving objects to be detected/tracked are small, since the pixel error in motion compensating the background will hide the small targets. A spatial distribution of Gaussians model is proposed to deal with moving object detection where the motion compensation is not exact but approximated. The distribution of each background pixel is temporally and spatially modeled. Based on this statistical model, a pixel in the current frame is classified as belonging to the foreground or background. In addition, a new background restoration and adaptation algorithm is developed for the non-stationary background over an extended period of time. Test cases involving a surveillance system to detect small moving objects (human and car) within a highly textured background and a pan-tilt human tracking system are demonstrated successfully.},
	number = {1-3},
	journal = {Pattern Recogn. Lett.},
	author = {Ying Ren and {Chin-Seng} Chua and {Yeong-Khing} Ho},
	year = {2003},
	keywords = {background restoration, foreground detection, non-stationary background, object tracking, sdg model},
	pages = {183--196}
},

@inproceedings{hwang_automatic_2006,
	title = {An automatic three-dimensional human behavior analysis system for video surveillance applications},
	doi = {10.1109/ISCAS.2006.1692632},
	abstract = {We developed a system for automatic {3-D} human body motion in video surveillance sequences. As the first step, the human body that will be used for the analysis is extracted and tracked based on video object {(VO)} segmentation technique. Once human body is extracted, this {2-D} object, which consists of the merging of several regions, is then analyzed for activity and body part interpretation. The specific {2-D} human body shape along with the information derived from maximum probable explanation {(MPE)} of dynamic Bayesian modeling is then effectively utilized to obtain {3-D} body model frame-by-frame so that the {3-D} motion trajectory of the human object can be derived for any future event analysis, which is independent of the viewing perspective of the {2-D} video.},
	author = {{J.-N.} Hwang and I. Karliga and {H.-Y.} Cheng},
	year = {2006},
	keywords = {{2D} human body shape, {2D} video, {3D} motion trajectory, automatic {3D} human body motion, Bayes methods, dynamic Bayesian modeling, feature extracted, image motion analysis, image segmentation, image sequences, maximum probable explanation, surveillance, video object segmentation technique, video surveillance sequences},
	pages = {4 pp.}
},

@inproceedings{zhang_distributed_2008,
	title = {Distributed Smart Cameras, 2008. {ICDSC} 2008. Second {ACM/IEEE} International Conference on},
	doi = {10.1109/ICDSC.2008.4635727},
	abstract = {In this paper, we present a clothing recognition system that augments clothes recommendation and fashion exploration using the intelligent multi-view vision technology of the Responsive Mirror, an implicitly controlled human-computer interaction system for clothes fitting rooms. The Responsive Mirror provides shoppers with real-time “self” and “social” clothes comparisons. The system recommends clothing that is “similar” and “different” than the clothing that the person is trying on in the mirror. The goal of the research in this paper is to create a recommendation system that uses a definition of “similar” and “different” that matches human perception. We address the social nature of the recognition problem by conducting a user study to identify the salient clothes factors that people use to determine clothes similarity. We describe the computer vision and machine learning techniques employed to recognize the factors that human eyes perceive in term of clothing similarity from frontal-view outfit images. We describe the key components of the motion-tracking and clothes-recognition systems and evaluate their performance by user study and experiments on a simulated clothes fitting image dataset. The approach and results presented here will benefit designers and developers of similar applications in the future.},
	author = {Wei Zhang and Bo Begole and Maurice Chu and Juan Liu and Nicholas Yee},
	year = {2008},
	keywords = {clothes recognition, fashion, human-computer interaction, motion tracking, Multi-view vision},
	pages = {1--10}
},

@inproceedings{lee_biometrics_2008,
	title = {Biometrics Symposium, 2008. {BYSYM} '08},
	shorttitle = {Scars, marks and tattoos {(SMT)}},
	doi = {10.1109/BSYM.2008.4655515},
	abstract = {Scars, marks and tattoos {(SMT)} are being increasingly used for suspect and victim identification in forensics and law enforcement agencies. Tattoos, in particular, are getting serious attention because of their visual and demographic characteristics as well as their increasing prevalence. However, current tattoo matching procedure requires human-assigned class labels in the {ANSI/NIST} {ITL} 1–2000 standard which makes it time consuming and subjective with limited retrieval performance. Further, tattoo images are complex and often contain multiple objects with large intra-class variability, making it very difficult to assign a single category in the {ANSI/NIST} standard. We describe a content-based image retrieval {(CBIR)} system for matching and retrieving tattoo images. Based on Scale Invariant Feature Transform {(SIFT)} features extracted from tattoo images and optional accompanying demographical information, our system computes feature-based similarity between the query tattoo image and tattoos in the criminal database. Experimental results on two different tattoo databases show encouraging results.},
	author = {{Jung-Eun} Lee and Anil K. Jain and Rong Jin},
	year = {2008},
	pages = {1--8}
},

@inproceedings{matta_behavioural_2006,
	title = {A Behavioural Approach to Person Recognition},
	doi = {10.1109/ICME.2006.262817},
	abstract = {This paper describes a new approach for identity recognition using video sequences. While most image and video recognition systems discriminate identities using physical information only, our approach exploits the behavioral information of head dynamics; in particular the displacement signals of few head features directly extracted in the image plane. Due to the lack of standard video database, identification and verification scores have been obtained using a small collection of video sequences: the results for this new approach are nevertheless promising},
	author = {F. Matta and {J.-L.} Dugelay},
	year = {2006},
	keywords = {feature extraction, head feature extraction, image recognition, image sequences, person recognition, video sequence, video signal processing},
	pages = {1461--1464}
},

@inproceedings{adam_aggregated_2006,
	title = {Aggregated Dynamic Background Modeling},
	isbn = {1522-4880},
	doi = {10.1109/ICIP.2006.312881},
	abstract = {Standard practices in background modeling learn a separate model for every pixel in the image. However, in dynamic scenes the connection between an observation and the place where it was observed is much less important and is usually random. For example, a wave observed in an ocean scene could easily have been observed at another place in the image. Moreover, during a limited learning period, we cannot expect to observe at every pixel all the possible background behaviors. We therefore develop in this paper a background model in which observations are decoupled from the place in the image where they were observed. A single non-parametric model is used to describe the dynamic region of the scene, aggregating the observations from the whole region. Using high-order features, we demonstrate the feasibility of our approach on challenging ocean scenes using only grayscale information},
	author = {A. Adam and E. Rivlin and I. Shimshoni},
	year = {2006},
	keywords = {background modeling, dynamic background modeling, dynamic backgrounds, feature extraction, grayscale information, image processing, nonparametric model, observation aggregation, ocean scene, video signal processing, video surveillance},
	pages = {3313--3316}
},

@inproceedings{haibin_ling_study_2007,
	title = {A Study of Face Recognition as People Age},
	isbn = {1550-5499},
	doi = {10.1109/ICCV.2007.4409069},
	abstract = {In this paper we study face recognition across ages within a real passport photo verification task. First, we propose using the gradient orientation pyramid for this task. Discarding the gradient magnitude and utilizing hierarchical techniques, we found that the new descriptor yields a robust and discriminative representation. With the proposed descriptor, we model face verification as a two-class problem and use a support vector machine as a classifier. The approach is applied to two passport data sets containing more than 1,800 image pairs from each person with large age differences. Although simple, our approach outperforms previously tested Bayesian technique and other descriptors, including the intensity difference and gradient with magnitude. In addition, it works as well as two commercial systems. Second, for the first time, we empirically study how age differences affect recognition performance. Our experiments show that, although the aging process adds difficulty to the recognition task, it does not surpass illumination or expression as a confounding factor.},
	author = {Haibin Ling and S. Soatto and N. Ramanathan and {D.W.} Jacobs},
	year = {2007},
	keywords = {Bayesian technique, discriminative representation, face recognition, face verification, gradient magnitude, gradient orientation pyramid, hierarchical techniques, intensity difference, passport photo verification task, support vector machine, support vector machines},
	pages = {1--8}
},

@article{oliver_bayesian_2000,
	title = {A Bayesian computer vision system for modeling human interactions},
	volume = {22},
	issn = {0162-8828},
	doi = {10.1109/34.868684},
	abstract = {We describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task. The system deals in particularly with detecting when interactions between people occur and classifying the type of interaction. Examples of interesting interaction behaviors include following another person, altering one's path to meet another, and so forth. Our system combines top-down with bottom-up information in a closed feedback loop, with both components employing a statistical Bayesian approach. We propose and compare two different state-based learning architectures, namely, {HMMs} and {CHMMs} for modeling behaviors and interactions. Finally, a synthetic {“Alife-style”} training system is used to develop flexible prior models for recognizing human interactions. We demonstrate the ability to use these a priori models to accurately classify real human behaviors and interactions with no additional tuning or training},
	number = {8},
	journal = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {{N.M.} Oliver and B. Rosario and {A.P.} Pentland},
	year = {2000},
	keywords = {Bayes method, Bayes methods, computer vision, hidden Markov model, hidden Markov models, human behavior recognition, image segmentation, learning systems, machine learning, object recognition, pattern recognition, people detection, real-time systems, surveillance, visual surveillance},
	pages = {831--843}
},

@article{ming-hsuan_yang_detecting_2002,
	title = {Detecting faces in images: a survey},
	volume = {24},
	issn = {0162-8828},
	shorttitle = {Detecting faces in images},
	doi = {10.1109/34.982883},
	abstract = {Images containing faces are essential to intelligent vision-based human-computer interaction, and research efforts in face processing include face recognition, face tracking, pose estimation and expression recognition. However, many reported methods assume that the faces in an image or an image sequence have been identified and localized. To build fully automated systems that analyze the information contained in face images, robust and efficient face detection algorithms are required. Given a single image, the goal of face detection is to identify all image regions which contain a face, regardless of its {3D} position, orientation and lighting conditions. Such a problem is challenging because faces are non-rigid and have a high degree of variability in size, shape, color and texture. Numerous techniques have been developed to detect faces in a single image, and the purpose of this paper is to categorize and evaluate these algorithms. We also discuss relevant issues such as data collection, evaluation metrics and benchmarking. After analyzing these algorithms and identifying their limitations, we conclude with several promising directions for future research},
	number = {1},
	journal = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {{Ming-Hsuan} Yang and {D.J.} Kriegman and N. Ahuja},
	year = {2002},
	keywords = {{3D} position, benchmarking, computer vision, data collection, evaluation metrics, expression recognition, face color, face detection algorithms, face images, face orientation, face recognition, face shape, face size, face texture, face tracking, feature extraction, fully automated systems, image sequence, intelligent vision-based human-computer interaction, lighting conditions, machine learning, object detection, object recognition, pose estimation, reviews, statistical pattern recognition, survey, view-based recognition},
	pages = {34--58}
},

@article{viola_robust_2001,
	title = {Robust Real-time Object Detection},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.2751},
	doi = {10.1.1.23.2751},
	journal = {International Journal of Computer Vision},
	author = {Paul Viola and Michael Jones},
	year = {2001}
},

@inproceedings{bhattacharya_evaluation_2008,
	title = {An evaluation of the Tight Optical Integration {(TOI)} algorithm sensitivity to inertial and camera errors},
	doi = {10.1109/PLANS.2008.4570061},
	abstract = {The objective of this paper is to demonstrate the sensitivity of the {GPS-camera} tight optical integration {(TOI)} algorithm to errors stemming from two sources; (a) inertial measurement errors and, (b) camera measurement errors. The {TOI} algorithm has been described by the authors in a recent paper, {"An} algorithm for {GPS} tight optical integration {(TOI)".} In the {TOI} algorithm, the inertial measurements are primarily used to derive attitude information to transform the camera measurements from the body frame to the navigation frame of reference. Clearly, errors on these inertial measurements contribute directly to the angular information from the camera used for a final position solution. Also, an initial position estimate is required to transform the measurements from the navigation frame of reference to an earth reference frame. As a result, an error in the initial position estimate will introduce some error in the position solution, which is included as an equivalent inertial error in the overall error analysis. The function of the camera is to make angular measurements from a pre-defined "marker". The errors in these measurements are sensitive to the range of the marker from the camera, i.e. marker range. Although the marker range is not a measurement used in the algorithm, it is necessary to investigate the sensitivity of the algorithm to variations in the marker range. It is shown that the sensitivity of the error in the final position estimate increases as this marker range increases. Moreover, the camera needs to identify the marker and hence, an error in the pixel selection in the image or a marker position error will affect the accuracy of the final position estimates. The sensitivity of the algorithm to these errors contributed by the camera is also analyzed. The {TOI} algorithm performance is shown to be similar to {GPS} performance levels, in the presence of the error sources discussed above. This makes it a viable solution for navigation capabilities in environ- - ments where sufficient satellites for a {GPS} only solution are not available, e.g. an urban canyon.},
	author = {S. Bhattacharya and T. Arthur and M. Uijt de Haag and Z. Zhu and K. Scheff},
	year = {2008},
	keywords = {angular measurement, angular measurements, camera measurement errors, inertial measurement errors, measurement errors, navigation, position measurement, tight optical integration},
	pages = {533--540}
},

@inproceedings{sano_human_2004,
	title = {Human body shape imaging for Japanese kimono design},
	volume = {2},
	isbn = {1091-5281},
	doi = {10.1109/IMTC.2004.1351260},
	abstract = {A yukata is a type of traditional Japanese kimono. An alignment of its texture pattern is an important factor of the yukata design. The wearing condition of the yukata is affected by the wearer's body shape and the way of wearing the yukata. Accordingly, a three dimensional display of the yukata is necessary for designing the yukata. In this paper, we developed a human body shape imaging system for yukata design. Firstly, we developed an algorithm to measure the wearer's upper half of the body, which is important to display the wearing condition of the yukata. Secondly, we developed an algorithm to map the texture pattern of the kimono cloth on the wearer's body shape. The designer and the wearer can make sure of the condition of the texture alignment exactly because the yukata is displayed three dimensionally on the wearer's body shape.},
	author = {T. Sano and H. Yamamoto},
	year = {2004},
	keywords = {{CAD,} {CAD} system, clothing, human body shape imaging, image texture, Japanese kimono design, kimono cloth texture pattern mapping, shape measurement, texture pattern alignment, three-dimensional displays, yukata {3D} display, yukata wearing condition},
	pages = {1120--1123 Vol.2}
},

@article{xuelong_li_gait_2008,
	title = {Gait Components and Their Application to Gender Recognition},
	volume = {38},
	issn = {1094-6977},
	doi = {10.1109/TSMCC.2007.913886},
	abstract = {Human gait is a promising biometrics resource. In this paper, the information about gait is obtained from the motions of the different parts of the silhouette. The human silhouette is segmented into seven components, namely head, arm, trunk, thigh, front-leg, back-leg, and feet. The leg silhouettes for the front-leg and the back-leg are considered separately because, during walking, the left leg and the right leg are in front or at the back by turns. Each of the seven components and a number of combinations of the components are then studied with regard to two useful applications: human identification {(ID)} recognition and gender recognition. More than 500 different experiments on human {ID} and gender recognition are carried out under a wide range of circumstances. The effectiveness of the seven human gait components for {ID} and gender recognition is analyzed.},
	number = {2},
	journal = {Systems, Man, and Cybernetics, Part C: Applications and Reviews, {IEEE} Transactions on},
	author = {Xuelong Li and {S.J.} Maybank and Shuicheng Yan and Dacheng Tao and Dong Xu},
	year = {2008},
	keywords = {biometrics, biometrics (access control), gait analysis, gait components, gender recognition, human gait, human gait recognition, Human identification, human silhouette, image motion analysis, image recognition, image segmentation, visual surveillance},
	pages = {145--155}
},

@article{yacoob_detection_2006,
	title = {Detection and analysis of hair},
	volume = {28},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2006.139},
	abstract = {We develop computational models for measuring hair appearance for comparing different people. The models and methods developed have applications to person recognition and image indexing. An automatic hair detection algorithm is described and results reported. A multidimensional representation of hair appearance is presented and computational algorithms are described. Results on a data set of 524 subjects are reported. Identification of people using hair attributes is compared to eigenface-based recognition along with a joint, eigenface-hair-based identification.},
	number = {7},
	journal = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {Y. Yacoob and {L.S.} Davis},
	year = {2006},
	keywords = {automatic hair detection, eigenfaces, face recognition, feature extraction, hair analysis, hair appearance, hair detection., Human identification, image indexing, image representation, multidimensional representation, object detection, person recognition},
	pages = {1164--1169}
},

@inproceedings{davis_computer_2004,
	title = {Computer Vision and Pattern Recognition Workshop, 2004 Conference on},
	doi = {10.1109/CVPR.2004.80},
	abstract = {We present an adaptive three-mode {PCA} framework for recognizing gender from walking movements. Prototype female and male walkers are initially decomposed into a sub-space of their three-mode components (posture, time, gender). We then assign an importance weight to each motion trajectory in the sub-space and have the model automatically learn the weight values (key features) from labeled training data. We present experiments of recognizing physical (actual) and perceived (from perceptual experiments) gender for 40 walkers. The model demonstrates greater than 90\% recognition for both contexts and shows greater flexibility than standard {PCA.}},
	author = {{J.W.} Davis and Hui Gao},
	year = {2004},
	pages = {9}
},

@inproceedings{kanaujia_computer_2006,
	title = {Computer Vision and Pattern Recognition Workshop, 2006 Conference on},
	doi = {10.1109/CVPRW.2006.69},
	abstract = {Tracking facial features across large head rotations is a challenging research problem. Both {2D} and {3D} model based approaches have been proposed for feature analysis from multiple views. Accurate feature tracking enables useful video processing applications like emblem detection(an event or movement that symbolizes an idea), facial expressions recognition, morphing and synthesis. A crucial requirement is generalizability of the tracking framework across appearance variations, presence of facial hair and illumination changes. We propose a framework to detect emblems that combines active shape model with a predictive face aspect model to track features across large head movements and runs close to real time. Active Shape {Model(ASM)} is a deformable model for shape registration that detect facial features by combining prior shape information with the observed image data. Our view based framework represents various head poses by multiple {2D} shape models and accounts for large head rotations by dynamically switching between them. Our switching variable (the current model to use) is discriminatively predicted from the {SIFT} descriptors computed over the bounding box of low resolution face image. We demonstrate the use of tracking framework to recognize high level events like head nodding, shaking and eye blinking.},
	author = {A. Kanaujia and Yuchi Huang and D. Metaxas},
	year = {2006},
	pages = {108}
},

@inproceedings{balan_computer_2007,
	title = {Computer Vision and Pattern Recognition, 2007. {CVPR} '07. {IEEE} Conference on},
	doi = {10.1109/CVPR.2007.383340},
	abstract = {Much of the research on video-based human motion capture assumes the body shape is known a priori and is represented coarsely (e.g. using cylinders or superquadrics to model limbs). These body models stand in sharp contrast to the richly detailed {3D} body models used by the graphics community. Here we propose a method for recovering such models directly from images. Specifically, we represent the body using a recently proposed triangulated mesh model called {SCAPE} which employs a low-dimensional, but detailed, parametric model of shape and pose-dependent deformations that is learned from a database of range scans of human bodies. Previous work showed that the parameters of the {SCAPE} model could be estimated from marker-based motion capture data. Here we go further to estimate the parameters directly from image data. We define a cost function between image observations and a hypothesized mesh and formulate the problem as optimization over the body shape and pose parameters using stochastic search. Our results show that such rich generative models enable the automatic recovery of detailed human shape and pose from images.},
	author = {{A.O.} Balan and L. Sigal and {M.J.} Black and {J.E.} Davis and {H.W.} Haussecker},
	year = {2007},
	keywords = {automatic recovery, gesture recognition, human pose, human shape, hypothesized mesh, image motion analysis, image observations, image segmentation, marker-based motion capture data, {SCAPE,} stochastic search, triangulated mesh model, video-based human motion capture},
	pages = {1--8}
},

@inproceedings{arthur_demonstration_2008,
	title = {Demonstration of Tight Optical Integration {(TOI)} algorithm using field data},
	doi = {10.1109/PLANS.2008.4570065},
	abstract = {Recently a biologically inspired algorithm, called tight optical integration {(TOI),} was developed for tightly integrating optical sensor with {GPS.} The algorithm involves the integration of a standard camera along with {GPS} range (pseudorange or carrier phase) measurements to form position estimates. Initial simulations showed that {TOI} is capable of providing a position solution with an insufficient number of {GPS} satellites and a visible ldquomarkerrdquo at a known location, with an inertial unit to provide attitude information. This paper demonstrates how a marker is selected from a picture frame and tracked among consecutive frames. {TOI} has the potential to navigate with one known marker and two or three {GPS} satellites. In this work attitude information is derived from the {GPS} velocity estimates assuming zero roll for a terrestrial vehicle. Additionally, the same {TOI} algorithm can auto-locate unknown features when the position of the marker is not available, and navigate by these features when location is lost. The {TOI} algorithm is unique because it relies only on {GPS} range measurements and the pixel data from a camera. No ranging sources such as radar or {LIDAR} are required. It has particular application to scenarios involving a reduced constellation; such a reduced constellation may be due either to an urban canyon or a denied signal environment.},
	author = {T. Arthur and Z. Zhu and S. Bhattacharya and K. Johnson and K. Scheff},
	year = {2008},
	keywords = {attitude information, autolocate unknown features, biologically inspired algorithm, cameras, Global Positioning System, {GPS} satellites, optical sensor, optical sensors, tight optical integration algorithm},
	pages = {744--751}
},

@inproceedings{gallagher_clothing_2008,
	title = {Clothing cosegmentation for recognizing people},
	isbn = {1063-6919},
	doi = {10.1109/CVPR.2008.4587481},
	abstract = {Researches have verified that clothing provides information about the identity of the individual. To extract features from the clothing, the clothing region first must be localized or segmented in the image. At the same time, given multiple images of the same person wearing the same clothing, we expect to improve the effectiveness of clothing segmentation. Therefore, the identity recognition and clothing segmentation problems are inter-twined; a good solution for one aides in the solution for the other. We build on this idea by analyzing the mutual information between pixel locations near the face and the identity of the person to learn a global clothing mask. We segment the clothing region in each image using graph cuts based on a clothing model learned from one or multiple images believed to be the same person wearing the same clothing. We use facial features and clothing features to recognize individuals in other images. The results show that clothing segmentation provides a significant improvement in recognition accuracy for large image collections, and useful clothing masks are simultaneously produced. A further significant contribution is that we introduce a publicly available consumer image collection where each individual is identified. We hope this dataset allows the vision community to more easily compare results for tasks related to recognizing people in consumer image collections.},
	author = {{A.C.} Gallagher and Tsuhan Chen},
	year = {2008},
	keywords = {clothing cosegmentation, consumer image collections, face recognition, feature extraction, global clothing mask, image resolution, image segmentation, people recognition, pixel locations},
	pages = {1--8}
},

@inproceedings{barnard_body_2008,
	title = {Body part segmentation of noisy human silhouette images},
	doi = {10.1109/ICME.2008.4607653},
	abstract = {In this paper we propose a solution to the problem of body part segmentation in noisy silhouette images. In developing this solution we revisit the issue of insufficient labeled training data, by investigating how synthetically generated data can be used to train general statistical models for shape classification. In our proposed solution we produce sequences of synthetically generated images, using three dimensional rendering and motion capture information. Each image in these sequences is labeled automatically as it is generated and this labeling is based on the hand labeling of a single initial {image.We} use shape context features and Hidden Markov Models trained based on this labeled synthetic data. This model is then used to segment silhouettes into four body parts; arms, legs, body and head. Importantly, in all the experiments we conducted the same model is employed with no modification of any parameters after initial training.},
	author = {M. Barnard and M. Matilainen and J. Heikkila},
	year = {2008},
	keywords = {body part recognition, body part segmentation, hidden Markov model, hidden Markov models, image classification, image motion analysis, image segmentation, image sequence, image sequences, labeled synthetic data, motion capture information, noisy human silhouette image, rendering (computer graphics), shape classification, shape context features, silhouette segmentation, statistical model, three dimensional rendering},
	pages = {1189--1192}
},

@inproceedings{ram_people_1998,
	title = {The people sensor: a mobility aid for the visually impaired},
	shorttitle = {The people sensor},
	doi = {10.1109/ISWC.1998.729548},
	abstract = {Electronic Travel Aids, which transform visual environmental cues into another sensory modality, have been proven to help visually impaired people travel with a greater degree of psychological comfort and independence. The People Sensor is an Electronic Travel Aid designed to address two issues of importance to visually impaired people: inadvertent cane contact with other pedestrians and objects, and speaking to a person who is no longer within hearing range. The device uses pyroelectric and ultrasound sensors to locate and differentiate between animate (human) and inanimate (non-human) obstructions in the detection path. The distance between the user and the obstruction along with the nature of the obstruction (human or non-human) is transmitted via modulated vibrotactile feedback. Armed with advance knowledge of the presence and location of objects and people in the environment, users of The People Sensor can travel with increased independence, safety and confidence},
	author = {S. Ram and J. Sharf},
	year = {1998},
	keywords = {Electronic Travel Aid, handicapped aids, modulated vibrotactile feedback, People Sensor, pyroelectric detectors, visual environmental cues, visually impaired people},
	pages = {166--167}
},

@article{radke_image_2005,
	title = {Image change detection algorithms: a systematic survey},
	volume = {14},
	issn = {1057-7149},
	shorttitle = {Image change detection algorithms},
	doi = {10.1109/TIP.2004.838698},
	abstract = {Detecting regions of change in multiple images of the same scene taken at different times is of widespread interest due to a large number of applications in diverse disciplines, including remote sensing, surveillance, medical diagnosis and treatment, civil infrastructure, and underwater sensing. This paper presents a systematic survey of the common processing steps and core decision rules in modern change detection algorithms, including significance and hypothesis testing, predictive models, the shading model, and background modeling. We also discuss important preprocessing methods, approaches to enforcing the consistency of the change mask, and principles for evaluating and comparing the performance of change detection algorithms. It is hoped that our classification of algorithms into a relatively small number of categories will provide useful guidance to the algorithm designer.},
	number = {3},
	journal = {Image Processing, {IEEE} Transactions on},
	author = {{R.J.} Radke and S. Andra and O. {Al-Kofahi} and B. Roysam},
	year = {2005},
	keywords = {background modeling, change detection, change mask, hypothesis testing, illumination invariance, image change detection algorithm, image classification, mixture models, predictive model, predictive models, shading model, significance testing, systematic survey},
	pages = {294--307}
},

@inproceedings{sangho_park_motion_2002,
	title = {Motion and Video Computing, 2002. Proceedings. Workshop on},
	doi = {10.1109/MOTION.2002.1182221},
	abstract = {The paper presents a system to segment and track multiple body parts of interacting humans in the presence of mutual occlusion and shadow. The color image sequence is processed at three levels: pixel level, blob level, and object level. A Gaussian mixture model is used at the pixel level to train and classify individual pixel colors. A Markov random field {(MRF)} framework is used at the blob level to merge the pixels into coherent blobs and to register inter-blob relations. A coarse model of the human body is applied at the object level as empirical domain knowledge to resolve ambiguity due to occlusion and to recover from intermittent tracking failures. A two-fold tracking scheme is used which consists of blob to blob matching in consecutive frames and blob to body part association within a frame. The tracking scheme resembles a multi-target, multi-assignment framework. The result is a tracking system that simultaneously segments and tracks multiple body parts of interacting people. Example sequences illustrate the success of the proposed paradigm.},
	author = {Sangho Park and {J.K.} Aggarwal},
	year = {2002},
	keywords = {blob level processing, body part segmentation, body part tracking, color image sequence, Gaussian mixture model, Gaussian processes, hidden feature removal, image classification, image colour analysis, image matching, image registration, image segmentation, image sequences, learning (artificial intelligence), Markov processes, Markov random field, object detection, object level processing, occlusion, optical tracking, pixel level processing, shadowing, surveillance, video signal processing, video surveillance},
	pages = {105--111}
},

@inproceedings{_person_2002,
	title = {Person Identification Using Automatic Height and Stride Estimation},
	isbn = {{0-7695-1695-X}},
	url = {http://portal.acm.org/citation.cfm?id=846227.848606},
	abstract = {We present a parametric method to automatically identify people in monocular low-resolution video by estimating the height and stride parameters of their gait. Stride parameters (stride length and cadence) are functions of body height, weight, and gender. Previous work has demonstrated effective use of these biometrics for identification and verification of people. In this paper, we show that performance is significantly improved by using height as an additional discriminant feature. Height is estimated by segmenting the person from the background and fitting their apparent height to a time-dependent model. With a database of 45 people and 4samples of each, we show that a person is correctly identified with 49\% probability when using both height and stride parameters, compared with 21\% when using stride parameters only. Height estimates for this configuration are accurate to within s = 3.5cm. This method works with low-resolution images of people, and is robust to changes in lighting, clothing, and tracking errors.},
	publisher = {{IEEE} Computer Society},
	year = {2002},
	pages = {40377}
},

@inproceedings{kawade_micromechatronics_2002,
	title = {Micromechatronics and Human Science, 2002. {MHS} 2002. Proceedings of 2002 International Symposium on},
	doi = {10.1109/MHS.2002.1058006},
	abstract = {When people communicate with each other directly (face to face), the role of the visual information is very essential, especially facial images which include the most important information. If artificial vision systems that can understand human faces are realized, they will enable machines to communicate with people more effectively. For this purpose, we have been continuing the research and development of vision-based face understanding technologies, such as face detection, facial feature point detection, face recognition, face tracking, encoding and transmission of facial motion, face situation estimation, gender and age estimation, etc. This paper introduces these technologies and their applications.},
	author = {M. Kawade},
	year = {2002},
	keywords = {age estimation, computer vision, face detection, face recognition, face tracking, face understanding vision system, facial feature point detection, feature extraction, gender estimation, human faces, race recognition, wavelet transform, wavelet transforms},
	pages = {27--32}
},

@inproceedings{usman_saeed_multimedia_2006,
	title = {Multimedia Signal Processing, 2006 {IEEE} 8th Workshop on},
	doi = {10.1109/MMSP.2006.285262},
	abstract = {Face is considered as an attractive biometric but because of multiple sources of variabilities, the associated recognition rate is not high enough, when working on appearance only, for most of real applications. Considering that most available visual data are videos and not still images, we investigate in this article the possible contribution of some dynamic parameters (head displacements and mouth motion) in person recognition. Some preliminary results tend to validate this original proposal that opens some new perspectives in the possible design of future hybrid and efficient system combining appearance and dynamics of faces},
	author = {Usman Saeed and Federico Matta and {Jean-Luc} Dugelay},
	year = {2006},
	keywords = {biometric, biometrics (access control), dynamic parameters person recognition, face recognition, head-mouth dynamics},
	pages = {29--32}
},

@inproceedings{huchuan_lu_gender_2007,
	title = {Gender Recognition using Adaboosted Feature},
	volume = {2},
	doi = {10.1109/ICNC.2007.396},
	abstract = {In this paper, a novel approach for gender recognition combining the ellipse face images, Gabor filters, Adaboost learning and {SVM} classifier is proposed. Face representation based on Harr-like feature, Gabor feature or {ICA} is an effective method to extract facial appearance information. So we compare these three kinds of features selected by Adaboost method using {FERET} database. In the first experiment, several different preprocessing methods (face detector, warp face images and ellipse face images) have been compared, meanwhile comparing different feature extraction methods {(Gabor} wavelets, Haar-like wavelets, {PCA,} {ICA).The} experimental results show that our proposed approach (combination of ellipse face images, Gabor wavelets and {Ada+SVM} classifier) achieves better performance. The second experiment is tested on {PCA} and {ICA} feature extraction method with different explanation. It is shown that {ICA} is much steadier than {PCA} method when the explanation changed.},
	author = {Huchuan Lu and Hui Lin},
	year = {2007},
	keywords = {Adaboost learning, Adaboost method, ellipse face images, face recognition, face representation, {FERET} database, Gabor filters, gender recognition, Harr-like feature, principal component analysis, support vector machines, support vector machines classifier, visual databases},
	pages = {646--650}
},

@inproceedings{xuelong_li_gender_2007,
	title = {Gender recognition based on local body motions},
	doi = {10.1109/ICSMC.2007.4413898},
	abstract = {Human body motions, including gait information, are a promising biometrics resource. In this paper, the human silhouette is segmented into seven components for visual surveillance applications, namely, head, arm, body, thigh, front-leg, back-leg, and feet. The legs are classified as front-leg or back-leg because of the bipedal walking style: during walking, the left-leg and the right-leg are in front or at the back in turn. The motions of the individual components and of a number of combinations of components are then studied for gender recognition. For {HumanID} recognition under different cases, the performances of and underlying links amongst the seven human gait components are analyzed.},
	author = {Xuelong Li and S. Maybank and Dacheng Tao},
	year = {2007},
	keywords = {biometrics, biometrics (access control), bipedal walking, gait analysis, gait information, gender recognition, human body motion, human gait analysis, human silhouette segmentation, {HumanID} recognition, image classification, image motion analysis, image segmentation, local body motion, surveillance, visual surveillance application},
	pages = {3881--3886}
},

@inproceedings{saeed_person_2006,
	title = {Person Recognition based on Head and Mouth Dynamics},
	doi = {10.1109/MMSP.2006.285262},
	abstract = {Face is considered as an attractive biometric but because of multiple sources of variabilities, the associated recognition rate is not high enough, when working on appearance only, for most of real applications. Considering that most available visual data are videos and not still images, we investigate in this article the possible contribution of some dynamic parameters (head displacements and mouth motion) in person recognition. Some preliminary results tend to validate this original proposal that opens some new perspectives in the possible design of future hybrid and efficient system combining appearance and dynamics of faces},
	booktitle = {Multimedia Signal Processing, 2006 {IEEE} 8th Workshop on},
	author = {U. Saeed and F. Matta and {J.-L.} Dugelay},
	year = {2006},
	keywords = {biometric, biometrics (access control), dynamic parameters person recognition, face recognition, head-mouth dynamics},
	pages = {29--32}
},

@inproceedings{saeed_person_2007,
	title = {Person Recognition Form Video using Facial Mimics},
	volume = {1},
	isbn = {1520-6149},
	doi = {10.1109/ICASSP.2007.366724},
	abstract = {Video based facial recognition is an appealing modality in biometrics due to its acceptability and ease of use but the associated recognition rate is not high enough due to multiple sources of variation and lack of constraints in real world applications. In this article we investigate the possible contribution of facial mimics extracted from low quality videos for person recognition. The initial results reported tend to validate this original proposal, thus opening some new perspectives for design of future hybrid and efficient system combining facial appearance and dynamics.},
	booktitle = {Acoustics, Speech and Signal Processing, 2007. {ICASSP} 2007. {IEEE} International Conference on},
	author = {U. Saeed and J. Dugelay},
	year = {2007},
	keywords = {biometric modality, biometrics (access control), face recognition, facial mimic extraction, feature extraction, image processing, person recognition rate, video based facial recognition},
	pages = {I--493-I-496}
},

@inproceedings{wei_zhang_real-time_2008,
	title = {Real-time clothes comparison based on multi-view vision},
	doi = {10.1109/ICDSC.2008.4635727},
	abstract = {In this paper, we present a clothing recognition system that augments clothes recommendation and fashion exploration using the intelligent multi-view vision technology of the responsive mirror, an implicitly controlled human-computer interaction system for clothes fitting rooms. The responsive mirror provides shoppers with real-time ldquoselfrdquo and ldquosocialrdquo clothes comparisons. The system recommends clothing that is ldquosimilarrdquo and ldquodifferentrdquo than the clothing that the person is trying on in the mirror. The goal of the research in this paper is to create a recommendation system that uses a definition of ldquosimilarrdquo and ldquodifferentrdquo that matches human perception. We address the social nature of the recognition problem by conducting a user study to identify the salient clothes factors that people use to determine clothes similarity. We describe the computer vision and machine learning techniques employed to recognize the factors that human eyes perceive in term of clothing similarity from frontal-view outfit images. We describe the key components of the motion-tracking and clothes-recognition systems and evaluate their performance by user study and experiments on a simulated clothes fitting image dataset. The approach and results presented here will benefit designers and developers of similar applications in the future.},
	booktitle = {Distributed Smart Cameras, 2008. {ICDSC} 2008. Second {ACM/IEEE} International Conference on},
	author = {Wei Zhang and Bo Begole and M. Chu and Juan Liu and Nicholas Yee},
	year = {2008},
	keywords = {clothes recognition, clothes recommendation, clothing recognition system, computer vision, display devices, fashion, fashion exploration, human computer interaction, human-computer interaction, human-computer interaction system, image recognition, intelligent multi-view vision technology, learning (artificial intelligence), machine learning, motion tracking, Multi-view vision, real-time clothes comparison, responsive mirror, retailing},
	pages = {1--10}
},

@inproceedings{balan_detailed_2007,
	title = {Detailed Human Shape and Pose from Images},
	url = {http://dx.doi.org/10.1109/CVPR.2007.383340},
	abstract = {Much of the research on video-based human motion capture assumes the body shape is known a priori and is represented coarsely (e.g. using cylinders or superquadrics to model limbs). These body models stand in sharp contrast to the richly detailed {3D} body models used by the graphics community. Here we propose a method for recovering such models directly from images. Specifically, we represent the body using a recently proposed triangulated mesh model called {SCAPE} which employs a low-dimensional, but detailed, parametric model of shape and pose-dependent deformations that is learned from a database of range scans of human bodies. Previous work showed that the parameters of the {SCAPE} model could be estimated from marker-based motion capture data. Here we go further to estimate the parameters directly from image data. We define a cost function between image observations and a hypothesized mesh and formulate the problem as optimization over the body shape and pose parameters using stochastic search. Our results show that such rich generative models enable the automatic recovery of detailed human shape and pose from images.},
	booktitle = {Computer Vision and Pattern Recognition, 2007. {CVPR} '07. {IEEE} Conference on},
	author = {{AO} Balan and L Sigal and {MJ} Black and {JE} Davis and {HW} Haussecker},
	year = {2007},
	keywords = {human\_motion\_capture, needs\_fix, pose\_estimation, studio},
	pages = {8, 1}
},

@article{gaines_introduction_2006,
	title = {Introduction to the special {ACS} issue of population research and policy review},
	volume = {25},
	issn = {0167-5923},
	url = {http://www.springerlink.com.ezproxy1.lib.asu.edu/content/45360426jx782183/},
	doi = {10.1007/s11113-006-9004-7},
	number = {3},
	journal = {Population Research and Policy Review},
	author = {Leonard M. Gaines},
	year = {2006},
	pages = {197--199}
},

@article{griffin_american_2006,
	title = {American Community Survey overview and the role of external evaluations},
	volume = {25},
	issn = {0167-5923},
	url = {http://www.springerlink.com.ezproxy1.lib.asu.edu/content/34q72n5t92127150/},
	doi = {10.1007/s11113-006-0006-2},
	number = {3},
	journal = {Population Research and Policy Review},
	author = {Deborah H. Griffin and Preston Jay Waite},
	year = {2006},
	pages = {201--223}
},

@inproceedings{maw_studying_2004,
	address = {Chicago, {IL}},
	title = {Studying Human Face Recognition with the {Gaze-Contingent} Window Technique},
	author = {N. N. Maw and M. Pomplun},
	year = {2004},
	pages = {927--932}
},

@article{calvo_eye-movement_2009,
	title = {Eye-movement assessment of the time course in facial expression recognition: Neurophysiological implications},
	volume = {9},
	shorttitle = {Eye-movement assessment of the time course in facial expression recognition},
	url = {http://cabn.psychonomic-journals.org/content/9/4/398.abstract},
	doi = {10.3758/CABN.9.4.398},
	abstract = {Happy, surprised, disgusted, angry, sad, fearful, and neutral faces were presented extrafoveally, with fixations on faces allowed or not. The faces were preceded by a cue word that designated the face to be saccaded in a two-alternative forced-choice discrimination task {(2AFC;} Experiments 1 and 2), or were followed by a probe word for recognition {(Experiment} 3). Eye tracking was used to decompose the recognition process into stages. Relative to the other expressions, happy faces (1) were identified faster (as early as 160 msec from stimulus onset) in extrafoveal vision, as revealed by shorter saccade latencies in the {2AFC} task; (2) required less encoding effort, as indexed by shorter first fixations and dwell times; and (3) required less decision-making effort, as indicated by fewer refixations on the face after the recognition probe was presented. This reveals a happy-face identification advantage both prior to and during overt attentional processing. The results are discussed in relation to prior neurophysiological findings on latencies in facial expression recognition.},
	number = {4},
	journal = {Cognitive, Affective, \& Behavioral Neuroscience},
	author = {Manuel G. Calvo and Lauri Nummenmaa},
	month = dec,
	year = {2009},
	pages = {398--411}
},

@article{eimer_role_2003,
	title = {The role of spatial attention in the processing of facial expression: an {ERP} study of rapid brain responses to six basic emotions},
	volume = {3},
	issn = {1530-7026},
	shorttitle = {The role of spatial attention in the processing of facial expression},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/12943325},
	abstract = {To investigate the time course of emotional expression processing, we recorded {ERP} responses to stimulus arrays containing neutral versus angry, disgusted, fearful, happy, sad, or surprised faces. In one half of the experiment, the task was to discriminate emotional and neutral facial expressions. Here, an enhanced early frontocentral positivity was elicited in response to emotional as opposed to neutral faces, followed by a broadly distributed positivity and an enhanced negativity at lateral posterior sites. These emotional expression effects were very similar for all six basic emotional expressions. In the other half of the experiment, attention was directed away from the faces toward a demanding perceptual discrimination task. Under these conditions, emotional expression effects were completely eliminated, demonstrating that brain processes involved in the detection and analysis of facial expression require focal attention. The face-specific N170 component was unaffected by any emotional expression, supporting the hypothesis that structural encoding and expression analysis are independent processes.},
	number = {2},
	journal = {Cognitive, Affective \& Behavioral Neuroscience},
	author = {Martin Eimer and Amanda Holmes and Francis P {McGlone}},
	month = jun,
	year = {2003},
	note = {{PMID:} 12943325},
	keywords = {Adolescent, Adult, Attention, Discrimination {(Psychology),} Emotions, Evoked Potentials, Facial expression, Female, Humans, Male, Middle Aged, Psychomotor Performance, Reaction Time},
	pages = {97--110}
},

@article{frischen_visual_2008,
	title = {Visual search for faces with emotional expressions},
	volume = {134},
	issn = {0033-2909},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/18729567},
	doi = {10.1037/0033-2909.134.5.662},
	abstract = {The goal of this review is to critically examine contradictory findings in the study of visual search for emotionally expressive faces. Several key issues are addressed: Can emotional faces be processed preattentively and guide attention? What properties of these faces influence search efficiency? Is search moderated by the emotional state of the observer? The authors argue that the evidence is consistent with claims that (a) preattentive search processes are sensitive to and influenced by facial expressions of emotion, (b) attention guidance is influenced by a dynamic interplay of emotional and perceptual factors, and (c) visual search for emotional faces is influenced by the emotional state of the observer to some extent. The authors also argue that the way in which contextual factors interact to determine search performance needs to be explored further to draw sound conclusions about the precise influence of emotional expressions on search efficiency. Methodological considerations (e.g., set size, distractor background, task set) and ecological limitations of the visual search task are discussed. Finally, specific recommendations are made for future research directions.},
	number = {5},
	journal = {Psychological Bulletin},
	author = {Alexandra Frischen and John D Eastwood and Daniel Smilek},
	month = sep,
	year = {2008},
	note = {{PMID:} 18729567},
	keywords = {Attention, Emotions, Facial expression, Humans, Mental Processes, Photic Stimulation, Visual Perception},
	pages = {662--676}
},

@article{leppaenen_positive_2004,
	title = {Positive facial expressions are recognized faster than negative facial expressions, but why?},
	volume = {69},
	url = {http://dx.doi.org/10.1007/s00426-003-0157-2},
	doi = {10.1007/s00426-003-0157-2},
	abstract = {Three experiments examined the recognition speed advantage for happy faces. The results replicated earlier findings by showing that positive (happy) facial expressions were recognized faster than negative (disgusted or sad) facial expressions {(Experiments} 1 and 2). In addition, the results showed that this effect was evident even when low-level physical differences between positive and negative faces were controlled by using schematic faces {(Experiment} 2), and that the effect was not attributable to an artifact arising from facilitated recognition of a single feature in the happy faces (up-turned mouth line, Experiment 3). Together, these results suggest that the happy face advantage may reflect a higher-level asymmetry in the recognition and categorization of emotionally positive and negative signals.},
	number = {1},
	journal = {Psychological Research},
	author = {Jukka M. Leppänen and Jari K. Hietanen},
	month = dec,
	year = {2004},
	pages = {22--29}
},

@article{reynolds_monitoring_2009,
	title = {Monitoring eye movements while searching for affective faces},
	volume = {17},
	url = {http://www.informaworld.com/10.1080/13506280701623704},
	number = {3},
	journal = {Visual Cognition},
	author = {Michael G Reynolds and John D Eastwood and Marita Partanen and Alexandra Frischen and Daniel Smilek},
	year = {2009},
	pages = {318 -- 333}
},

@article{lederman_haptic_2008,
	title = {Haptic Processing of Facial Expressions of Emotion in {2D} {Raised-Line} Drawings},
	volume = {1},
	url = {http://portal.acm.org/citation.cfm?id=1449676},
	abstract = {Participants haptically (vs. visually) classified universal facial expressions of emotion {(FEEs)} depicted in simple {2D} raised-line displays. Experiments 1 and 2 established that haptic classification was well above chance; face-inversion effects further indicated that the upright orientation was privileged. Experiment 2 added a third condition in which the normal configuration of the upright features was spatially scrambled. Results confirmed that configural processing played a critical role, since upright {FEEs} were classified more accurately and confidently than either scrambled or inverted {FEEs,} which did not differ. Because accuracy in both scrambled and inverted conditions was above chance, feature processing also played a role, as confirmed by commonalities across confusions for upright, inverted, and scrambled faces. Experiment 3 required participants to visually and haptically assign emotional valence (positive/negative) and magnitude to upright and inverted {2-D} {FEE} displays. While emotional magnitude could be assigned using either modality, haptic presentation led to more variable valence judgments. We also documented a new face-inversion effect for emotional valence visually, but not haptically. These results suggest emotions can be interpreted from {2-D} displays presented haptically as well as visually; however, emotional impact is judged more reliably by vision than by touch. Potential applications of this work are also considered.},
	number = {1},
	journal = {{EEE} Trans. Haptics},
	author = {Susan J. Lederman and Roberta L. Klatzky and E. {Rennert-May} and J. H. Lee and K. Ng and Cheryl Hamilton},
	year = {2008},
	keywords = {cognition, education, perception and psychophysics, social communication},
	pages = {27--38}
},

@article{ojala_multiresolution_2002,
	title = {Multiresolution {Gray-Scale} and Rotation Invariant Texture Classification with Local Binary Patterns},
	volume = {24},
	url = {http://portal.acm.org/citation.cfm?id=628808},
	abstract = {This paper presents a theoretically very simple, yet efficient, multiresolution approach to gray-scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The method is based on recognizing that certain local binary patterns, termed "uniform" are fundamental properties of local image texture and their occurrence histogram is proven to be a very powerful texture feature. We derive a generalized gray-scale and rotation invariant operator presentation that allows for detecting the "uniform" patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis. The proposed approach is very robust in terms of gray-scale variations since the operator is, by definition, invariant against any monotonic transformation of the gray scale. Another advantage is computational simplicity as the operator can be realized with a few operations in a small neighborhood and a lookup table. Excellent experimental results obtained in true problems of rotation invariance, where the classifier is trained at one particular rotation angle and tested with samples from other rotation angles, demonstrate that good discrimination can be achieved with the occurrence statistics of simple rotation invariant local binary patterns. These operators characterize the spatial configuration of local image texture and the performance can be further improved by combining them with rotation invariant variance measures that characterize the contrast of local image texture. The joint distributions of these orthogonal measures are shown to be very powerful tools for rotation invariant texture analysis.},
	number = {7},
	journal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Timo Ojala and Matti Pietikäinen and Topi Mäenpää},
	year = {2002},
	keywords = {brodatz, contrast., distribution, histogram, nonparametric, outex, texture analysis},
	pages = {971--987}
},

@inproceedings{hao_tang_3d_2008,
	title = {{3D} facial expression recognition based on properties of line segments connecting facial feature points},
	doi = {10.1109/AFGR.2008.4813304},
	abstract = {The {3D} facial geometry contains ample information about human facial expressions. Such information is invariant to pose and lighting conditions, which have imposed serious hurdles on many {2D} facial analysis problems. In this paper, we perform person and gender independent facial expression recognition based on properties of the line segments connecting certain {3D} facial feature points. The normalized distances and slopes of these line segments comprise a set of 96 distinguishing features for recognizing six universal facial expressions, namely anger, disgust, fear, happiness, sadness, and surprise. Using a multi-class support vector machine {(SVM)} classifier, an 87.1\% average recognition rate is achieved on the publicly available {3D} facial expression database {BU-3DFE.} The highest average recognition rate obtained in our experiments is 99.2\% for the recognition of surprise. Our result outperforms the result reported in the prior work, which uses elaborately extracted primitive facial surface features and an {LDA} classifier and which yields an average recognition rate of 83.6\% on the same database.},
	booktitle = {Automatic Face \& Gesture Recognition, 2008. {FG} '08. 8th {IEEE} International Conference on},
	author = {Hao Tang and {T.S.} Huang},
	year = {2008},
	keywords = {{3D} facial expression recognition, {3D} facial geometry, computational geometry, emotion recognition, face recognition, facial feature point, image classification, line segment property, multiclass support vector machine, support vector machines, {SVM} classifier},
	pages = {1--6}
},

@inproceedings{barralon_development_2007,
	address = {Singapore},
	title = {Development and evaluation of multidimensional tactons for a wearable tactile display},
	isbn = {978-1-59593-862-6},
	url = {http://portal.acm.org/citation.cfm?id=1377999.1378005},
	doi = {10.1145/1377999.1378005},
	abstract = {We developed a novel wearable tactile display system as an alternative to the visual and audio displays routinely used by anesthesiologists to monitor patients in the operating room {(OR).} Visual displays and auditory alarms can be distracting or insufficient in their alarm transmission whereas a tactile display, which utilizes the sense of touch, can act as an effective conduit for alert delivery. A sophisticated alarm scheme is essential to convey the complex array of physiological information available in current monitoring systems; therefore, to report all relevant alerts to the attending anesthesiologist, it is essential that an augmenting or replacement display system be at least as effective and efficacious as conventional systems. Using multidimensional Tactons, we designed a tactile alert scheme consisting of 36 unique stimuli and evaluated the accuracy and response time in stimuli recognition using a tactile prototype worn as a belt. We observed an overall accuracy of 81\% and a response time of 4.8 seconds. 4.18 bits (18.07 tokens) of messages were successfully communicated without loss of information. These results demonstrate that the novel tactile display represents an effective and potentially work-load-reducing method to convey vital information non-visually and non-aurally.},
	booktitle = {Proceedings of the 9th international conference on Human computer interaction with mobile devices and services},
	publisher = {{ACM}},
	author = {Pierre Barralon and Ginna Ng and Guy Dumont and Stephan K. W. Schwarz and Mark Ansermino},
	year = {2007},
	keywords = {Abdomen, Tactile Display, tactile icons, tactons},
	pages = {186--189}
},

@inproceedings{brown_first_2005,
	title = {A first investigation into the effectiveness of Tactons},
	doi = {10.1109/WHC.2005.6},
	abstract = {This paper reports two experiments relating to the design of Tactons (or tactile icons). The first experiment investigated perception of vibro-tactile "roughness" (created using amplitude modulated sinusoids), and the results indicated that roughness could be used as a parameter for constructing Tactons. The second experiment is the first full evaluation of Tactons, and uses three values of roughness identified in the first experiment, along with three rhythms to create a set of Tactons. The results of this experiment showed that Tactons could be a successful means of communicating information in user interfaces, with an overall recognition rate of 71\%, and recognition rates of 93\% for rhythm and 80\% for roughness.},
	booktitle = {Eurohaptics Conference, 2005 and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems, 2005. World Haptics 2005. First Joint},
	author = {{L.M.} Brown and {S.A.} Brewster and {H.C.} Purchase},
	year = {2005},
	keywords = {amplitude modulated sinusoids, haptic interfaces, human computer interaction, human-computer interaction, tactile icons, tactons, user interfaces, vibro-tactile devices},
	pages = {167--176}
},

@book{hall_hidden_1990,
	title = {The Hidden Dimension},
	isbn = {0385084765},
	publisher = {Anchor},
	author = {Edward T. Hall},
	month = oct,
	year = {1990}
},

@article{ram_people_1998,
	title = {The People Sensor: A Mobility Aid for the Visually Impaired},
	volume = {00},
	shorttitle = {The People Sensor},
	url = {http://dx.doi.org/10.1109/ISWC.1998.729548},
	abstract = {Electronic Travel Aids, which transform visual environmental cues into another sensory modality, have been proven to help visually impaired people travel with a greater degree of psychological comfort and independence. The People Sensor is an Electronic Travel Aid designed to address two issues of importance to visually impaired people: inadvertent cane contact with other pedestrians and objects, and speaking to a person who is no longer within hearing range. The device uses pyroelectric and ultrasound sensors to locate and differentiate between animate (human) and inanimate (non-human) obstructions in the detection path. The distance between the user and the obstruction, along with the nature of the obstruction (human or non-human) is transmitted via modulated vibrotactile feedback. Armed with advance knowledge of the presence and location of objects and people in the environment, users of The People Sensor can travel with increased independence, safety and confidence.},
	journal = {iswc},
	author = {Sunita Ram and Jennie Sharf},
	year = {1998},
	keywords = {audio, wearable}
},

@inproceedings{brewster_tactons:_2004,
	title = {Tactons: structured tactile messages for non-visual information display},
	shorttitle = {Tactons},
	url = {http://portal.acm.org/citation.cfm?id=976310.976313},
	booktitle = {{AUIC} '04: Proceedings of the fifth conference on Australasian user interface},
	publisher = {Australian Computer Society, Inc.},
	author = {Stephen Brewster and Lorna Brown},
	year = {2004},
	keywords = {tactile},
	pages = {23, 15}
},

@article{hjelmas_face_2001,
	title = {Face Detection: A Survey},
	volume = {83},
	issn = {1077-3142},
	shorttitle = {Face Detection},
	url = {http://www.sciencedirect.com.ezproxy1.lib.asu.edu/science/article/B6WCX-458P9XF-3/2/25c703bcdc7e96439e46210c9c9fffc2},
	doi = {10.1006/cviu.2001.0921},
	abstract = {In this paper we present a comprehensive and critical survey of face detection algorithms. Face detection is a necessary first-step in face recognition systems, with the purpose of localizing and extracting the face region from the background. It also has several applications in areas such as content-based image retrieval, video coding, video conferencing, crowd surveillance, and intelligent human-computer interfaces. However, it was not until recently that the face detection problem received considerable attention among researchers. The human face is a dynamic object and has a high degree of variability in its apperance, which makes face detection a difficult problem in computer vision. A wide variety of techniques have been proposed, ranging from simple edge-based algorithms to composite high-level approaches utilizing advanced pattern recognition methods. The algorithms presented in this paper are classified as either feature-based or image-based and are discussed in terms of their technical approach and performance. Due to the lack of standardized tests, we do not provide a comprehensive comparative evaluation, but in cases where results are reported on common datasets, comparisons are presented. We also give a presentation of some proposed applications and possible application areas.},
	number = {3},
	journal = {Computer Vision and Image Understanding},
	author = {Erik Hjelmås and Boon Kee Low},
	month = sep,
	year = {2001},
	pages = {236--274}
},

@article{yang_detecting_2002,
	title = {Detecting Faces in Images: A Survey},
	volume = {24},
	issn = {0162-8828},
	shorttitle = {Detecting Faces in Images},
	doi = {http://doi.ieeecomputersociety.org/10.1109/34.982883},
	abstract = {Images containing faces are essential to intelligent vision-based human computer interaction, and research efforts in face processing include face recognition, face tracking, pose estimation, and expression recognition. However, many reported methods assume that the faces in an image or an image sequence have been identified and localized. To build fully automated systems that analyze the information contained in face images, robust and efficient face detection algorithms are required. Given a single image, the goal of face detection is to identify all image regions which contain a face regardless of its three-dimensional position, orientation, and lighting conditions. Such a problem is challenging because faces are nonrigid and have a high degree of variability in size, shape, color, and texture. Numerous techniques have been developed to detect faces in a single image, and the purpose of this paper is to categorize and evaluate these algorithms. We also discuss relevant issues such as data collection, evaluation metrics, and benchmarking. After analyzing these algorithms and identifying their limitations, we conclude with several promising directions for future research.},
	number = {1},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {{Ming-Hsuan} Yang and David J. Kriegman and Narendra Ahuja},
	year = {2002},
	keywords = {face detection, face recognition, machine learning., object recognition, statistical pattern recognition, view-based recognition},
	pages = {34--58},
	annote = {Complete {PDF} document was either not available or accessible. Please make sure you're logged in to the digital library to retrieve the complete {PDF} document.}
},

@techreport{sentz_combination_2002,
	title = {Combination of evidence in dempster-shafer theory},
	institution = {Sandia National Laboratories},
	author = {K. Sentz and S. Ferson},
	year = {2002}
},

@article{phillips_feret_2000,
	title = {The {FERET} evaluation methodology for face-recognition algorithms},
	volume = {22},
	issn = {0162-8828},
	doi = {10.1109/34.879790},
	abstract = {Two of the most critical requirements in support of producing reliable face-recognition systems are a large database of facial images and a testing procedure to evaluate systems. The Face Recognition Technology {(FERET)} program has addressed both issues through the {FERET} database of facial images and the establishment of the {FERET} tests. To date, 14,126 images from 1,199 individuals are included in the {FERET} database, which is divided into development and sequestered portions of the database. In September 1996, the {FERET} program administered the third in a series of {FERET} face-recognition tests. The primary objectives of the third test were to 1) assess the state of the art, 2) identify future areas of research, and 3) measure algorithm performance.},
	number = {10},
	journal = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {{P.J.} Phillips and Hyeonjoon Moon and {S.A.} Rizvi and {P.J.} Rauss},
	year = {2000},
	keywords = {algorithm performance, computer vision, face recognition, Face Recognition Technology program, face-recognition algorithms, {FERET} evaluation methodology, large database},
	pages = {1090--1104}
},

@misc{bilmes_gentle_1997,
	title = {A gentle tutorial on the {EM} algorithm and its application to parameter estimation for gaussian mixture and hidden markov models},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.613},
	abstract = {We describe the maximum-likelihood parameter estimation problem and how the Expectation-form of the {EM} algorithm as it is often given in the literature. We then develop the {EM} parameter estimation procedure for two applications: 1) finding the parameters of a mixture of Gaussian densities, and 2) finding the parameters of a hidden Markov model {(HMM)} (i.e., the {Baum-Welch} algorithm) for both discrete and Gaussian mixture observation models. We derive the update equations in fairly explicit detail but we do not prove any convergence properties. We try to emphasize intuition rather than mathematical rigor. ii 1 Maximum-likelihood Recall the definition of the maximum-likelihood estimation problem. We have a density function p(xj) that is governed by the set of parameters (e.g., p might be a set of Gaussians and could be the means and covariances). We also have a data set of size N, supposedly drawn from this distribution, i.e., X = fx1;:::;x Ng. That is, we assume that these data vectors are independent and},
	author = {Jeff Bilmes},
	year = {1997},
	keywords = {em, learning},
	howpublished = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.613}
},

@article{perez_markov_1998,
	title = {Markov Random Fields and images},
	volume = {11},
	journal = {{CWI} Quaterly},
	author = {P. Perez},
	year = {1998},
	pages = {413--437}
},

@article{vezjak_anthropological_????,
	title = {An anthropological model for automatic recognition of the male human face},
	url = {http://www.ingentaconnect.com/content/tandf/tahb/1994/00000021/00000004/art00006},
	abstract = {The human face is a characteristic pattern most familiar to us when distinguishing people. Although recognizing human faces is one of our everyday activities, we are mostly not aware how the mechanisms of recognition actually work. Attempts to recognize the human face by machine are rarer (less frequent) than those of the recognition of some other phenomena in everyday life. This paper describes the automated analysis of a human face from the grey level picture, defined as an individual description of a face, given founded on the anthropological model of a human face that incorporates 19 facial parameters of a male face in norma facialis . On the basis of these parameters it is possible to analyse, recognize and identify the human face. The contour image is used as an input to the pattern analysis program. Some algorithms for the search of characteristic face areas are presented. The Hough transform for an ellipse is used to determine the position of the head in a grey image, and to define the eye region within a face. The integral projections of the contour picture are proved to be useful techniques for picture processing to define the nose and the mouth regions within a face. Furthermore, a few algorithms for the precise determination of the facial parameters based on sharp edge transitions are developed. The method is illustrated using photographs of human faces from our data base and the results obtained are also given. The system is realized on a {PC/AT} computer.},
	author = {M Vezjak},
	keywords = {anthropological, model},
	pages = {380, 363}
},

@inproceedings{paget_texture_1997,
	title = {Texture classification using nonparametric markov random fields},
	volume = {1},
	author = {R. Paget and I. D. Longstaff and B. Lovell},
	year = {1997},
	pages = {67--70}
},

@inproceedings{erp_vibrotactile_2005,
	address = {Los Alamitos, {CA,} {USA}},
	title = {Vibrotactile Spatial Acuity on the Torso: Effects of Location and Timing Parameters},
	volume = {0},
	isbn = {0-7695-2310-2},
	shorttitle = {Vibrotactile Spatial Acuity on the Torso},
	doi = {http://doi.ieeecomputersociety.org/10.1109/WHC.2005.144},
	abstract = {The processing of spatio-temporal vibrotactile patterns by the torso was examined in two experiments. The first investigated the spatial acuity as function of location on the torso. A uniform acuity between 2 and 3 cm was found, except on the body midline where acuity was about 1 cm. This favorable effect was hypothesized to be caused by specific processing characteristics of the central nervous system. The second experiment focussed on the effect of timing parameters in a localization task. The results indicated that localization performance was closely related to both temporal ordering and apparent motion. The results can be used to optimize torso display design and the patterns presented on those displays. Also, they demonstrate that even the most advanced displays have not reached the borders of the processing capacities of the torso.},
	booktitle = {World Haptics Conference},
	publisher = {{IEEE} Computer Society},
	author = {Jan {B.F.} van Erp},
	year = {2005},
	pages = {80--85},
	annote = {Complete {PDF} document was either not available or accessible. Please make sure you're logged in to the digital library to retrieve the complete {PDF} document.}
},

@article{geldard_adventures_1957,
	title = {Adventures in tactile literacy.},
	volume = {12},
	issn = {{0003-066X}},
	abstract = {{"The} human integument has been the object of precious little research effort on the part of psychologists. The reasons are not easy to ascertain… ." The integument is discussed as a receiving surface for several different forms of energy—mechanical, thermal, electrical, and chemical. The discriminative capacities of the skin are considered with regard to the transmission of messages cutaneously, particularly mechanically. 3 primary and independent dimensions of vibratory cutaneous stimulation are indicated: amplitude, duration, and locus. The value of a vibratory language is discussed. A summary of a film {"Vibratory} Communication Experiments, University of Virginia, 1956" is presented. {(PsycINFO} Database Record (c) 2009 {APA,} all rights reserved)},
	number = {3},
	journal = {American Psychologist. Vol. 12(3)},
	author = {Frank A Geldard},
	month = mar,
	year = {1957},
	keywords = {language, literacy, psychologists},
	pages = {115--124}
},

@inproceedings{krishna_using_2008,
	title = {Using a Haptic Belt to convey {Non-Verbal} communication cues during Social Interactions to Individuals who are Blind.},
	booktitle = {Workshop on Computer Vision Applications for the Visually Impaired {(CVAVI} 08), A Satellite Workshop of {ECCV} 2008},
	author = {Sreekar Krishna and Vineeth Balasubramanian and Dirk Colbry and Sethuraman Panchanathan and Troy {McDaniel}},
	year = {2008}
},

@article{segrin_poor_2000,
	title = {Poor Social Skills Are a Vulnerability Factor in the Development of Psychosocial Problems.},
	volume = {26},
	issn = {{ISSN-0360-3989}},
	number = {3},
	journal = {Human Communication Research},
	author = {Chris Segrin and Jeanne Flora},
	year = {2000},
	pages = {489--514},
	annote = {Presumes that poor social skills are thought to make people vulnerable to psychosocial problems pursuant to the experience of stressful life events. Indicates that lower social skills scores of a group of college freshmen were predictive of a worsening of psychosocial problems. Finds that social skills interacted with stressful life events to predict changes in depression and loneliness. {(SC)}}
},

@article{russell_ucla_1996,
	title = {{UCLA} Loneliness Scale {(Version} 3): reliability, validity, and factor structure},
	volume = {66},
	issn = {0022-3891},
	shorttitle = {{UCLA} Loneliness Scale {(Version} 3)},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/8576833},
	abstract = {In this article I evaluated the psychometric properties of the {UCLA} Loneliness Scale {(Version} 3). Using data from prior studies of college students, nurses, teachers, and the elderly, analyses of the reliability, validity, and factor structure of this new version of the {UCLA} Loneliness Scale were conducted. Results indicated that the measure was highly reliable, both in terms of internal consistency (coefficient alpha ranging from .89 to .94) and test-retest reliability over a 1-year period (r = .73). Convergent validity for the scale was indicated by significant correlations with other measures of loneliness. Construct validity was supported by significant relations with measures of the adequacy of the individual's interpersonal relationships, and by correlations between loneliness and measures of health and well-being. Confirmatory factor analyses indicated that a model incorporating a global bipolar loneliness factor along with two method factor reflecting direction of item wording provided a very good fit to the data across samples. Implications of these results for future measurement research on loneliness are discussed.},
	number = {1},
	journal = {Journal of Personality Assessment},
	author = {D W Russell},
	month = feb,
	year = {1996},
	note = {{PMID:} 8576833},
	keywords = {Adult, Aged, Attitude to Health, Factor Analysis, Statistical, Female, Humans, Interpersonal Relations, Loneliness, Longitudinal Studies, Male, Middle Aged, Personality Inventory, Psychometrics, Reference Values, Reproducibility of Results},
	pages = {20--40}
},

@article{beck_inventory_1961,
	title = {An Inventory for Measuring Depression},
	volume = {4},
	issn = {{0003-990X}},
	url = {http://view.ncbi.nlm.nih.gov/pubmed/13688369},
	journal = {Archives of General Psychiatry},
	author = {{AT} Beck and {CH} Ward and M Mendelson and J Mock and J Erbaugh},
	month = jun,
	year = {1961},
	keywords = {depression, measurement, psychiatry, Psychometrics, scales, symptoms},
	pages = {571, 561}
},

@book{riggio_social_1989,
	address = {Palo Alto, {CA}},
	title = {Social Skills Inventory},
	publisher = {Consulting Psychologists Press},
	author = {R. E. Riggio},
	year = {1989}
},

@article{riggio_assessment_1986,
	title = {Assessment of basic social skills},
	volume = {51},
	number = {3},
	journal = {Journal of Personality and Social Psychology},
	author = {R. E. Riggio},
	year = {1986},
	pages = {649--660}
},

@incollection{riggio_social_1991,
	address = {London},
	title = {Social skills and interpersonal competence: Influences on social support and social seeking},
	booktitle = {Advances in Personal Relationships},
	publisher = {Jessica Kingsley},
	author = {R. E. Riggio and J. Zimmermann},
	editor = {W. H. Jones and D. Perlman},
	year = {1991},
	pages = {133--155}
},

@article{magnusson_analysis_1991,
	title = {An Analysis of Situational Dimensions},
	volume = {32},
	journal = {Perceptual and Motor Skills},
	author = {D. Magnusson},
	year = {1991},
	pages = {851--867}
},

@book{mcginnis_skillstreaming_1997,
	edition = {Revised},
	title = {Skillstreaming the Elementary School Child: New Strategies and Perspectives for Teaching Prosocial Skills},
	isbn = {{087822372X}},
	shorttitle = {Skillstreaming the Elementary School Child},
	publisher = {Research Press},
	author = {Ellen {McGinnis} and Arnold P. Goldstein},
	month = aug,
	year = {1997}
},

@article{estevis_cognitive_1994,
	title = {A cognitive approach to reducing stereotypic body rocking.},
	volume = {26},
	issn = {08991510},
	url = {http://login.ezproxy1.lib.asu.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=9412141474&site=ehost-live},
	doi = {Article},
	abstract = {Reports on the treatment of stereotypic behaviors of individuals with visual impairments. Head rocking behavior in a visually impaired child; Body rocking; Intervention methods; Development of a self-monitoring script.},
	number = {3},
	journal = {{Re:View}},
	author = {Anne Hailey Estevis and Alan J. Koenig},
	year = {1994},
	keywords = {{VISION} disorders},
	pages = {119}
},

@article{simpson_modification_1982,
	title = {Modification of Manneristic Behavior in a Blind Child via a {Time-Out} Procedure},
	volume = {14},
	number = {2},
	journal = {Education of the Visually Handicapped},
	author = {Richard L. Simpson and And Others},
	year = {1982},
	keywords = {Mannerisms},
	pages = {50--55}
},

@article{raver_increasing_1988,
	title = {Increasing social skills training for visually impaired children},
	volume = {19},
	journal = {Education of the Visually Handicapped},
	author = {Sharon Raver and P. W. Darsh},
	year = {1988},
	pages = {147--155}
},

@article{ohlsen_control_1978,
	title = {Control of body rocking in the blind through the use of vigorous exercise},
	volume = {5},
	journal = {Journal of Instructional Psychology},
	author = {R. L. Ohlsen},
	year = {1978},
	pages = {19--22}
},

@article{mcadam_self-monitoring_1993,
	title = {Self-monitoring and verbal feedback to reduce stereotypic body rocking in a congenitally blind adult},
	volume = {24},
	issn = {08991510},
	url = {http://login.ezproxy1.lib.asu.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=9610170793&site=ehost-live},
	doi = {Article},
	abstract = {Presents the results of a study on the reduction of stereotypic body rocking among blind adults. Treatment of stereotypy through self-management; Examining efficacy of self-monitoring; Experimenting in a natural setting.},
	number = {4},
	journal = {{Re:View}},
	author = {David B. {McAdam} and Conall M. {O'Cleirigh}},
	year = {1993},
	keywords = {{VISION} disorders in old age},
	pages = {163}
},

@article{blasch_blindisms:_1972,
	title = {Blindisms: Treatment by Punishment and Reward in Laboratory and Natural Settings},
	shorttitle = {Blindisms},
	journal = {Journal of Visual Impairment \& Blindness},
	author = {Bruce B. Blasch},
	year = {1972},
	keywords = {Stereotypic behavior},
	pages = {215--230}
},

@incollection{jankovic_stereotypies_1994,
	address = {London},
	title = {Stereotypies},
	volume = {3},
	booktitle = {Moverment Disorders},
	publisher = {{Butterworth-Heinemann}},
	author = {J. Jankovic},
	editor = {C. D. Marsden and S. Fahn},
	year = {1994},
	pages = {503--517}
},

@article{thompson_stereotyped_1995,
	title = {Stereotyped movement disorder in an adult following acquired brain injury: effect of environmental stimulation},
	volume = {10},
	issn = {10720847},
	shorttitle = {{STEREOTYPED} {MOVEMENT} {DISORDER} {IN} {AN} {ADULT} {FOLLOWING} {ACQUIRED} {BRAIN} {INJURY}},
	url = {http://login.ezproxy1.lib.asu.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=12196818&site=ehost-live},
	doi = {Article},
	abstract = {In the present study, a naturalistic functional analysis procedure was used to assess the effect of environmental stimulation on the stereotypic behavior (body rocking) of an adult whose stereotypy appeared to be induced by an acquired brain injury. Environmental stimulation, operationalized in terms of both physical arid social characteristics, resulted in only minimal changes in the occurrence of body rocking. Body rocking occurred in excess of 50\% of the observations in all environmental stimulation conditions and in a naturally occurring baseline condition. To our knowledge, this is the first published report of a functional analysis of stereotypy associated with acquired brain injury. {[ABSTRACT} {FROM} {AUTHOR]}},
	number = {2},
	journal = {Behavioral Interventions},
	author = {Thomas J. Thompson and Sharon M. Pearcey and James W. Bodfish and Timothy W. Crawford and Mark H. Lewis},
	month = apr,
	year = {1995},
	keywords = {{ATTITUDE} {(Psychology),} behavior, {BRAIN} -- Wounds \& injuries, {BRAIN} damage, {FUNCTIONAL} analysis, {MOTOR} ability, {PSYCHOTHERAPY,} {RESTRICTED} environmental stimulation, {SOCIAL} psychology, {STEREOTYPES} {(Social} psychology)},
	pages = {79--85}
},

@article{shabani_reducing_2001,
	title = {Reducing stereotypic behavior through discrimination training, differential reinforcement of other behavior, and self-monitoring.},
	volume = {16},
	issn = {10720847},
	url = {http://login.ezproxy1.lib.asu.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=11818622&site=ehost-live},
	doi = {10.1002/bin.096},
	abstract = {The effect of a treatment package designed to reduce stereotypic body rocking was examined in a child diagnosed with autism. After baseline, the participant was taught to discriminate between inappropriate (e.g. sitting in a chair and rocking) and appropriate (e.g. sitting in a chair without rocking) behavior. During intervention, both a therapist and the participant himself monitored the occurrence of rocking behavior. A non-resetting 5 min differential reinforcement of other behavior {(DRO)} schedule was also introduced. A multiple baseline across behaviors (sitting and standing) design was used to evaluate the effects of the intervention package. The results indicated that the intervention was effective in eliminating body rocking. In addition, the {DRO} schedule was successfully increased to 20 min for sitting and 17 min for standing and the treatment was successfully introduced at the child's school. Copyright © 2001 John Wiley \& Sons, Ltd. {[ABSTRACT} {FROM} {AUTHOR]}},
	number = {4},
	journal = {Behavioral Interventions},
	author = {Daniel B. Shabani and David A. Wilder and William A. Flood},
	month = oct,
	year = {2001},
	keywords = {{AUTISM,} {AUTISM} in children, {CHILD} psychology, {OPERANT} behavior},
	pages = {279--286}
},

@article{mchugh_impact_2003,
	title = {The Impact of Developmental Factors on Stereotypic Rocking of Children with Visual Impairments.},
	volume = {97},
	issn = {{0145482X}},
	url = {http://login.ezproxy1.lib.asu.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=10536062&site=ehost-live},
	doi = {Article},
	abstract = {Abstract: Of 52 children who attended a sports camp for children with visual impairments, 15 demonstrated stereotypic rocking currently or in the past. Three factors were associated with rocking: etiology of visual impairment, visual status, and early medical history. Children who were the most likely to exhibit rocking were those with retinopathy of prematurity who underwent lengthy hospital stays and multiple surgeries early in their lives and who were totally blind from birth. {[ABSTRACT} {FROM} {AUTHOR]}},
	number = {8},
	journal = {Journal of Visual Impairment \& Blindness},
	author = {Elaine {McHugh} and Lauren Lieberman},
	year = {2003},
	keywords = {{ATTITUDE} {(Psychology),} {BLIND} children, {CHILDREN} with disabilities, {STEREOTYPES} {(Social} psychology), {VISION} disorders in children},
	pages = {453}
},

@article{mchugh_development_1999,
	title = {The Development of Rocking Among Children Who Are Blind.},
	volume = {93},
	issn = {{0145482X}},
	url = {http://login.ezproxy1.lib.asu.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=1581540&site=ehost-live},
	doi = {Article},
	abstract = {Provides a qualitative study of rocking in children who are blind. Proposed physiological explanations for rocking; Theoretical perspectives on rocking; View of behaviorists on rocking.},
	number = {2},
	journal = {Journal of Visual Impairment \& Blindness},
	author = {Elaine {McHugh} and Jean Pyfer},
	month = feb,
	year = {1999},
	keywords = {{BLIND} children, {BLINDNESS}},
	pages = {82}
},

@article{haag_repetitive_1985,
	title = {Repetitive verbal behavior in severe aphasia},
	volume = {56},
	issn = {0028-2804},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/2415840},
	number = {10},
	journal = {Der Nervenarzt},
	author = {E Haag and W Huber and R Hündgen and U Stiller and K Willmes},
	month = oct,
	year = {1985},
	note = {{PMID:} 2415840},
	keywords = {Adult, Aged, Anomia, Aphasia, Aphasia, Broca, Aphasia, Wernicke, Echolalia, Female, {Follow-Up} Studies, Humans, Male, Middle Aged, Neuropsychological Tests, Semantics, Tomography, {X-Ray} Computed, Verbal Behavior},
	pages = {543--52}
},

@article{reivich_behavior_1972,
	title = {Behavior Problems of Deaf Children and Adolescents: A {Factor-Analytic} Study},
	volume = {15},
	shorttitle = {Behavior Problems of Deaf Children and Adolescents},
	url = {http://jslhr.asha.org/cgi/content/abstract/15/1/93},
	abstract = {Using the Behavior Problem Checklist, teachers rated 327 students (ages 6-20) in a state school for the deaf. Traits rated as present in at least 10\% of the students were intercorrelated, then a principal-component factor analysis and an orthogonal rotation of the factor matrix were accomplished by electronic computer. Five factors were rotated in accordance with Kaiser's varimax criteria. The first three factors extracted--which accounted for approximately 70\% of the common factor variance and the preponderance of disturbed behavior--were strikingly similar to the conduct, personality, and immaturity dimensions consistently identified in previously studied normal and disturbed populations. Two other factors, labeled isolation and communication problem, were also extracted. These may represent a more or less deafness-specific cluster of behavior problems.},
	number = {1},
	journal = {J Speech Hear Res},
	author = {Ronald S. Reivich and Irvin A. Rothrock},
	month = mar,
	year = {1972},
	pages = {93--104}
},

@article{newell_variability_1999,
	title = {Variability of stereotypic body-rocking in adults with mental retardation},
	volume = {104},
	number = {3 {(May} 1999)},
	journal = {American Journal on Mental Retardation},
	author = {Karl M. Newell and Thomas Incledon and James W. Bodfish},
	month = may,
	year = {1999},
	pages = {279 -- 88}
},

@article{eichel_taxonomy_1979,
	title = {A Taxonomy for Mannerisms of Blind Children.},
	volume = {73},
	number = {5},
	journal = {Journal of Visual Impairment and Blindness},
	author = {Valerie J. Eichel},
	month = may,
	year = {1979},
	keywords = {Blindisms},
	pages = {167--78}
},

@article{segrin_poor_2000,
	title = {Poor Social Skills Are a Vulnerability Factor in the Development of Psychosocial Problems},
	volume = {26},
	issn = {{ISSN-0360-3989}},
	number = {3},
	journal = {Human Communication Research},
	author = {Chris Segrin and Jeanne Flora},
	year = {2000},
	pages = {489--514}
},

@article{jindal-snape_generalization_2004,
	title = {Generalization and Maintenance of Social Skills of Children with Visual Impairments: Self-evaluation and the Role of Feedback},
	volume = {98},
	journal = {Journal of Visual Impairment \& Blindness},
	author = {Divya {Jindal-Snape}},
	year = {2004},
	pages = {470--483}
},

@article{karantonis_implementation_2006,
	title = {Implementation of a real-time human movement classifier using a triaxial accelerometer for ambulatory monitoring},
	volume = {10},
	issn = {1089-7771},
	doi = {10.1109/TITB.2005.856864},
	abstract = {The real-time monitoring of human movement can provide valuable information regarding an individual's degree of functional ability and general level of activity. This paper presents the implementation of a real-time classification system for the types of human movement associated with the data acquired from a single, waist-mounted triaxial accelerometer unit. The major advance proposed by the system is to perform the vast majority of signal processing onboard the wearable unit using embedded intelligence. In this way, the system distinguishes between periods of activity and rest, recognizes the postural orientation of the wearer, detects events such as walking and falls, and provides an estimation of metabolic energy expenditure. A laboratory-based trial involving six subjects was undertaken, with results indicating an overall accuracy of 90.8\% across a series of 12 tasks (283 tests) involving a variety of movements related to normal daily activities. Distinction between activity and rest was performed without error; recognition of postural orientation was carried out with 94.1\% accuracy, classification of walking was achieved with less certainty (83.3\% accuracy), and detection of possible falls was made with 95.6\% accuracy. Results demonstrate the feasibility of implementing an accelerometry-based, real-time movement classifier using embedded intelligence},
	number = {1},
	journal = {Information Technology in Biomedicine, {IEEE} Transactions on},
	author = {{D.M.} Karantonis and {M.R.} Narayanan and M. Mathie and {N.H.} Lovell and {B.G.} Celler},
	year = {2006},
	keywords = {accelerometer, accelerometers, ambulatory monitoring, biomedical measurement, biomedical telemetry, biomedical transducers, daily activity, embedded intelligence, embedded systems, gait analysis, health care, home computing, home telecare, mechanoception, medical signal processing, metabolic energy expenditure, movement classification, patient monitoring, postural orientation recognition, real-time human movement classifier, signal classification, signal processing, waist-mounted triaxial accelerometer, walking},
	pages = {156--167}
},

@article{yu_loneliness_2005,
	title = {Loneliness, peer acceptance, and family functioning of Chinese children with learning disabilities: Characteristics and relationships},
	volume = {42},
	shorttitle = {Loneliness, peer acceptance, and family functioning of Chinese children with learning disabilities},
	url = {http://dx.doi.org/10.1002/pits.20083},
	doi = {10.1002/pits.20083},
	abstract = {Although children with learning disabilities are often considered to be a heterogeneous group, they are always situated in specific social surroundings such as schools and families with which they interact dynamically in everyday life. Therefore, peer acceptance and family functioning may be related to the loneliness experienced by children with learning disabilities. This study explores the characteristics of loneliness and peer acceptance among children with learning disabilities and discusses the relationships among loneliness, peer acceptance, and family functioning. The results indicate that children with learning disabilities reported higher degrees of loneliness, but lower levels of peer acceptance; significant correlations existed between peer acceptance and loneliness, and between peer acceptance and family functioning; however, no significant correlations were found between loneliness and family functioning. ? 2005 Wiley Periodicals, Inc. Psychol Schs 42: 325-331, 2005.},
	number = {3},
	journal = {Psychology in the Schools},
	author = {Guoliang Yu and Yaming Zhang and Rong Yan},
	year = {2005},
	pages = {325--331}
},

@incollection{schloss_increasing_1994,
	address = {Boston},
	title = {Increasing appropriate
behavior through related personal characteristics},
	booktitle = {Applied Behavior Analysis in the Classroom},
	publisher = {Allyn \& Bacon},
	author = {P. J. Schloss and M. A. Smith},
	year = {1994}
},

@article{loftin_social_2008,
	title = {Social Interaction and Repetitive Motor Behaviors.},
	volume = {38},
	issn = {01623257},
	url = {http://login.ezproxy1.lib.asu.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=32537513&site=ehost-live},
	doi = {10.1007/s10803-007-0499-5},
	abstract = {Students with autism have difficulty initiating social interactions and may exhibit repetitive motor behavior (e.g., body rocking, hand flapping). Increasing social interaction by teaching new skills may lead to reductions in problem behavior, such as motor stereotypies. Additionally, self-monitoring strategies can increase the maintenance of skills. A multiple baseline design was used to examine whether multi-component social skills intervention (including peer training, social initiation instruction, and self-monitoring) led to a decrease in repetitive motor behavior. Social initiations for all participants increased when taught to initiate, and social interactions continued when self-monitoring was introduced. Additionally, participants’ repetitive motor behavior was reduced. Changes in social behavior and in repetitive motor behavior maintained more than one month after the intervention ended. {[ABSTRACT} {FROM} {AUTHOR]}},
	number = {6},
	journal = {Journal of Autism \& Developmental Disorders},
	author = {Rachel L. Loftin and Samuel L. Odom and Johanna F. Lantz},
	month = jul,
	year = {2008},
	keywords = {{AUTISM,} Interpersonal Relations, {MOTOR} ability, {PEER} teaching, Peer training, Repetitive motor behavior, Self-monitoring, {SELF-monitoring} {(Psychology),} {SOCIAL} interaction, social skills, {SOCIAL} skills -- Study \& teaching, {STEREOTYPED} behavior {(Psychiatry),} Stereotypic behavior, {STUDENTS}},
	pages = {1124--1135}
},

@article{raver_modification_1984,
	title = {Modification of Head Droop during Conversation in a {3-Year-Old} Visually Impaired Child: A Case Study},
	volume = {78},
	shorttitle = {Modification of Head Droop during Conversation in a {3-Year-Old} Visually Impaired Child},
	number = {7},
	journal = {Journal of Visual Impairment and Blindness},
	author = {Sharon Raver},
	year = {1984},
	pages = {307--10}
},

@book{cartledge_teaching_1986,
	edition = {2},
	title = {Teaching Social Skills to Children: Innovative Approaches},
	isbn = {0205142990},
	shorttitle = {Teaching Social Skills to Children},
	publisher = {Allyn \& Bacon},
	author = {Gwendolyn Cartledge},
	month = jun,
	year = {1986}
},

@article{felps_modification_1988,
	title = {Modification of Stereotypic Rocking of a Blind Adult.},
	volume = {82},
	number = {3},
	journal = {Journal of Visual Impairment and Blindness},
	author = {J. N. Felps and R. J. Devlin},
	year = {1988},
	pages = {107--08}
}?
@inproceedings{russell_british_2007,
	title = {British Machine Vision Conference},
	author = {David Russell and Shaogang Gong},
	year = {2007}
},

@article{radke_image_2005,
	title = {Image change detection algorithms: a systematic survey},
	volume = {14},
	issn = {1057-7149},
	shorttitle = {Image change detection algorithms},
	doi = {10.1109/TIP.2004.838698},
	abstract = {Detecting regions of change in multiple images of the same scene taken at different times is of widespread interest due to a large number of applications in diverse disciplines, including remote sensing, surveillance, medical diagnosis and treatment, civil infrastructure, and underwater sensing. This paper presents a systematic survey of the common processing steps and core decision rules in modern change detection algorithms, including significance and hypothesis testing, predictive models, the shading model, and background modeling. We also discuss important preprocessing methods, approaches to enforcing the consistency of the change mask, and principles for evaluating and comparing the performance of change detection algorithms. It is hoped that our classification of algorithms into a relatively small number of categories will provide useful guidance to the algorithm designer.},
	number = {3},
	journal = {Image Processing, {IEEE} Transactions on},
	author = {{R.J.} Radke and S. Andra and O. {Al-Kofahi} and B. Roysam},
	year = {2005},
	keywords = {background modeling, change detection, change mask, hypothesis testing, illumination invariance, image change detection algorithm, image classification, mixture models, predictive model, predictive models, shading model, significance testing, systematic survey},
	pages = {294--307}
},

@article{yun_zhai_video_2006,
	title = {Video scene segmentation using Markov chain Monte Carlo},
	volume = {8},
	issn = {1520-9210},
	abstract = {Videos are composed of many shots that are caused by different camera operations, e.g., on/off operations and switching between cameras. One important goal in video analysis is to group the shots into temporal scenes, such that all the shots in a single scene are related to the same subject, which could be a particular physical setting, an ongoing action or a theme. In this paper, we present a general framework for temporal scene segmentation in various video domains. The proposed method is formulated in a statistical fashion and uses the Markov chain Monte Carlo {(MCMC)} technique to determine the boundaries between video scenes. In this approach, a set of arbitrary scene boundaries are initialized at random locations and are automatically updated using two types of updates: diffusion and jumps. Diffusion is the process of updating the boundaries between adjacent scenes. Jumps consist of two reversible operations: the merging of two scenes and the splitting of an existing scene. The posterior probability of the target distribution of the number of scenes and their corresponding boundary locations is computed based on the model priors and the data likelihood. The updates of the model parameters are controlled by the hypothesis ratio test in the {MCMC} process, and the samples are collected to generate the final scene boundaries. The major advantage of the proposed framework is two-fold: 1) it is able to find the weak boundaries as well as the strong boundaries, i.e., it does not rely on the fixed threshold; 2) it can be applied to different video domains. We have tested the proposed method on two video domains: home videos and feature films, and accurate results have been obtained.},
	number = {4},
	journal = {Multimedia, {IEEE} Transactions on},
	author = {Yun Zhai and M. Shah},
	year = {2006},
	keywords = {diffusion process, image segmentation, jumps process, Markov chain Monte Carlo, Markov processes, Monte Carlo methods, posterior probability, target distribution, video analysis, video scene segmentation, video signal processing},
	pages = {686--697}
},

@inproceedings{fan_active_2004,
	title = {Active Mining of Data Streams},
	author = {Wei Fan and Yi-an Huang and Haixun Wang and Philip S. Yu},
	year = {2004}
},

@article{torr_feature_1999,
	title = {Feature based methods for structure and motion estimation},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.1517},
	doi = {10.1.1.37.1517},
	journal = {Vision Algorithms: Theory and Practice, number 1883 in {LNCS}},
	author = {P. H. S Torr and A. Zisserman},
	year = {1999},
	pages = {278---295}
},

@inproceedings{kawamura_estimation_2002,
	title = {Estimation of motion using motion blur for tracking vision system},
	volume = {13},
	abstract = {In this paper, we propose a novel algorithm for the optical flow estimation of a moving object by using the motion blur in a scene. The proposed technique is applied to the visual tracking system for the object moving on the {2D} plane that is perpendicular to the axis of lens of a camera. To estimate the motion parameters, we use the motion blur, which has been treated as noises under the conventional method. And we use the feature of motion blur in the frequency domain. The first step is the estimation of orientation of motion, which is achieved by computing the log power spectrum of a local image patch, and operating the coordinate transformation. The second step is the estimation of velocity by using the cepstral analysis. It is achieved by detecting the period of sine function relating to the length of motion blur. In order to evaluate the feasibility of the proposed algorithm, we investigate the accuracy of the estimated optical flow with artificial blurred images and real images. And we apply the proposed algorithm to the tracking vision system, which consists of a camera performing the pan motion, a video processing board, and a standard {PC.} Through some experiments, we show that the proposed technique is advantageous to the high-speed-tracking system.},
	author = {S. Kawamura and K. Kondo and Y. Konishi and H. Ishigaki},
	year = {2002},
	keywords = {{2D} plane, camera lens axis, cepstral analysis, coordinate transformation, high-speed-tracking system, image sequences, local image patch, log power spectrum, motion blur, motion estimation, noises, optical flow estimation, optical tracking, pan motion, {PC,} sine function period detection, tracking vision system, velocity estimation, video processing board, visual tracking system},
	pages = {371--376}
},

@article{bosch_scene_2008,
	title = {Scene Classification Using a Hybrid {Generative/Discriminative} Approach},
	volume = {30},
	issn = {0162-8828},
	abstract = {We investigate whether dimensionality reduction using a latent generative model is beneficial for the task of weakly supervised scene classification. In detail, we are given a set of labeled images of scenes (for example, coast, forest, city, river, etc.), and our objective is to classify a new image into one of these categories. Our approach consists of first discovering latent ";topics"; using probabilistic Latent Semantic Analysis {(pLSA),} a generative model from the statistical text literature here applied to a bag of visual words representation for each image, and subsequently, training a multiway classifier on the topic distribution vector for each image. We compare this approach to that of representing each image by a bag of visual words vector directly and training a multiway classifier on these vectors. To this end, we introduce a novel vocabulary using dense color {SIFT} descriptors and then investigate the classification performance under changes in the size of the visual vocabulary, the number of latent topics learned, and the type of discriminative classifier used (k-nearest neighbor or {SVM).} We achieve superior classification performance to recent publications that have used a bag of visual word representation, in all cases, using the authors' own data sets and testing protocols. We also investigate the gain in adding spatial information. We show applications to image retrieval with relevance feedback and to scene classification in videos.},
	number = {4},
	journal = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {A. Bosch and A. Zisserman and X. Muoz},
	year = {2008},
	keywords = {image classification, latent generative model, multiway classifier, {pLSA,} probabilistic latent semantic analysis, relevance feedback, Scene Classification, Spatial Information, statistical text literature, supervised scene classification, support vector machines, topic distribution vector, visual word representation, visual words vector},
	pages = {712--727}
},

@inproceedings{chowdhury_computer_2007,
	title = {Computer and Information Science, 2007. {ICIS} 2007. 6th {IEEE/ACIS} International Conference on},
	abstract = {This work presents a new approach to detecting the scene change in the successive capture of photographs of a place within equal time interval. This method is based on a gray level histogram of every image. In this method the histogram of an image is processed to modify it for matching with the processed histogram of a reference image. The coefficient of correlation is taken as the measure of matching. As the method does not do any heavy signal processing, and the images are taken successively with a multi-shot digital still camera, it can be applied for real-time processing of such pictures for detection of a scene change. A multi-camera in multi-position approach is also shown to evaluate the change in scene simultaneously from different angles. Both multi-camera and single-camera approaches are compared in detecting a scene change.},
	author = {{M.U.} Chowdhury and R. Rahman and J. Sana and {S.M.R.} Kabir},
	year = {2007},
	keywords = {correlation coefficient, correlation methods, fast scene change detection, gray level histogram, image matching, multiposition approach, multishot digital still camera, object detection, photographs, real-time processing, reference image},
	pages = {229--233}
},

@article{lelescu_statistical_2003,
	title = {Statistical sequential analysis for real-time video scene change detection on compressed multimedia bitstream},
	volume = {5},
	issn = {1520-9210},
	abstract = {The increased availability and usage of multimedia information have created a critical need for efficient multimedia processing algorithms. These algorithms must offer capabilities related to browsing, indexing, and retrieval of relevant data. A crucial step in multimedia processing is that of reliable video segmentation into visually coherent video shots through scene change detection. Video segmentation enables subsequent processing operations on video shots, such as video indexing, semantic representation, or tracking of selected video information. Since video sequences generally contain both abrupt and gradual scene changes, video segmentation algorithms must be able to detect a large variety of changes. While existing algorithms perform relatively well for detecting abrupt transitions (video cuts), reliable detection of gradual changes is much more difficult. A novel one-pass, real-time approach to video scene change detection based on statistical sequential analysis and operating on a compressed multimedia bitstream is proposed. Our approach models video sequences as stochastic processes, with scene changes being reflected by changes in the characteristics (parameters) of the process. Statistical sequential analysis is used to provide an unified framework for the detection of both abrupt and gradual scene changes.},
	number = {1},
	journal = {Multimedia, {IEEE} Transactions on},
	author = {D. Lelescu and D. Schonfeld},
	year = {2003},
	keywords = {abrupt scene changes, browsing, compressed multimedia bitstream, gradual scene changes, image retrieval, image segmentation, image sequences, indexing, multimedia processing algorithms, multimedia systems, one-pass real-time video scene change detection, real-time systems, reliable video segmentation, retrieval, statistical analysis, statistical sequential analysis, stochastic processes, video indexing, video semantic representation, video sequences, video tracking, visually coherent video shots},
	pages = {106--117}
},

@inproceedings{kyungnam_kim_background_2004,
	title = {Background modeling and subtraction by codebook construction},
	volume = {5},
	isbn = {1522-4880},
	abstract = {We present a new fast algorithm for background modeling and subtraction. Sample background values at each pixel are quantized into codebooks which represent a compressed form of background model for a long image sequence. This allows us to capture structural background variation due to periodic-like motion over a long period of time under limited memory. Our method can handle scenes containing moving backgrounds or illumination variations (shadows and highlights), and it achieves robust detection for compressed videos. We compared our method with other multimode modeling techniques.},
	author = {Kyungnam Kim and {T.H.} Chalidabhongse and D. Harwood and L. Davis},
	year = {2004},
	keywords = {background modeling, background subtraction, codebook construction, data compression, image colour analysis, image motion analysis, image representation, image sequence, image sequences, multimode modeling technique, quantisation (signal), video coding, video compression},
	pages = {3061--3064 Vol. 5}
},

@inproceedings{chaisorn_multimedia_2002,
	title = {Multimedia and Expo, 2002. {ICME} '02. Proceedings. 2002 {IEEE} International Conference on},
	volume = {1},
	doi = {10.1109/ICME.2002.1035721},
	abstract = {The segmentation of news video into single-story semantic units is a challenging problem. This research proposes a two-level, multi-modal framework to tackle this problem. The video is analyzed at the shot and story unit (or scene) levels using a variety of features and techniques. At the shot level, we employ a decision tree to classify the shot into one of 13 predefined categories. At the scene level, we perform {HMM} (hidden Markov models) analysis to locate the story boundaries. We test the performance of our system using two days of news video obtained from the {MediaCorp} of Singapore. Our initial results indicate that we could achieve a high accuracy of over 95\% for shot classification, and over 89\% in F1 measure on scene/story boundary detection.},
	author = {L. Chaisorn and {Tat-Seng} Chua and {Chin-Hui} Lee},
	year = {2002},
	keywords = {content-based retrieval, decision tree, decision trees, feature extraction, hidden Markov models, {HMM,} image classification, image retrieval, image segmentation, news video browsing, news video retrieval, news video segmentation, scene boundary detection, shot classification, story boundary detection, story units, video signal processing},
	pages = {73--76 vol.1}
},

@inproceedings{shu-ching_chen_multimedia_2001,
	title = {Multimedia and Expo, 2001. {ICME} 2001. {IEEE} International Conference on},
	author = {{Shu-Ching} Chen and {Mei-Ling} Shyu and {Cheng-Cui} Zhang and {R.L.} Kashyap},
	year = {2001},
	pages = {56--59}
},

@inproceedings{shengyang_dai_computer_2008,
	title = {Computer Vision and Pattern Recognition, 2008. {CVPR} 2008. {IEEE} Conference on},
	isbn = {1063-6919},
	doi = {10.1109/CVPR.2008.4587582},
	abstract = {Motion blur retains some information about motion, based on which motion may be recovered from blurred images. This is a difficult problem, as the situations of motion blur can be quite complicated, such as they may be space-variant, nonlinear, and local. This paper addresses a very challenging problem: can we recover motion blindly from a single motion-blurred image? A major contribution of this paper is a new finding of an elegant motion blur constraint. Exhibiting a very similar mathematical form as the optical flow constraint, this linear constraint applies locally to pixels in the image. Therefore, a number of challenging problems can be addressed, including estimating global affine motion blur, estimating global rotational motion blur, estimating and segmenting multiple motion blur, and estimating nonparametric motion blur field. Extensive experiments on blur estimation and image deblurring on both synthesized and real data demonstrate the accuracy and general applicability of the proposed approach.},
	author = {Shengyang Dai and Ying Wu},
	year = {2008},
	keywords = {blurred images, elegant motion, global affine motion blur, image deblurring, image motion analysis, image segmentation, linear constraint, optical flow constraint, single motion-blurred image},
	pages = {1--8}
},

@article{poor_quickest_1998,
	title = {Quickest Detection with Exponential Penalty for Delay},
	volume = {26},
	issn = {00905364},
	url = {http://www.jstor.org.ezproxy1.lib.asu.edu/stable/120083},
	number = {6},
	journal = {The Annals of Statistics},
	author = {H. Vincent Poor},
	month = dec,
	year = {1998},
	note = {The problem of detecting a change in the probability distribution of a random sequence is considered. Stopping times are derived that optimize the tradeoff between detection delay and false alarms within two criteria. In both cases, the detection delay is penalized exponentially rather than linearly, as has been the case in previous formulations of this problem. The first of these two criteria is to minimize a worst-case measure of the exponential detection delay within a lower-bound constraint on the mean time between false alarms. Expressions for the performance of the optimal detection rule are also developed for this case. It is seen, for example, that the classical Page {CUSUM} test can be arbitrarily unfavorable relative to the optimal test under exponential delay penalty. The second criterion considered is a Bayesian one, in which the unknown change point is assumed to obey a geometric prior distribution. In this case, the optimal stopping time effects an optimal trade-off between the expected exponential detection delay and the probability of false alarm. Finally, generalizations of these results to problems in which the penalties for delay may be path dependent are also considered.},
	pages = {2179--2205}
},

@article{pare_paucity_2003,
	title = {Paucity of presumptive ruffini corpuscles in the index finger pad of humans},
	volume = {456},
	url = {http://dx.doi.org/10.1002/cne.10519},
	doi = {10.1002/cne.10519},
	abstract = {Classically recognized as the cutaneous stretch receptors associated with the slowly adapting type {II} {(SAII)} primary afferents, Ruffini corpuscles have rarely been reported in the skin, despite numerous histologic investigations. Electrophysiological recordings of the primary afferents in humans suggest that {SAII} fibers represent approximately 15\% of the myelinated mechanosensitive axons in the peripheral nerves innervating the volar surface of the hand. In the present study, an analysis of glabrous skin was conducted in human donors to assess the distribution of Ruffini and Ruffini-like corpuscles in the distal phalanx of the index finger. Only one presumptive Ruffini corpuscle was found in the skin processed for double immunofluorescence labeling with antibodies against protein gene product 9.5 and neurofilament {200-kDa} subunit. Based on their relatively scattered distributions, we conclude that very few {SAII} primary afferents are likely to terminate as Ruffini corpuscles in human glabrous skin. J. Comp. Neurol. 456:260-266, 2003. © 2003 {Wiley-Liss,} Inc.},
	number = {3},
	journal = {The Journal of Comparative Neurology},
	author = {Michel Paré and Catherine Behets and Olivier Cornu},
	year = {2003},
	pages = {260--266}
},

@article{scheibert_role_2009,
	title = {The Role of Fingerprints in the Coding of Tactile Information Probed with a Biomimetic Sensor},
	url = {http://www.sciencemag.org/cgi/content/abstract/1166467v1},
	doi = {10.1126/science.1166467},
	abstract = {In humans, the tactile perception of fine textures (spatial scale {\textless}200 {micro}m) is mediated by skin vibrations generated as the finger scans the surface. To establish the relationship between texture characteristics and subcutaneous vibrations, a biomimetic tactile sensor has been designed whose dimensions match those of the fingertip. When the sensor surface is patterned with parallel ridges mimicking the fingerprints, the spectrum of vibrations elicited by randomly textured substrates is dominated by one frequency set by the ratio of the scanning speed to the inter-ridge distance. For human touch, this frequency falls within the optimal range of sensitivity of Pacinian afferents which mediate the coding of fine textures. Thus, fingerprints may perform spectral selection and amplification of tactile information that facilitate its processing by specific mechanoreceptors.},
	journal = {Science},
	author = {J. Scheibert and S. Leurent and A. Prevost and G. Debregeas},
	month = jan,
	year = {2009},
	pages = {1166467}
},

@article{iggo_structure_1969,
	title = {The structure and function of a slowly adapting touch corpuscle in hairy skin},
	volume = {200},
	url = {http://jp.physoc.org/content/200/3/763.abstract},
	doi = {VL  - 200},
	abstract = {1. Slowly adapting cutaneous mechanoreceptors, in the cat and primates, have been studied by histological and neurophysiological methods.
2. Each touch corpuscle is a dome-shaped elevation of the epidermis, whose deepest layer contains up to fifty specialized tactile cells.
3. Nerve plates, enclosed by the tactile cell {(Merkel} cells), are connected to a single myelinated axon in the dense collagenous core of the corpuscle.
4. The corpuscle generated {\textgreater} 1000 impulses/sec when excited by vertical surface pressure. The response was highly localized and showed a low mechanical threshold, the frequency being dependent upon the velocity and amplitude of the displacement. There was a period of rapid adaptation before a sustained response which might continue for {\textgreater} 30 min.
5. A quantitative analysis of the responses to excitation by displacements of differing amplitude, velocity and duration is included.
6. The discharge of touch corpuscle units evoked by a mechanical stimulus was temperature-sensitive, and was enhanced by a fall in skin temperature.},
	number = {3},
	journal = {The Journal of Physiology},
	author = {A. Iggo and A. R. Muir},
	month = feb,
	year = {1969},
	pages = {763--796}
},

@article{dillon_relationship_2001,
	title = {The relationship of the number of Meissner's corpuscles to dermatoglyphic characters and finger size},
	volume = {199},
	issn = {0021-8782},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1468368/},
	doi = {10.1046/j.1469-7580.2001.19950577.x},
	number = {5},
	journal = {Journal of Anatomy},
	author = {Yvonne K. Dillon and Julie Haynes and Maciej Henneberg},
	year = {2001},
	pages = {577--584}
},

@article{hensel_functional_1976,
	title = {Functional and structural basis of thermoreception},
	volume = {43},
	issn = {0079-6123},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/815955},
	journal = {Progress in Brain Research},
	author = {H Hensel},
	year = {1976},
	note = {{PMID:} 815955},
	keywords = {Adaptation, Physiological, Animals, Cats, Cold Temperature, Electrophysiology, Fishes, Haplorhini, Hot Temperature, Humans, Male, Nose, Scrotum, Skin, Snakes, Species Specificity, Temperature Sense, Thalamus, Thermoreceptors, Trigeminal Nerve},
	pages = {105--118}
},

@article{nakamura_somatosensory_1998,
	title = {Somatosensory homunculus as drawn by {MEG}},
	volume = {7},
	issn = {1053-8119},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/9626677},
	doi = {10.1006/nimg.1998.0332},
	abstract = {We studied a detailed somatosensory representation map of the human primary somatosensory cortex using magnetoencephalography. Somatosensory-evoked magnetic fields following tactile stimulation of multiple points in the right hemibody (including the tongue, lips, fingers, arm, trunk, leg, and foot) were analyzed in five normal subjects. We were able to estimate equivalent current dipoles {(ECDs)} following stimulation of the tongue, lips, fingers, palm, forearm, elbow, upper arm, and toes in most subjects and those following the stimulation of the chest, ankle, and thigh in one subject. The {ECDs} were located in the postcentral gyrus and generally arranged in order along the central sulcus, which is compatible with the somatosensory "homunculus." Linear distances, averaged in five subjects, from the receptive area of the thumb to that of the tongue, little finger, forearm, upper arm, and toes were estimated to be 2.42 +/- 0.28, 1.25 +/- 0.28, 2.21 +/- 0.72, 2.75 +/- 0.63, and 5.29 +/- 0.48 cm, respectively. The moment of each {ECD,} which suggested the size of the cortical areas responsive to the stimulation, was also compatible with the bizarre proportion of the homunculus with a large tongue, lips, and fingers. According to these results, we were able to reproduce a large part of the somatosensory homunculus quantitatively on an individual brain {MRI.}},
	number = {4 Pt 1},
	journal = {{NeuroImage}},
	author = {A Nakamura and T Yamada and A Goto and T Kato and K Ito and Y Abe and T Kachi and R Kakigi},
	month = may,
	year = {1998},
	note = {{PMID:} 9626677},
	keywords = {Brain Mapping, Humans, Magnetic Resonance Imaging, Magnetoencephalography, Physical Stimulation, Reference Values, Somatosensory Cortex, Touch},
	pages = {377--386}
},
@book{gardner_frames_1993,
	edition = {10th},
	title = {Frames Of Mind: The Theory Of Multiple Intelligences},
	isbn = {0465025102},
	shorttitle = {Frames Of Mind},
	publisher = {Basic Books},
	author = {Howard E. Gardner},
	month = apr,
	year = {1993}
},

@article{thorndike_intelligence_1920,
	title = {Intelligence and its uses},
	volume = {140},
	journal = {Harper's Magazine},
	author = {Edward L. Thorndike},
	year = {1920},
	pages = {227–235}
},

@book{albrecht_social_2005,
	title = {Social Intelligence: The New Science of Success},
	isbn = {0787979384},
	shorttitle = {Social Intelligence},
	publisher = {Pfeiffer},
	author = {Karl Albrecht},
	month = nov,
	year = {2005}
},

@book{bradberry_emotional_2009,
	edition = {{Har/Onl} En},
	title = {Emotional Intelligence 2.0},
	isbn = {0974320625},
	publisher = {{TalentSmart}},
	author = {Travis Bradberry and Jean Greaves},
	month = jun,
	year = {2009}
},

@book{matthews_science_2007,
	edition = {1},
	title = {Science of Emotional Intelligence: Knowns and Unknowns},
	isbn = {0195181891},
	shorttitle = {Science of Emotional Intelligence},
	publisher = {Oxford University Press, {USA}},
	author = {Gerald Matthews and Moshe Zeidner and Richard D. Roberts},
	month = aug,
	year = {2007}
},

@book{goleman_working_2000,
	title = {Working with Emotional Intelligence},
	isbn = {0553378589},
	publisher = {Bantam},
	author = {Daniel Goleman},
	month = jan,
	year = {2000}
},

@article{petrides_location_2007,
	title = {The location of trait emotional intelligence in personality factor space},
	volume = {98},
	url = {http://www.ingentaconnect.com.ezproxy1.lib.asu.edu/content/bpsoc/bjp/2007/00000098/00000002/art00007},
	doi = {10.1348/000712606X120618},
	abstract = {The construct of trait emotional intelligence (trait {EI} or trait emotional self-efficacy) provides a comprehensive operationalization of emotion-related self-perceptions and dispositions. In the first part of the present study {(N=274,} 92 males), we performed two joint factor analyses to determine the location of trait {EI} in Eysenckian and Big Five factor space. The results showed that trait {EI} is a compound personality construct located at the lower levels of the two taxonomies. In the second part of the study, we performed six two-step hierarchical regressions to investigate the incremental validity of trait {EI} in predicting, over and above the Giant Three and Big Five personality dimensions, six distinct criteria (life satisfaction, rumination, two adaptive and two maladaptive coping styles). Trait {EI} incrementally predicted four criteria over the Giant Three and five criteria over the Big Five. The discussion addresses common questions about the operationalization of emotional intelligence as a personality trait.},
	journal = {British Journal of Psychology},
	author = {K. V. [1] Petrides and Ria [2] Pita and Flora [3] Kokkinaki},
	month = may,
	year = {2007},
	keywords = {1, 2, 3},
	pages = {273--289}
},

@book{humphrey_vision_1974,
	title = {Vision in a monkey without striate cortex: a case study},
	volume = {3},
	url = {http://cogprints.org/1757/},
	abstract = {A rhesus monkey, Helen, from whom the striate cortex was almost totally removed, was studied intensively over a period of 8 years. During this time she regained an effective, though limited, degree of visually guided behaviour. The evidence suggests that while Helen suffered a permanent loss of 'focal vision' she retained (initially unexpressed) the capacity for 'ambient vision'.},
	author = {Nicholas K Humphrey},
	year = {1974},
	keywords = {blindsight, monkey, striate cortex, two visual systems}
},

@article{brothers_social_????,
	title = {The social brain: A project for integrating primate behavior and neurophysiology in a new domain.},
	volume = {1},
	shorttitle = {The social brain},
	journal = {Concepts in Neuroscience},
	author = {L Brothers},
	keywords = {social},
	pages = {51, 27}
},

@article{baron-cohen_social_1999,
	title = {Social intelligence in the normal and autistic brain: An {fMRI} study},
	volume = {11},
	shorttitle = {Social intelligence in the normal and autistic brain},
	url = {http://www.sciencedirect.com/science/article/B6WR5-3WRJ0GH-1YH/2/41c405d53d5e1a18ead17880a8d8664e},
	abstract = {There is increasing support for the existence of 'social intelligence' Humphrey (1984) Consciousness Regained!, independent of general intelligence. Brothers (1990) J. Cog. Neurosci., 4, 107-118! proposed a network of neural regions that comprise the 'social brain': the orbito-frontal cortex {(OFC),} superior temporal gyrus {(STG)} and amygdala. We tested Brothers' theory by examining both normal subjects as well as patients with high-functioning autism or Asperger syndrome {(AS),} who are well known to have deficits in social intelligence, and perhaps deficits in amygdala function Bauman and Kemper (1988) J. Neuropath. Exp. Neurol., 47, 369!. We used a test of judging from the expressions of another person's eyes what that other person might be thinking or feeling. Using functional magnetic resonance imaging {(fMRI)} we confirmed Brothers' prediction that the {STG} and amygdala show increased activation when using social intelligence. Some areas of the prefrontal cortex also showed activation. In contrast, patients with autism or {AS} activated the fronto-temporal regions but not the amygdala when making mentalistic inferences from the eyes. These results provide support for the social brain theory of normal function, and the amygdala theory of autism. {[Journal} Article; 56 Refs; In English; Summary in English]},
	number = {6},
	journal = {European Journal of Neuroscience},
	author = {S {Baron-Cohen} and H Ring and S Wheelwright and E Bullmore and M Brammer and A Simmons and S Williams},
	year = {1999},
	keywords = {intelligence, social},
	pages = {1898, 1891}
},

@book{perret-clermont_joining_2003,
	title = {Joining Society: Social Interaction and Learning in Adolescence and Youth},
	isbn = {0521520428},
	shorttitle = {Joining Society},
	publisher = {Cambridge University Press},
	author = {{Anne-Nelly} {Perret-Clermont} and Clotilde Pontecorvo and Lauren B. Resnick and Tania Zittoun and Barbara Burge},
	month = nov,
	year = {2003}
},

@article{jameson_principles_1945,
	title = {Principles of Social Interactions},
	volume = {10},
	journal = {American Sociological Review},
	author = {S. H. Jameson},
	year = {1945},
	pages = {6--12}
},

@book{knapp_nonverbal_1996,
	edition = {4th},
	title = {Nonverbal Communication in Human Interaction},
	isbn = {0030180236},
	publisher = {Harcourt College Pub},
	author = {Mark L. Knapp and Judith A. Hall},
	month = nov,
	year = {1996}
}?
@article{jameson_principles_1945,
	title = {Principles of Social Interaction},
	volume = {10},
	journal = {American Sociological Review},
	author = {S. H. Jameson},
	month = feb,
	year = {1945},
	pages = {6--12}
},

@article{couch_feedback:_1992,
	title = {Feedback: a behavioral approach to adjustment services},
	volume = {25},
	journal = {Vocational Evaluation and Work Adjustment Bulletin},
	author = {{R.H.} Couch and D. Magrega},
	year = {1992},
	pages = {89--92}
},

@book{jakob_exploring_2008,
	title = {Exploring the Phenomenon of Empathy},
	isbn = {{363905444X}},
	publisher = {{VDM} Verlag Dr. Mueller {e.K.}},
	author = {Eklund Jakob},
	month = jul,
	year = {2008}
},

@book{argyle_bodily_1990,
	edition = {2 Sub},
	title = {Bodily Communication},
	isbn = {0823605515},
	publisher = {International Universities Press},
	author = {Michael Argyle},
	month = dec,
	year = {1990}
},

@article{mcgaha_interactions_2001,
	title = {Interactions in an inclusive classroom: The effects of visual status and setting.},
	volume = {95},
	journal = {Journal of Visual Impairment \& Blindness},
	author = {C. G. {McGaha} and D. C. Farran},
	year = {2001},
	pages = {80--94}
},

@book{hall_hidden_1990,
	title = {The Hidden Dimension},
	isbn = {0385084765},
	publisher = {Anchor},
	author = {Edward T. Hall},
	month = oct,
	year = {1990}
},

@inproceedings{mcdaniel_proceedings_2009,
	address = {Boston, {MA,} {USA}},
	title = {Proceedings of the 27th international conference extended abstracts on Human factors in computing systems},
	isbn = {978-1-60558-247-4},
	url = {http://portal.acm.org/citation.cfm?id=1520340.1520718},
	doi = {10.1145/1520340.1520718},
	abstract = {This paper presents a scheme for using tactile rhythms to convey interpersonal distance to individuals who are blind or visually impaired, with the goal of providing access to non-verbal cues during social interactions. A preliminary experiment revealed that subjects could identify the proposed tactile rhythms and found them intuitive for the given application. Future work aims to improve recognition results and increase the number of interpersonal distances conveyed by incorporating temporal change information into the proposed methodology.},
	publisher = {{ACM}},
	author = {Troy L. {McDaniel} and Sreekar Krishna and Dirk Colbry and Sethuraman Panchanathan},
	year = {2009},
	keywords = {assistive technology, haptic belt, haptic icons, tactile icons, tactons, vibrotactile belt},
	pages = {4669--4674}
},

@article{celeste_social_2007,
	title = {Social Skills Intervention for a Child Who Is Blind},
	volume = {101},
	journal = {Journal of Visual Impairment \& Blindness},
	author = {M. Celeste},
	year = {2007},
	pages = {521--533}
},

@incollection{roloff_judgements_1984,
	address = {Beverly Hills, {CA}},
	title = {Judgements of interpersonal competency: How you know, What you know, and Who you know},
	booktitle = {Competence in Communication: A Multidisciplinary Approach},
	publisher = {Sage Publications},
	author = {M. K. Roloff and K. Kellermann},
	year = {1984},
	pages = {175--218}
},

@book{kekelis_development_1992,
	title = {The Development of Social Skills by Blind and Visually Impaired Students: Exploratory Studies and Strategies},
	isbn = {0891282173},
	shorttitle = {The Development of Social Skills by Blind and Visually Impaired Students},
	publisher = {Amer Foundation for the Blind},
	author = {Linda Kekelis and Sharon Sacks and Robert {Gaylord-Ross}},
	month = jun,
	year = {1992}
},

@article{mcadam_self-monitoring_1993,
	title = {Self-monitoring and verbal feedback to reduce stereotypic body rocking in a congenitally blind adult},
	volume = {24},
	issn = {08991510},
	url = {http://login.ezproxy1.lib.asu.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=9610170793&site=ehost-live},
	doi = {Article},
	abstract = {Presents the results of a study on the reduction of stereotypic body rocking among blind adults. Treatment of stereotypy through self-management; Examining efficacy of self-monitoring; Experimenting in a natural setting.},
	number = {4},
	journal = {{Re:View}},
	author = {David B. {McAdam} and Conall M. {O'Cleirigh}},
	year = {1993},
	keywords = {{VISION} disorders in old age},
	pages = {163}
},

@inproceedings{mcdaniel_haptic_2008,
	title = {Haptic Audio visual Environments and Games, 2008. {HAVE} 2008. {IEEE} International Workshop on},
	doi = {10.1109/HAVE.2008.4685291},
	abstract = {Good social skills are important and provide for a healthy, successful life; however, individuals with visual impairments are at a disadvantage when interacting with sighted peers due to inaccessible non-verbal cues. This paper presents a haptic (vibrotactile) belt to assist individuals who are blind or visually impaired by communicating non-verbal cues during social interactions. We focus on non-verbal communication pertaining to the relative location of the communicators with respect to the user in terms of direction and distance. Results from two experiments show that the haptic belt is effective in using vibration location and duration to communicate the relative direction and distance, respectively, of an individual in the userpsilas visual field.},
	author = {T. {McDaniel} and S. Krishna and V. Balasubramanian and D. Colbry and S. Panchanathan},
	year = {2008},
	keywords = {assistive technology, blind individual assistance, handicapped aids, haptic belt, Haptic Display, haptic interfaces, human computer interaction, Non-verbal Communication, nonverbal communication cue, {SOCIAL} interaction, Tactile Cues, Tactile Display, vibration location, Vibrotactile Cues, vision defects, visual impairment},
	pages = {13--18}
},

@article{baron-cohen_social_1999,
	title = {Social intelligence in the normal and autistic brain: an {fMRI} study},
	volume = {11},
	shorttitle = {Social intelligence in the normal and autistic brain},
	url = {http://dx.doi.org/10.1046/j.1460-9568.1999.00621.x},
	doi = {10.1046/j.1460-9568.1999.00621.x},
	abstract = {There is increasing support for the existence of 'social {intelligence'[Humphrey} (1984) Consciousness Regained], independent of general intelligence. Brothers et?al. 1990 ) J. Cog. Neurosci., 4, 1072013118] proposed a network of neural regions that comprise the 'social brain': the orbito-frontal cortex {(OFC),} superior temporal gyrus {(STG)} and amygdala. We tested Brothers' theory by examining both normal subjects as well as patients with high-functioning autism or Asperger syndrome {(AS),} who are well known to have deficits in social intelligence, and perhaps deficits in amygdala function {[Bauman} \& Kemper (1988) J. Neuropath. Exp. Neurol., 47, 369]. We used a test of judging from the expressions of another person's eyes what that other person might be thinking or feeling. Using functional magnetic resonance imaging {(fMRI)} we confirmed Brothers' prediction that the {STG} and amygdala show increased activation when using social intelligence. Some areas of the prefrontal cortex also showed activation. In contrast, patients with autism or {AS} activated the fronto-temporal regions but not the amygdala when making mentalistic inferences from the eyes. These results provide support for the social brain theory of normal function, and the amygdala theory of autism.},
	number = {6},
	journal = {European Journal of Neuroscience},
	author = {Simon {Baron-Cohen} and Howard A. Ring and Sally Wheelwright and Edward T. Bullmore and Mick J. Brammer and Andrew Simmons and Steve C. R. Williams},
	year = {1999},
	pages = {1891--1898}
},

@article{viola_robust_2001,
	title = {Robust Real-time Object Detection},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.2751},
	doi = {10.1.1.23.2751},
	journal = {International Journal of Computer Vision},
	author = {Paul Viola and Michael Jones},
	year = {2001}
},

@inproceedings{krishna_systematic_2008,
	address = {Marseille, France},
	title = {A Systematic Requirements Analysis and Development of an Assistive Device to Enhance the Social Interaction of People Who are Blind or Visually Impaired},
	booktitle = {Workshop on Computer Vision Applications for the Visually Impaired {(CVAVI} 08), European Conference on Computer Vision {ECCV} 2008},
	author = {S. Krishna and D. Colbry and {J.A.} Black and V. Balasubramanian and S. Panchanathan},
	month = oct,
	year = {2008}
},

@incollection{argyle_new_1979,
	address = {New York},
	title = {New developments in the analysis of social skills},
	booktitle = {Nonverbal behavior: Application and cultural implications},
	publisher = {Academic Press},
	author = {M. Argyle},
	editor = {A. Wolfgang},
	year = {1979},
	pages = {139--158}
},

@incollection{jankovic_stereotypies_1994,
	address = {London},
	title = {Stereotypies},
	volume = {3},
	booktitle = {Moverment Disorders},
	publisher = {{Butterworth-Heinemann}},
	author = {J. Jankovic},
	editor = {C. D. Marsden and S. Fahn},
	year = {1994},
	pages = {503--517}
},

@article{dallura_enhancing_2002,
	title = {Enhancing the Social Interaction Skills of Preschoolers with Visual Impairments.},
	volume = {96},
	issn = {{0145482X}},
	url = {http://login.ezproxy1.lib.asu.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=7255427&site=ehost-live},
	doi = {Article},
	abstract = {Abstract: This longitudinal, observational study of 13 children in a preschool for children with visual impairments examined the effects of reverse mainstreaming, in combination with the cooperative learning strategy, on the social interaction patterns of preschoolers with and without visual impairments. It found that the type of environment provided and the learning strategies used affect both whether and how children relate to their environment. {[ABSTRACT} {FROM} {AUTHOR]}},
	number = {8},
	journal = {Journal of Visual Impairment \& Blindness},
	author = {Tana {D'Allura}},
	year = {2002},
	keywords = {{CHILDREN,} {MAINSTREAMING} in education, {SOCIAL} interaction, {VISION} disorders},
	pages = {576}
},

@incollection{brothers_social_2002,
	title = {The social brain: A project for integrating primate behavior and neurophysiology in a new domain},
	isbn = {{026253195X,} 9780262531955},
	booktitle = {Foundations in Social Neuroscience},
	author = {Leslie Brothers},
	editor = {John T. Cacioppo},
	year = {2002},
	pages = {1345}
},

@article{haag_repetitive_1985,
	title = {Repetitive verbal behavior in severe aphasia},
	volume = {56},
	issn = {0028-2804},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/2415840},
	number = {10},
	journal = {Der Nervenarzt},
	author = {E Haag and W Huber and R Hündgen and U Stiller and K Willmes},
	month = oct,
	year = {1985},
	note = {{PMID:} 2415840},
	keywords = {Adult, Aged, Anomia, Aphasia, Aphasia, Broca, Aphasia, Wernicke, Echolalia, Female, {Follow-Up} Studies, Humans, Male, Middle Aged, Neuropsychological Tests, Semantics, Tomography, {X-Ray} Computed, Verbal Behavior},
	pages = {543--52}
},

@article{reivich_behavior_1972,
	title = {Behavior Problems of Deaf Children and Adolescents: A {Factor-Analytic} Study},
	volume = {15},
	shorttitle = {Behavior Problems of Deaf Children and Adolescents},
	url = {http://jslhr.asha.org/cgi/content/abstract/15/1/93},
	abstract = {Using the Behavior Problem Checklist, teachers rated 327 students (ages 6-20) in a state school for the deaf. Traits rated as present in at least 10\% of the students were intercorrelated, then a principal-component factor analysis and an orthogonal rotation of the factor matrix were accomplished by electronic computer. Five factors were rotated in accordance with Kaiser's varimax criteria. The first three factors extracted--which accounted for approximately 70\% of the common factor variance and the preponderance of disturbed behavior--were strikingly similar to the conduct, personality, and immaturity dimensions consistently identified in previously studied normal and disturbed populations. Two other factors, labeled isolation and communication problem, were also extracted. These may represent a more or less deafness-specific cluster of behavior problems.},
	number = {1},
	journal = {J Speech Hear Res},
	author = {Ronald S. Reivich and Irvin A. Rothrock},
	month = mar,
	year = {1972},
	pages = {93--104}
},

@article{riggio_assessment_1986,
	title = {Assessment of basic social skills},
	volume = {51},
	issn = {0022-3514},
	abstract = {Proposes a conceptual framework for defining and assessing basic social skills derived from the attempts of social personality psychologists to measure individual differences in nonverbal communication skills. Preliminary testing resulted in the development of a 105-item, pencil-and-paper measure of 7 basic dimensions of social skills, the Social Skills Inventory {(SSI).} In a series of validation studies using 149 undergraduate students, the {SSI} demonstrated convergent and discriminant validity in relation to other measures of nonverbal social skill and traditional personality scales (e.g., the Sixteen Personality Factor Questionnaire {[16PF],} the {Marlowe-Crowne} Social Desirability Scale). Scores on the {SSI} also predicted some social group memberships, typical social behaviors, and the depth of social networks. Evidence suggests that the {SSI} could prove to be a valuable tool for research in personality and social psychology and for work in applied settings. (57 ref) {(PsycINFO} Database Record (c) 2007 {APA,} all rights reserved)},
	number = {3},
	journal = {Journal of Personality and Social Psychology},
	author = {Ronald E Riggio},
	year = {1986},
	keywords = {inventories, measurement, social skills, test validity},
	pages = {649--660}
},

@incollection{riggio_social_1991,
	address = {London},
	title = {Social skills and interpersonal competence: Influences on social support and social seeking},
	booktitle = {Advances in Personal Relationships},
	publisher = {Jessica Kingsley},
	author = {Ronald E Riggio and J. Zimmermann},
	editor = {W. H. Jones and D. Perlman},
	year = {1991},
	pages = {133--155}
},

@article{segrin_poor_2000,
	title = {Poor Social Skills Are a Vulnerability Factor in the Development of Psychosocial Problems},
	volume = {26},
	issn = {{ISSN-0360-3989}},
	number = {3},
	journal = {Human Communication Research},
	author = {Chris Segrin and Jeanne Flora},
	year = {2000},
	pages = {489--514}
},

@book{mcginnis_skillstreaming_1997,
	title = {Skillstreaming the Elementary School Child: New Strategies and Perspectives for Teaching Prosocial Skills},
	publisher = {Research Press},
	author = {E. {McGinnis} and A. P. Goldstein},
	year = {1997}
},

@article{ricci-bitti_universals_1987,
	title = {Universals and cultural differences in the judgments of facial expressions of emotion},
	volume = {53},
	issn = {0022-3514},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/3681648},
	doi = {3681648},
	abstract = {We present here new evidence of cross-cultural agreement in the judgement of facial expression. Subjects in 10 cultures performed a more complex judgment task than has been used in previous cross-cultural studies. Instead of limiting the subjects to selecting only one emotion term for each expression, this task allowed them to indicate that multiple emotions were evident and the intensity of each emotion. Agreement was very high across cultures about which emotion was the most intense. The 10 cultures also agreed about the second most intense emotion signaled by an expression and about the relative intensity among expressions of the same emotion. However, cultural differences were found in judgments of the absolute level of emotional intensity.},
	number = {4},
	journal = {Journal of Personality and Social Psychology},
	author = {P E {Ricci-Bitti} and P Ekman and W V Friesen and M {O'Sullivan} and A Chan and I {Diacoyanni-Tarlatzis} and K Heider and R Krause and W A {LeCompte} and T Pitcairn},
	month = oct,
	year = {1987},
	note = {{PMID:} 3681648},
	keywords = {Adult, Attitude, {Cross-Cultural} Comparison, Emotions, Facial expression, Female, Humans, Judgment, Male},
	pages = {712--7}
},

@article{yu_loneliness_2005,
	title = {Loneliness, peer acceptance, and family functioning of Chinese children with learning disabilities: Characteristics and relationships},
	volume = {42},
	shorttitle = {Loneliness, peer acceptance, and family functioning of Chinese children with learning disabilities},
	url = {http://dx.doi.org/10.1002/pits.20083},
	doi = {10.1002/pits.20083},
	abstract = {Although children with learning disabilities are often considered to be a heterogeneous group, they are always situated in specific social surroundings such as schools and families with which they interact dynamically in everyday life. Therefore, peer acceptance and family functioning may be related to the loneliness experienced by children with learning disabilities. This study explores the characteristics of loneliness and peer acceptance among children with learning disabilities and discusses the relationships among loneliness, peer acceptance, and family functioning. The results indicate that children with learning disabilities reported higher degrees of loneliness, but lower levels of peer acceptance; significant correlations existed between peer acceptance and loneliness, and between peer acceptance and family functioning; however, no significant correlations were found between loneliness and family functioning. ? 2005 Wiley Periodicals, Inc. Psychol Schs 42: 325-331, 2005.},
	number = {3},
	journal = {Psychology in the Schools},
	author = {Guoliang Yu and Yaming Zhang and Rong Yan},
	year = {2005},
	pages = {325--331}
},

@article{jindal-snape_generalization_2004,
	title = {Generalization and Maintenance of Social Skills of Children with Visual Impairments: Self-evaluation and the Role of Feedback.},
	volume = {98},
	issn = {{0145482X}},
	shorttitle = {Generalization and Maintenance of Social Skills of Children with Visual Impairments},
	url = {http://login.ezproxy1.lib.asu.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=14199629&site=ehost-live},
	doi = {Article},
	abstract = {Abstract: A multiple baseline design across behaviors was used with two visually impaired girls to establish the effectiveness of self-evaluation and the role of feedback. In both cases, self-evaluation was effective in increasing the girls' social skills and social interaction. Implications of the role of significant others in providing feedback are discussed. {[ABSTRACT} {FROM} {AUTHOR]}},
	number = {8},
	journal = {Journal of Visual Impairment \& Blindness},
	author = {Divya {Jindal-Snape}},
	year = {2004},
	keywords = {{CHILDREN} with visual disabilities, {PEOPLE} with visual disabilities, {SELF-evaluation,} {SOCIAL} interaction, social skills},
	pages = {470--483}
},

@article{felps_modification_1988,
	title = {Modification of Stereotypic Rocking of a Blind Adult.},
	volume = {82},
	number = {3},
	journal = {Journal of Visual Impairment and Blindness},
	author = {J. N. Felps and R. J. Devlin},
	year = {1988},
	pages = {107--08}
},

@article{fawcett_introduction_2006,
	title = {An introduction to {ROC} analysis},
	volume = {27},
	url = {http://portal.acm.org/citation.cfm?id=1159475},
	abstract = {Receiver operating characteristics {(ROC)} graphs are useful for organizing classifiers and visualizing their performance. {ROC} graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although {ROC} graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to {ROC} graphs and as a guide for using them in research.},
	number = {8},
	journal = {Pattern Recogn. Lett.},
	author = {Tom Fawcett},
	year = {2006},
	keywords = {classifier evaluation, evaluation metrics, roc analysis},
	pages = {861--874}
},

@article{rabiner_tutorial_1989,
	title = {A tutorial on hidden Markov models and selected applications in speech recognition},
	volume = {77},
	issn = {0018-9219},
	doi = {10.1109/5.18626},
	abstract = {This tutorial provides an overview of the basic theory of hidden
Markov models {(HMMs)} as originated by {L.E.} Baum and T. Petrie (1966) and
gives practical details on methods of implementation of the theory along
with a description of selected applications of the theory to distinct
problems in speech recognition. Results from a number of original
sources are combined to provide a single source of acquiring the
background required to pursue further this area of research. The author
first reviews the theory of discrete Markov chains and shows how the
concept of hidden states, where the observation is a probabilistic
function of the state, can be used effectively. The theory is
illustrated with two simple examples, namely coin-tossing, and the
classic balls-in-urns system. Three fundamental problems of {HMMs} are
noted and several practical techniques for solving these problems are
given. The various types of {HMMs} that have been studied, including
ergodic as well as left-right models, are described},
	number = {2},
	journal = {Proceedings of the {IEEE}},
	author = {{L.R.} Rabiner},
	year = {1989},
	keywords = {balls-in-urns system, coin-tossing, discrete Markov chains, ergodic models, hidden Markov models, hidden states, left-right models, Markov processes, probabilistic function, speech recognition},
	pages = {257--286}
},

@inproceedings{bao_activity_2004,
	title = {Activity recognition from user-annotated acceleration data},
	author = {Ling Bao and Stephen S. Intille},
	year = {2004},
	pages = {1--17}
},

@article{bussmann_second_2001,
	title = {Second International Conference Proceedings, Pervasive Computing},
	volume = {33},
	number = {3},
	journal = {Behavior Research Methods, Instruments, \& Computers},
	author = {J. B. Bussmann and W. L. Martens},
	year = {2001},
	pages = {349--356}
},

@article{yuan_induction_1995,
	title = {Induction of fuzzy decision trees},
	volume = {69},
	url = {http://portal.acm.org/citation.cfm?id=211860.211865},
	number = {2},
	journal = {Fuzzy Sets Syst.},
	author = {Yufei Yuan and Michael J. Shaw},
	year = {1995},
	keywords = {expert systems, knowledge acquisition and learning, measures of information, possibility theory},
	pages = {125--139}
},

@article{seon-woo_lee_activity_2002,
	title = {Activity and location recognition using wearable sensors},
	volume = {1},
	issn = {1536-1268},
	doi = {10.1109/MPRV.2002.1037719},
	abstract = {Using measured acceleration and angular velocity data gathered through inexpensive, wearable sensors, this dead-reckoning method can determine a user's location, detect transitions between preselected locations, and recognize and classify sitting, standing, and walking behaviors. Experiments demonstrate the proposed method's effectiveness.},
	number = {3},
	journal = {Pervasive Computing, {IEEE}},
	author = {{Seon-Woo} Lee and K. Mase},
	year = {2002},
	keywords = {angular velocity, dead-reckoning method, measured acceleration, mobile computing, position measurement, preselected locations, sensors, sitting, standing, transitions, user's location, walking, wearable sensors},
	pages = {24--32}
},

@techreport{vezhnevets_gml_2007,
	title = {{GML} {AdaBoost} Matlab Toolbox 0.3},
	url = {http://research.graphicon.ru/machine-learning/modest-adaboost-4.html},
	institution = {Graphics and Media Lab, Moscow State University},
	author = {Alexander Vezhnevets and Vladimir Vezhnevets},
	month = aug,
	year = {2007}
},

@inproceedings{vezhnevets_modest_2005,
	address = {Novosibirsk Akademgorodok, Russia},
	title = {Modest {AdaBoost} - Teaching {AdaBoost} to Generalize Better},
	author = {Alexander Vezhnevets and Vladimir Vezhnevets},
	year = {2005}
},

@article{grant_validation_2006,
	title = {The validation of a novel activity monitor in the measurement of posture and motion during everyday activities},
	volume = {40},
	url = {http://bjsm.bmj.com/cgi/content/abstract/40/12/992},
	doi = {10.1136/bjsm.2006.030262},
	abstract = {Background: Accurate measurement of physical activity patterns can be used to identify sedentary behaviour and may facilitate interventions aimed at reducing inactivity. Objective: To evaluate the {activPAL} physical activity monitor as a measure of posture and motion in everyday activities using observational analysis as the criterion standard. Methods: Wearing three {activPAL} monitors, 10 healthy participants performed a range of randomly assigned everyday tasks incorporating walking, standing and sitting. Each trial was captured on a digital camera and the recordings were synchronised with the {activPAL.} The time spent in different postures was visually classified and this was compared with the {activPAL} output. Results: Intraclass correlation coefficients {(ICC} 2,1) for interdevice reliability ranged from 0.79 to 0.99. Using the Bland and Altman method, the mean percentage difference between the {activPAL} monitor and observation for total time spent sitting was 0.19\% (limits of agreement -0.68\% to 1.06\%) and for total time spent upright was -0.27\% (limits of agreement -1.38\% to 0.84\%). The mean difference for total time spent standing was 1.4\% (limits of agreement -6.2\% to 9.1\%) and for total time spent walking was -2.0\% (limits of agreement -16.1\% to 12.1\%). A second-by-second analysis between observer and monitor found an overall agreement of 95.9\%. Conclusion: The {activPAL} activity monitor is a valid and reliable measure of posture and motion during everyday physical activities.},
	number = {12},
	journal = {Br J Sports Med},
	author = {P M Grant and C G Ryan and W W Tigbe and M H Granat},
	month = dec,
	year = {2006},
	pages = {992--997}
},

@article{foerster_detection_1999,
	title = {Detection of posture and motion by accelerometry: a validation in amulatory monitoring},
	volume = {15},
	journal = {Computer in Human Behavior},
	author = {F. Foerster and M. Smeja and J. Fahrenberg},
	year = {1999},
	pages = {571--583}
},

@book{cristianini_introduction_2000,
	edition = {1},
	title = {An Introduction to Support Vector Machines and Other Kernel-based Learning Methods},
	isbn = {0521780195},
	publisher = {Cambridge University Press},
	author = {Nello Cristianini and John {Shawe-Taylor}},
	month = mar,
	year = {2000}
},

@inproceedings{krishnan_acoustics_2008,
	title = {Acoustics, Speech and Signal Processing, 2008. {ICASSP} 2008. {IEEE} International Conference on},
	isbn = {1520-6149},
	doi = {10.1109/ICASSP.2008.4518365},
	abstract = {The advent of wearable sensors like accelerometers has opened a plethora of opportunities to recognize human activities from other low resolution sensory streams. In this paper we formulate recognizing activities from accelerometer data as a classification problem. In addition to the statistical and spectral features extracted from the acceleration data, we propose to extract features that characterize the variations in the first order derivative of the acceleration signal. We evaluate the performance of different state of the art discriminative classifiers like, boosted decision stumps {(AdaBoost),} support vector machines {(SVM)} and regularized logistic regression {(RLogReg)} under three different evaluation scenarios (namely subject independent, subject adaptive and subject dependent). We propose a novel computationally inexpensive methodology for incorporating smoothing classification temporally, that can be coupled with any classifier with minimal training for classifying continuous sequences. While a 3\% increase in the classification accuracy was observed on adding the new features, the proposed technique for continuous recognition showed a 2.5 - 3\% improvement in the performance.},
	author = {{N.C.} Krishnan and S. Panchanathan},
	year = {2008},
	keywords = {acceleration signal, accelerometer, accelerometers, {AdaBoost,} biomedical equipment, boosted decision stumps, continuous recognition, data analysis, discriminative classifiers, feature extraction, gait analysis, handicapped aids, human activity recognition, low resolution sensory streams, medical signal processing, regression analysis, regularized logistic regression, sequence classification, signal classification, smoothing classification, spectral features, statistical features, support vector machines, {SVM,} wearable sensors},
	pages = {3337--3340}
},

@inproceedings{chambers_hierarchical_2002,
	title = {Hierarchical recognition of intentional human gestures for sports video annotation},
	volume = {2},
	isbn = {1051-4651},
	doi = {10.1109/ICPR.2002.1048493},
	abstract = {We present a novel technique for the recognition of complex human gestures for video annotation using accelerometers and the hidden Markov model. Our extension to the standard hidden Markov model allows us to consider gestures at different levels of abstraction through a hierarchy of hidden states. Accelerometers in the form of wrist bands are attached to humans performing intentional gestures, such as umpires in sports. Video annotation is then performed by populating the video with time stamps indicating significant events, where a particular gesture occurs. The novelty of the technique lies in the development of a probabilistic hierarchical framework for complex gesture recognition and the use of accelerometers to extract gestures and significant events for video annotation.},
	author = {{G.S.} Chambers and S. Venkatesh and {G.A.W.} West and {H.H.} Bui},
	year = {2002},
	keywords = {accelerometers, complex gesture recognition, gesture extraction, gesture recognition, hidden Markov model, hidden Markov models, hidden states, hierarchical recognition, intentional human gestures, probabilistic hierarchical framework, significant events, sport, sports video annotation, time stamps, umpires, video signal processing, wrist bands},
	pages = {1082--1085 vol.2}
},

@inproceedings{arteaga_low-cost_2008,
	address = {Halifax, Nova Scotia, Canada},
	title = {Low-cost accelerometry-based posture monitoring system for stroke survivors},
	isbn = {978-1-59593-976-0},
	url = {http://portal.acm.org/citation.cfm?id=1414471.1414519},
	doi = {10.1145/1414471.1414519},
	abstract = {This paper reports a low-cost autonomous wearable accelerometry-based posture monitoring system for stroke survivors. The hardware part of the system consists of monitoring devices, each of which comprises of a three-axial accelerometer and a beeper, {LED} light and vibrator to provide redundant modes of inappropriate posture warnings that would hopefully trigger self-correction. The inappropriate posture data are stored in an {EEPROM.} The software part of the system downloads, analyzes and presents the data in graphical format to enable a carer or therapist to quickly glance at the durations, frequency and locations of inappropriate postures.},
	publisher = {{ACM}},
	author = {Sonia Arteaga and Jessica Chevalier and Andrew Coile and Andrew William Hill and Serdar Sali and Sangheeta Sudhakhrisnan and Sri H. Kurniawan},
	year = {2008},
	keywords = {accelerometer, physiotherapy, rehabilitation, stroke},
	pages = {243--244}
},

@article{polikar_ensemble_2006,
	title = {Ensemble based systems in decision making},
	volume = {6},
	issn = {{1531-636X}},
	doi = {10.1109/MCAS.2006.1688199},
	abstract = {In matters of great importance that have financial, medical, social, or other implications, we often seek a second opinion before making a decision, sometimes a third, and sometimes many more. In doing so, we weigh the individual opinions, and combine them through some thought process to reach a final decision that is presumably the most informed one. The process of consulting "several experts" before making a final decision is perhaps second nature to us; yet, the extensive benefits of such a process in automated decision making applications have only recently been discovered by computational intelligence community. Also known under various other names, such as multiple classifier systems, committee of classifiers, or mixture of experts, ensemble based systems have shown to produce favorable results compared to those of single-expert systems for a broad range of applications and under a variety of scenarios. Design, implementation and application of such systems are the main topics of this article. Specifically, this paper reviews conditions under which ensemble based systems may be more beneficial than their single classifier counterparts, algorithms for generating individual components of the ensemble systems, and various procedures through which the individual classifiers can be combined. We discuss popular ensemble based algorithms, such as bagging, boosting, {AdaBoost,} stacked generalization, and hierarchical mixture of experts; as well as commonly used combination rules, including algebraic combination of outputs, voting based techniques, behavior knowledge space, and decision templates. Finally, we look at current and future research directions for novel applications of ensemble systems. Such applications include incremental learning, data fusion, feature selection, learning with missing features, confidence estimation, and error correcting output codes; all areas in which ensemble systems have shown great promise},
	number = {3},
	journal = {Circuits and Systems Magazine, {IEEE}},
	author = {R. Polikar},
	year = {2006},
	keywords = {{AdaBoost,} automated decision making, bagging, behavior knowledge space, boosting, confidence estimation, data fusion, decision making, decision support systems, decision templates, ensemble based systems, error correcting output codes, error correction codes, expert systems, feature selection, incremental learning, learning (artificial intelligence), learning systems, multiple classifier systems, pattern classification, reviews, sensor fusion, single-expert systems, stacked generalization, voting based techniques},
	pages = {21--45}
},

@techreport{_drm103_2008,
	title = {{DRM103} Designer Reference Manual},
	number = {{DRM103} Rev. 1},
	institution = {Freescale Semiconductor},
	month = aug,
	year = {2008},
	pages = {88}
},

@article{benjamini_opening_1988,
	title = {Opening the Box of a Boxplot},
	volume = {42},
	issn = {00031305},
	url = {http://www.jstor.org/stable/2685133},
	doi = {10.2307/2685133},
	abstract = {Variations of the boxplot are suggested, in which the sides of the box are used to convey information about the density of the values in a batch. The ease of computation by hand of the original boxplot had to be sacrificed, as the variations are computer-intensive. Still, the plots were implemented on a desktop personal computer {(Apple} Macintosh), in a way designed to keep their ease of computation by computer. The result is a dynamic display of densities and summaries.},
	number = {4},
	journal = {The American Statistician},
	author = {Yoav Benjamini},
	month = nov,
	year = {1988},
	note = {{ArticleType:} primary\_article / Full publication date: Nov., 1988 / Copyright © 1988 American Statistical Association},
	pages = {257--262}
},

@article{kotsiantis_supervised_2007,
	title = {Supervised machine learning: a review of classification techniques},
	volume = {31},
	number = {3},
	journal = {Informatica},
	author = {S. B. Kotsiantis},
	year = {2007},
	pages = {249--268}
},

@inproceedings{krishnan_analysis_2008,
	title = {Analysis of low resolution accelerometer data for continuous human activity recognition},
	isbn = {1520-6149},
	doi = {10.1109/ICASSP.2008.4518365},
	abstract = {The advent of wearable sensors like accelerometers has opened a plethora of opportunities to recognize human activities from other low resolution sensory streams. In this paper we formulate recognizing activities from accelerometer data as a classification problem. In addition to the statistical and spectral features extracted from the acceleration data, we propose to extract features that characterize the variations in the first order derivative of the acceleration signal. We evaluate the performance of different state of the art discriminative classifiers like, boosted decision stumps {(AdaBoost),} support vector machines {(SVM)} and regularized logistic regression {(RLogReg)} under three different evaluation scenarios (namely subject independent, subject adaptive and subject dependent). We propose a novel computationally inexpensive methodology for incorporating smoothing classification temporally, that can be coupled with any classifier with minimal training for classifying continuous sequences. While a 3\% increase in the classification accuracy was observed on adding the new features, the proposed technique for continuous recognition showed a 2.5 - 3\% improvement in the performance.},
	booktitle = {Acoustics, Speech and Signal Processing, 2008. {ICASSP} 2008. {IEEE} International Conference on},
	author = {{N.C.} Krishnan and S. Panchanathan},
	year = {2008},
	keywords = {acceleration signal, accelerometer, accelerometers, {AdaBoost,} biomedical equipment, boosted decision stumps, continuous recognition, data analysis, discriminative classifiers, feature extraction, gait analysis, handicapped aids, human activity recognition, low resolution sensory streams, medical signal processing, regression analysis, regularized logistic regression, sequence classification, signal classification, smoothing classification, spectral features, statistical features, support vector machines, {SVM,} wearable sensors},
	pages = {3337--3340}
},

@inproceedings{raina_self-taught_2007,
	address = {Corvalis, Oregon},
	title = {Self-taught learning: transfer learning from unlabeled data},
	isbn = {978-1-59593-793-3},
	shorttitle = {Self-taught learning},
	url = {http://portal.acm.org/citation.cfm?id=1273592},
	doi = {10.1145/1273496.1273592},
	abstract = {We present a new machine learning framework called "self-taught learning" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an {SVM} for classification, we further show how a Fisher kernel can be learned for this representation.},
	booktitle = {Proceedings of the 24th international conference on Machine learning},
	publisher = {{ACM}},
	author = {Rajat Raina and Alexis Battle and Honglak Lee and Benjamin Packer and Andrew Y. Ng},
	year = {2007},
	pages = {759--766}
},

@article{jindal-snape_use_2005,
	title = {Use of Feedback from Sighted Peers in Promoting Social Interaction Skills},
	volume = {99},
	issn = {{ISSN-0145-482X}},
	url = {http://www.eric.ed.gov/ERICWebPortal/contentdelivery/servlet/ERICServlet?accno=EJ692451},
	number = {7},
	journal = {Journal of Visual Impairment and Blindness},
	author = {Divya {Jindal-Snape}},
	month = jul,
	year = {2005},
	keywords = {India {(New} Delhi)},
	pages = {1--16},
	annote = {A boy who was visually impaired was trained to self-evaluate his social interaction, and a sighted peer was trained to provide relevant feedback to the boy through verbal reinforcement by the researcher. This feedback enhanced the boy's social interaction with his sighted peers, improved certain aspects of his social behavior, and increased the accuracy of his self-evaluation for behaviors that require visual cues.}
},

@article{jindal-snape_using_1998,
	title = {Using self-evaluation procedures to maintain social skills in a child who is blind},
	volume = {92},
	issn = {{ISSN-0145-482X}},
	url = {http://www.eric.ed.gov/ERICWebPortal/contentdelivery/servlet/ERICServlet?accno=EJ692451},
	journal = {Journal of Visual Impairment and Blindness},
	author = {Divya {Jindal-Snape}},
	year = {1998},
	keywords = {India {(New} Delhi)},
	pages = {362--366}
},

@article{jindal-snape_generalization_2004,
	title = {Generalization and Maintenance of Social Skills of Children with Visual Impairments: {Self-Evaluation} and the Role of Feedback},
	volume = {98},
	issn = {{ISSN-0145-482X}},
	shorttitle = {Generalization and Maintenance of Social Skills of Children with Visual Impairments},
	url = {http://www.eric.ed.gov/ERICWebPortal/contentdelivery/servlet/ERICServlet?accno=EJ683815},
	number = {8},
	journal = {Journal of Visual Impairment and Blindness},
	author = {Divya {Jindal-Snape}},
	year = {2004},
	pages = {470--483},
	annote = {A multiple baseline design across behaviors was used with two visually impaired girls to establish the effectiveness of self-evaluation and the role of feedback. In both cases, self-evaluation was effective in increasing the girls' social skills and social interaction. Implications of the role of significant others in providing feedback are discussed.}
},

@article{dallura_enhancing_2002,
	title = {Enhancing the Social Interaction Skills of Preschoolers with Visual Impairments.},
	volume = {96},
	issn = {{ISSN-0145-482X}},
	number = {8},
	journal = {Journal of Visual Impairment \& Blindness},
	author = {Tana {D'Allura}},
	year = {2002},
	pages = {576--84},
	annote = {A study involving 9 preschool children with visual impairments found that following a cooperative learning strategy intervention in an integrated preschool, the children with visual impairments interacted with peers at levels comparable to those of sighted children, while those in the self-contained class maintained the same rate of interactions. {(Contains} references.) {(CR)}}
},

@article{mcgaha_interactions_2001,
	title = {Interactions in an Inclusive Classroom: The Effects of Visual Status and Setting.},
	volume = {95},
	issn = {{ISSN-0145-482X}},
	shorttitle = {Interactions in an Inclusive Classroom},
	number = {2},
	journal = {Journal of Visual Impairment \& Blindness},
	author = {Cindy G. {McGaha} and Dale C. Farran},
	year = {2001},
	pages = {80--94},
	annote = {A study examined the effect of visual status (visually impaired or sighted) and setting (indoor or outdoor) on the social behaviors of nine preschoolers with visual impairments and 11 typical preschoolers in an inclusive setting. Regardless of their visual status, the children spent significantly more time near sighted children. {(Contains} references.) {(Author/CR)}}
},

@book{kekelis_development_1992,
	title = {The Development of Social Skills by Blind and Visually Impaired Students: Exploratory Studies and Strategies},
	isbn = {0891282173},
	shorttitle = {The Development of Social Skills by Blind and Visually Impaired Students},
	publisher = {Amer Foundation for the Blind},
	author = {Linda Kekelis},
	month = jun,
	year = {1992}
},

@article{eichel_mannerisms_1978,
	title = {Mannerisms of the Blind: A Review of the Literature.},
	volume = {72},
	shorttitle = {Mannerisms of the Blind},
	journal = {Journal of Visual Impairment \& Blindness},
	author = {Valerie J. Eichel},
	year = {1978},
	keywords = {Manneristic Behavior},
	pages = {125--130}
},


@article{shinohara_blind_2009,
	title = {A blind person's interactions with technology},
	volume = {52},
	url = {http://portal.acm.org/ft_gateway.cfm?id=1536636&type=html&coll=GUIDE&dl=GUIDE&CFID=69626716&CFTOKEN=78291016},
	doi = {10.1145/1536616.1536636},
	abstract = {Meaning can be as important as usability in the design of technology.},
	number = {8},
	journal = {Commun. {ACM}},
	author = {Kristen Shinohara and Josh Tenenberg},
	year = {2009},
	pages = {58--66}
},

@article{viola_robust_2004,
	title = {Robust {Real-Time} Face Detection},
	volume = {57},
	url = {http://portal.acm.org/citation.cfm?id=966458&dl=},
	abstract = {This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the {“Integral} Image” which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the {AdaBoost} learning algorithm {(Freund} and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a “cascade” which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems {(Sung} and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second.},
	number = {2},
	journal = {Int. J. Comput. Vision},
	author = {Paul Viola and Michael J. Jones},
	year = {2004},
	keywords = {boosting, face detection, human sensing},
	pages = {137--154}
},

@inproceedings{mcdaniel_using_2008,
	title = {Using a haptic belt to convey non-verbal communication cues during social interactions to individuals who are blind},
	doi = {10.1109/HAVE.2008.4685291},
	abstract = {Good social skills are important and provide for a healthy, successful life; however, individuals with visual impairments are at a disadvantage when interacting with sighted peers due to inaccessible non-verbal cues. This paper presents a haptic (vibrotactile) belt to assist individuals who are blind or visually impaired by communicating non-verbal cues during social interactions. We focus on non-verbal communication pertaining to the relative location of the communicators with respect to the user in terms of direction and distance. Results from two experiments show that the haptic belt is effective in using vibration location and duration to communicate the relative direction and distance, respectively, of an individual in the userpsilas visual field.},
	booktitle = {Haptic Audio visual Environments and Games, 2008. {HAVE} 2008. {IEEE} International Workshop on},
	author = {T. {McDaniel} and S. Krishna and V. Balasubramanian and D. Colbry and S. Panchanathan},
	year = {2008},
	keywords = {assistive technology, blind individual assistance, handicapped aids, haptic belt, Haptic Display, haptic interfaces, human computer interaction, Non-verbal Communication, nonverbal communication cue, {SOCIAL} interaction, Tactile Cues, Tactile Display, vibration location, Vibrotactile Cues, vision defects, visual impairment},
	pages = {13--18}
},

@inproceedings{krishna_haptic_2009,
	address = {Los Angeles, {CA}},
	title = {Haptic Belt for Delivering Nonverbal Communication Cues to People who are Blind or Visually Impaired},
	booktitle = {25th Annual International Technology \& Persons with Disabilities},
	author = {S. Krishna and T. {McDaniel} and S. Panchanathan},
	month = mar,
	year = {2009}
},

@inproceedings{edwards_pragmatic_2009,
	address = {Italy},
	title = {A Pragmatic Approach to the Design and Implementation of a Vibrotactile Belt and its Applications},
	author = {Nathan Edwards and Jacob Rosenthal and Daniel Molbery and Jon Lindsey and Kris Blair and T. {McDaniel} and S. Krishna and S. Panchanathan},
	year = {2009}
},

@article{jindal-snape_generalization_2004,
	title = {Generalization and Maintenance of Social Skills of Children with Visual Impairments: Self-evaluation and the Role of Feedback},
	volume = {98},
	journal = {Journal of Visual Impairment \& Blindness},
	author = {Divya {Jindal-Snape}},
	year = {2004},
	pages = {470--483}
},

@inproceedings{krishna_systematic_2008,
	address = {Marseille, France},
	title = {A Systematic Requirements Analysis and Development of an Assistive Device to Enhance the Social Interaction of People Who are Blind or Visually Impaired},
	booktitle = {Workshop on Computer Vision Applications for the Visually Impaired},
	author = {Sreekar Krishna and Dirk colbry and John Black and Vineeth Balasubramanian and S. Panchanathan},
	year = {2008}
},

@inproceedings{mcdaniel_using_2009,
	address = {Boston, {MA,} {USA}},
	title = {Using tactile rhythm to convey interpersonal distances to individuals who are blind},
	isbn = {978-1-60558-247-4},
	url = {http://portal.acm.org/citation.cfm?id=1520340.1520718},
	doi = {10.1145/1520340.1520718},
	abstract = {This paper presents a scheme for using tactile rhythms to convey interpersonal distance to individuals who are blind or visually impaired, with the goal of providing access to non-verbal cues during social interactions. A preliminary experiment revealed that subjects could identify the proposed tactile rhythms and found them intuitive for the given application. Future work aims to improve recognition results and increase the number of interpersonal distances conveyed by incorporating temporal change information into the proposed methodology.},
	booktitle = {Proceedings of the 27th international conference extended abstracts on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Troy L. {McDaniel} and Sreekar Krishna and Dirk Colbry and Sethuraman Panchanathan},
	year = {2009},
	keywords = {assistive technology, haptic belt, haptic icons, tactile icons, tactons, vibrotactile belt},
	pages = {4669--4674}
},

@inproceedings{shinohara_observing_2007,
	address = {Tempe, Arizona, {USA}},
	title = {Observing Sara: a case study of a blind person's interactions with technology},
	isbn = {978-1-59593-573-1},
	shorttitle = {Observing Sara},
	url = {http://portal.acm.org/citation.cfm?doid=1296843.1296873},
	doi = {10.1145/1296843.1296873},
	abstract = {While software is increasingly being improved to enhance access and use, software interfaces nonetheless often create barriers for people who are blind. In response, the blind computer user develops workarounds, strategies to overcome the constraints of a physical and social world engineered for the sighted. This paper describes an interview and observational study of a blind college student interacting with various technologies within her home. Structured around Blythe, Monk and Park's Technology Biographies, these experience centered sessions focus not only on technology function, but on the relationship of function to the meanings and values that this student attributes to technology use in different settings. Studying a single user across a range of devices and tasks provides a broader and more nuanced understanding of the contexts and causes of task failure and of the workarounds employed than is possible with a more narrowly focused usability study. Themes that were revealed across a range of tasks include the importance for technologies to not "mark" the user as being blind within a predominantly sighted social world, to support user independence through portability and user control, and to allow user "resets" and brute-force fallbacks in the face of persistent task failure.},
	booktitle = {Proceedings of the 9th international {ACM} {SIGACCESS} conference on Computers and accessibility},
	publisher = {{ACM}},
	author = {Kristen Shinohara and Josh Tenenberg},
	year = {2007},
	keywords = {assistive technology, technology biographies, user-centered inclusive design},
	pages = {171--178}
},

@book{knapp_nonverbal_1996,
	edition = {4th},
	title = {Nonverbal Communication in Human Interaction},
	isbn = {0030180236},
	publisher = {Harcourt College Pub},
	author = {Mark L. Knapp and Judith A. Hall},
	month = nov,
	year = {1996}
},

@article{ambady_thin_1992,
	title = {Thin Slices of Expressive behavior as Predictors of Interpersonal Consequences : a {Meta-Analysis}},
	volume = {111},
	shorttitle = {Thin Slices of Expressive behavior as Predictors of Interpersonal Consequences},
	url = {http://www.tufts.edu/~nambad01/Thin%20slices%20of%20expressive%20behavior%20as%20predictors%20of%20interpersonal%20consequences.pdf},
	abstract = {A meta-analysis was conducted on the accuracy of predictions of various objective outcomes in the areas of social and clinical psychology from short observations of expressive behavior (under 5 min). The overall effect size (r) for the accuracy of predictions for 38 different results was .39. Studies based on longer periods of behavioral observations did not yield greater predictive accuracy; predictions based on observations under 1/2 min in length did not differ significantly from predictions based on 4- and 5-min observations. The type of behavioral channel (such as the face, speech, the body, tone of voice) on which the ratings were based was not related to the accuracy of predictions. Accuracy did not vary significantly between behaviors manipulated in a laboratory and more naturally occurring behavior. Last, effect sizes did not differ significantly for predictions in the areas of clinical psychology, social psychology, and the accuracy of detecting deception.},
	number = {2},
	journal = {Psychological Bulletin},
	author = {Nalini Ambady and Robert Rosenthal},
	year = {1992},
	keywords = {nonverbal, thin-slices},
	pages = {274, 256}
},

@article{haans_mediated_2006,
	title = {Mediated social touch: a review of current research and future directions},
	volume = {9},
	shorttitle = {Mediated social touch},
	url = {http://portal.acm.org/citation.cfm?id=1124640},
	abstract = {In this paper, we review research and applications in the area of mediated or remote social touch. Whereas current communication media rely predominately on vision and hearing, mediated social touch allows people to touch each other over a distance by means of haptic feedback technology. Overall, the reviewed applications have interesting potential, such as the communication of simple ideas (e.g., through Hapticons), establishing a feeling of connectedness between distant lovers, or the recovery from stress. However, the beneficial effects of mediated social touch are usually only assumed and have not yet been submitted to empirical scrutiny. Based on social psychological literature on touch, communication, and the effects of media, we assess the current research and design efforts and propose future directions for the field of mediated social touch.},
	number = {2},
	journal = {Virtual Real.},
	author = {Antal Haans and Wijnand {IJsselsteijn}},
	year = {2006},
	keywords = {Computer mediated communication, haptic feedback, interpersonal interaction, literature review, physical contact, social touch},
	pages = {149--159}
},

@article{burgoon_relational_1984,
	title = {Relational Messages Associated with Nonverbal Behaviors},
	volume = {10},
	url = {http://dx.doi.org/10.1111/j.1468-2958.1984.tb00023.x},
	doi = {10.1111/j.1468-2958.1984.tb00023.x},
	abstract = {Based on the assumptions that relational messages are multidimensional and that they are frequently communicated by nonverbal cues, this experiment manipulated five nonverbal cues -eye contact, proximity, body lean, smiling, and touch - to determine what meanings they convey along four relational message dimensions. Subjects {(N=} 150) observed 2 out of 40 videotaped conversational segments in which a male-female dyad presented various combinations of the nonverbal cues. High eye contact, close proximity, forward body lean, and smiling all conveyed greater intimacy, attraction, and trust. Low eye contact, a distal position, backward body lean, and the absence of smiling and touch communicated greater detachment. High eye contact, close proximity, and smiling also communicated less emotional arousal and greater composure, while high eye contact and close proximity alone conveyed greater dominance and control. Effects of combinations of cues and sex-differences are also reported.},
	number = {3},
	journal = {Human Communication Research},
	author = {Judee Burgoon and David Buller and Jerold Hale and Mark Turck},
	year = {1984},
	pages = {351--378}
},

@article{wetzel_midas_1984,
	title = {The Midas Touch: The Effects of Interpersonal Touch on Restaurant Tipping},
	volume = {10},
	number = {4},
	journal = {Personality and Social Psychology Bulletin},
	author = {Christopher Wetzel},
	year = {1984},
	pages = {512--517}
},

@incollection{sameroff_reproductive_1975,
	address = {Chicago},
	title = {Reproductive risk and the continuum of caretaker casualty},
	volume = {4},
	booktitle = {Review of Child Development Research},
	publisher = {University of Chicago Press},
	author = {A. J. Sameroff and M. J. Chandler},
	editor = {F. D. Horowitz},
	year = {1975}
},

@incollection{altmann_analysis_2007,
	title = {Analysis of Nonverbal Involvement in Dyadic Interactions},
	url = {http://dx.doi.org/10.1007/978-3-540-76442-7_4},
	abstract = {In the following, we comment on the assignment of the dynamic variable, its meaning, indicators and furthermore its dimensions.
We examine some interaction models which incorporate nonverbal involvement as a dynamic variable. Then we give a short overview
of two recently undertaken studies in advance of dyadic interactions focusing on nonverbal involvement measured in a multivariate
manner. The first study concentrates on conflict regulation of interacting children being friends. The second study examines
intrapersonal conflict and its social expression through “emotional overinvolvement” {(EOI)} of patients in psychotherapy. We
also mention a pilot-study in which the proxemic behaviour between two children in a conflict episode is analysed focusing
here on violation of personal space and its restoration through synchronisation. We end with some comments on multiple dimensions
and scaling with respect to involvement including thoughts about multidimensional interaction data {(MID).}},
	booktitle = {Verbal and Nonverbal Communication Behaviours},
	author = {Uwe Altmann and Rico Hermkes and {Lutz-Michael} Alisch},
	year = {2007},
	pages = {37--50}
},

@inproceedings{zancanaro_automatic_2006,
	address = {Banff, Alberta, Canada},
	title = {Automatic detection of group functional roles in face to face interactions},
	isbn = {{1-59593-541-X}},
	url = {http://portal.acm.org/citation.cfm?id=1180995.1181003},
	doi = {10.1145/1180995.1181003},
	abstract = {In this paper, we discuss a machine learning approach to automatically detect functional roles played by participants in a face to face interaction. We shortly introduce the coding scheme we used to classify the roles of the group members and the corpus we collected to assess the coding scheme reliability as well as to train statistical systems for automatic recognition of roles. We then discuss a machine learning approach based on multi-class {SVM} to automatically detect such roles by employing simple features of the visual and acoustical scene. The effectiveness of the classification is better than the chosen baselines and although the results are not yet good enough for a real application, they demonstrate the feasibility of the task of detecting group functional roles in face to face interactions.},
	publisher = {{ACM}},
	author = {Massimo Zancanaro and Bruno Lepri and Fabio Pianesi},
	year = {2006},
	keywords = {group interaction, intelligent environments, support vector machines},
	pages = {28--34}
},

@inproceedings{dong_using_2007,
	address = {Nagoya, Aichi, Japan},
	title = {Using the influence model to recognize functional roles in meetings},
	isbn = {978-1-59593-817-6},
	url = {http://portal.acm.org/citation.cfm?id=1322239},
	doi = {10.1145/1322192.1322239},
	abstract = {In this paper, an influence model is used to recognize functional roles played during meetings. Previous works on the same corpus demonstrated a high recognition accuracy using {SVMs} with {RBF} kernels. In this paper, we discuss the problems of that approach, mainly over-fitting, the curse of dimensionality and the inability to generalize to different group configurations. We present results obtained with an influence modeling method that avoid these problems and ensures both greater robustness and generalization capability.},
	booktitle = {Proceedings of the 9th international conference on Multimodal interfaces},
	publisher = {{ACM}},
	author = {Wen Dong and Bruno Lepri and Alessandro Cappelletti and Alex Sandy Pentland and Fabio Pianesi and Massimo Zancanaro},
	year = {2007},
	keywords = {group interaction, intelligent environments, support vector machines},
	pages = {271--278}
},

@article{rogers_edward_2002,
	title = {Edward T. Hall and the History of Intercultural Communication: The United States and Japan},
	volume = {24},
	shorttitle = {Edward T. Hall and the History of Intercultural Communication},
	url = {http://www.mediacom.keio.ac.jp/publication/pdf2002/review24/2.pdf},
	abstract = {Here we trace the role of anthropologist Edward T. Hall in founding the scholarly field of intercultural communication during the 1951-1955 period when he was at the Foreign Service Institute of the {U.S.} Department of States. The scholarly field of intercultural communication was then mainly advanced by university-based scholars of communication in the United States and Japan, and in other countries. The development of intercultural communication in the {U.S.} and Japan is analyzed here.},
	journal = {Keio Communication Review},
	author = {Everett Rogers and William Hart and Yoshitaka Miike},
	year = {2002},
	keywords = {Communication, cross-cultural, Culture, history, intercultural-relations, intercultural-rhetoric, us},
	pages = {26, 3}
},

@book{hawkins_intelligence_2005,
	edition = {First Edition},
	title = {On Intelligence},
	isbn = {0805078533},
	publisher = {St. Martin's Griffin},
	author = {Jeff Hawkins and Sandra Blakeslee},
	month = aug,
	year = {2005}
},

@incollection{krishna_person-specific_2009,
	title = {{Person-Specific} Characteristic Feature Selection for Face Recognition},
	booktitle = {Biometrics: Theory, Methods, and Applications {(IEEE} Press Series on Computational Intelligence)},
	publisher = {{Wiley-IEEE} Press},
	author = {S. Krishna and V. Balasubramanian and {J.A.} Black and S. Panchanathan},
	editor = {N. V. Boulgoris and K. N. Plataniotis and E. {Micheli-Tzanakou}},
	year = {2009},
	pages = {113--142}
},

@techreport{solomon_challenges_2010,
	address = {New York, {NY}},
	title = {The Challenges of Working in Virtual Teams: Virtual Teams Survey Report 2010},
	institution = {{RW3} {CultureWizard}},
	author = {Charlene Solomon},
	year = {2010},
	pages = {31}
},

@article{hall_system_1963,
	series = {New Series},
	title = {A System for the Notation of Proxemic Behavior},
	volume = {65},
	issn = {00027294},
	url = {http://www.jstor.org/stable/668580},
	number = {5},
	journal = {American Anthropologist},
	author = {Edward T. Hall},
	month = oct,
	year = {1963},
	note = {{ArticleType:} primary\_article / Issue Title: Selected Papers in Method and Technique / Full publication date: Oct., 1963 / Copyright © 1963 American Anthropological Association},
	pages = {1003--1026}
},

@inproceedings{krishna_vibroglove:_2010,
	address = {Atlanta, Georgia, {USA}},
	title = {{VibroGlove:} an assistive technology aid for conveying facial expressions},
	isbn = {978-1-60558-930-5},
	shorttitle = {{VibroGlove}},
	url = {http://portal.acm.org/citation.cfm?id=1754031},
	doi = {10.1145/1753846.1754031},
	abstract = {In this paper, a novel interface is described for enhancing human-human interpersonal interactions. Specifically, the device is targeted as an assistive aid to deliver the facial expressions of an interaction partner to people who are blind or visually impaired. Vibro-tactors, mounted on the back of a glove, provide a means for conveying haptic emoticons that represent the six basic human emotions and the neutral expression of the user's interaction partner. The detailed design of the haptic interface and haptic icons of expressions are presented, along with a user study involving a subject who is blind, as well as sighted, blind-folded participants. Results reveal the potential for enriching social communication for people with visual disabilities.},
	booktitle = {Proceedings of the 28th of the international conference extended abstracts on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Sreekar Krishna and Shantanu Bala and Troy {McDaniel} and Stephen {McGuire} and Sethuraman Panchanathan},
	year = {2010},
	keywords = {basic facial expressions, bilateral interpersonal interaction, haptic interface, vibrotactile, vibrotactile glove},
	pages = {3637--3642}
},

@article{hammon_participation_2009,
	title = {Participation and communication in virtual teams using representational avatars},
	volume = {3},
	url = {http://dx.doi.org/10.1002/jls.20123},
	doi = {10.1002/jls.20123},
	abstract = {The study examined how quantitative formative assessment tools may be applied to virtual team communication and provide quality and progress metrics to virtual team leaders. Representational avatars benefit virtual team communication and productivity. Organizational leaders need measurement tools to assess avatar innovation and monitor progress in virtual teams. A goal of this research was to supply a suite of tools to evaluate the potential for avatars to improve as well as monitor and measure virtual team participation and communication. The virtual teams were composed of students {(N=} 97) at the Auckland University of Technology {(AUT)} in New Zealand and Uppsala University in Sweden. Students enrolled in the Intelligent Business Systems {(IBS)} course formed teams by country and were randomly assigned to a text-only or avatar-enriched communication format. Survey results indicated that text-based teams were perceived as efficient but lacking in inspired participation. Avatar-enriched teams noted frustration with software but rated their experience as enjoyable. No significant difference in communication quality between teams was indicated, but avatar-enriched teams posted significantly more words on weekly logbooks than text-based teams. Recommendations include a model for data disaggregation that may be of value to improve assessment, productivity, and administrative costs of technology-enhanced virtual teams.},
	number = {3},
	journal = {Journal of Leadership Studies},
	author = {V. Arthur Hammon and Cheryl {Winsten-Bartlett}},
	year = {2009},
	pages = {53--67}
},

@article{hoyt_social_2003,
	title = {Social Inhibition in Immersive Virtual Environments},
	volume = {12},
	url = {http://dx.doi.org/10.1162/105474603321640932},
	doi = {10.1162/105474603321640932},
	abstract = {We assessed the utility of using immersive virtual environment {(IVE)} technology for social psychological research by attempting to replicate two classic social influence effects. Specifically, we sought to replicate the classic social facilitation/inhibition effects wherein individuals' performance on a task is affected by the presence of others. Within an {IVE,} participants mastered one of two tasks and subsequently performed the mastered or nonmastered task either alone or in the presence of a virtual human audience whom they were led to believe were either computer-controlled agents or human-controlled avatars. Those performing in the presence of avatars demonstrated classic social inhibition performance impairment effects relative to those performing alone or in the presence of agents. We discuss important elements involved in the experience of social influence within immersive virtual environments.},
	number = {2},
	journal = {Presence: Teleoperators and Virtual Environments},
	author = {Crystal L. Hoyt and Jim Blascovich and Kimberly R. Swinth},
	month = apr,
	year = {2003},
	pages = {183--195}
},

@article{bailenson_virtual_2008,
	title = {Virtual interpersonal touch: Haptic interaction and copresence in collaborative virtual environments},
	volume = {37},
	shorttitle = {Virtual interpersonal touch},
	url = {http://dx.doi.org/10.1007/s11042-007-0171-2},
	doi = {10.1007/s11042-007-0171-2},
	abstract = {Abstract  As digital communication becomes more commonplace and sensory rich, understanding the manner in which people interact with
one another is crucial. In the current study, we examined the manners in which people touch digital representations of people,
and compared those behaviors to the manner in which they touch digital representations of nonhuman objects. Results demonstrated
that people used less force when touching people than other nonhuman objects, and that people touched the face with less force
than the torso area. Finally, male digital representations were touched with more force than female representations by subjects
of both genders. We discuss the implications of these data to the development of haptic communication systems as well as for
a methodology of measuring the amount of copresence in virtual environments.},
	number = {1},
	journal = {Multimedia Tools and Applications},
	author = {Jeremy Bailenson and Nick Yee},
	month = mar,
	year = {2008},
	pages = {5--14}
}


@inproceedings{teeters_self-cam:_2006,
	address = {Boston, Massachusetts},
	title = {{Self-Cam:} feedback from what would be your social partner},
	isbn = {1-59593-364-6},
	shorttitle = {{Self-Cam}},
	url = {http://portal.acm.org/citation.cfm?id=1179782},
	doi = {10.1145/1179622.1179782},
	abstract = {Note: {OCR} errors may be found in this Reference List extracted from the full text article. {ACM} has opted to expose the complete List rather than only correct and linked references.},
	booktitle = {{ACM} {SIGGRAPH} 2006 Research posters},
	publisher = {{ACM}},
	author = {Alea Teeters and Rana El Kaliouby and Rosalind Picard},
	year = {2006},
	pages = {138}
},

@inproceedings{kang_communicators?_2008,
	address = {Los Alamitos, {CA,} {USA}},
	title = {Communicators? Perceptions of Social Presence as a Function of Avatar Realism in Small Display Mobile Communication Devices},
	shorttitle = {Communicators?},
	doi = {http://doi.ieeecomputersociety.org/10.1109/HICSS.2008.95},
	abstract = {This study describes an experiment in which 126 participants engaged via a mobile telephone simulation that included a visual display in a discussion that required self-disclosure and affective evaluation of the other participant. Participants in same gender and mixed gender dyads were represented by avatars that varied in visual realism (unmodified video, modified video, graphic display, or no visual display) and behavioral realism (static visual display versus dynamic or animated). Participants subsequently rated the Perceived Social Richness of the Medium and the Interactant Satisfaction with the conversation. Interactant Satisfaction was a new measure of social presence created to tap emotional and affective evaluations. Participants rated devices with higher-realism and more behaviorally realistic avatars as being more capable of effective social interaction, but their actual perceptions of affective dimensions of their conversational partner were essentially unaffected by visual representations.},
	booktitle = {Hawaii International Conference on System Sciences},
	publisher = {{IEEE} Computer Society},
	author = {{Sin-Hwa} Kang and James H. Watt and Sasi Kanth Ala},
	year = {2008},
	pages = {147},
	annote = {Complete {PDF} document was either not available or accessible. Please make sure you're logged in to the digital library to retrieve the complete {PDF} document.}
},

@inproceedings{garau_impact_2003,
	address = {Ft. Lauderdale, Florida, {USA}},
	title = {The impact of avatar realism and eye gaze control on perceived quality of communication in a shared immersive virtual environment},
	isbn = {1-58113-630-7},
	url = {http://portal.acm.org/citation.cfm?id=642703},
	doi = {10.1145/642611.642703},
	abstract = {This paper presents an experiment designed to investigate the impact of scommunication in an immersive virtual {environment.Participants} were paired by gender and were randomly assigned to a {CAVE-like} system or a head-mounted display. Both were represented by a humanoid avatar in the shared {3D} environment. The visual appearance of the avatars was either basic and genderless (like a "match-stick" figure), or more photorealistic and gender-specific. Similarly, eye gaze behavior was either random or inferred from voice, to reflect different levels of behavioral {realism.Our} comparative analysis of 48 post-experiment questionnaires confirms earlier findings from non-immersive studies using semi-photorealistic avatars, where inferred gaze significantly outperformed random gaze. However responses to the lower-realism avatar are adversely affected by inferred gaze, revealing a significant interaction effect between appearance and behavior. We discuss the importance of aligning visual and behavioral realism for increased avatar effectiveness.},
	booktitle = {Proceedings of the {SIGCHI} conference on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Maia Garau and Mel Slater and Vinoba Vinayagamoorthy and Andrea Brogni and Anthony Steed and M. Angela Sasse},
	year = {2003},
	keywords = {avatars, behavioral realism, copresence, eye gaze, mediated communication, photo-realism, social presence},
	pages = {529--536}
},

@article{bailenson_effect_2006,
	title = {The Effect of Behavioral Realism and Form Realism of {Real-Time} Avatar Faces on Verbal Disclosure, Nonverbal Disclosure, Emotion Recognition, and Copresence in Dyadic Interaction},
	volume = {15},
	url = {http://dx.doi.org/10.1162/pres.15.4.359},
	doi = {10.1162/pres.15.4.359},
	abstract = {The realism of avatars in terms of behavior and form is critical to the development of collaborative virtual environments. In the study we utilized state of the art, real-time face tracking technology to track and render facial expressions unobtrusively in a desktop {CVE.} Participants in dyads interacted with each other via either a video-conference (high behavioral realism and high form realism), voice only (low behavioral realism and low form realism), or an “emotibox” that rendered the dimensions of facial expressions abstractly in terms of color, shape, and orientation on a rectangular polygon (high behavioral realism and low form realism). Verbal and non-verbal self-disclosure were lowest in the videoconference condition while self-reported copresence and success of transmission and identification of emotions were lowest in the emotibox condition. Previous work demonstrates that avatar realism increases copresence while decreasing self-disclosure. We discuss the possibility of a hybrid realism solution that maintains high copresence without lowering self-disclosure, and the benefits of such an avatar on applications such as distance learning and therapy.},
	number = {4},
	journal = {Presence: Teleoperators and Virtual Environments},
	author = {Jeremy N Bailenson and Nick Yee and Dan Merget and Ralph Schroeder},
	year = {2006},
	pages = {359--372}
},

@article{sidner_effect_2006,
	title = {The effect of head-nod recognition in human-robot conversation},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.64.195},
	journal = {In Proceedings of the 1st Annual Conference on {Human-Robot} Interaction},
	author = {Ace L Sidner and Christopher Lee and Clifton Forlines},
	year = {2006}
},

@article{nakano_towards_2003,
	title = {Towards a Model of {Face-to-Face} Grounding},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.5095},
	journal = {{IN} {PROCEEDINGS} {OF} {THE} {ANNUAL} {MEETING} {OF} {THE} {ASSOCIATION} {FOR} {COMPUTATIONAL} {LINGUISTICS}},
	author = {Yukiko Nakano and Gabe Reinstein and Tom Stocky and Justine Cassell},
	year = {2003},
	pages = {553---561}
},

@article{chen_guiding_????,
	title = {Guiding Exploration Through {Three-Dimensional} Virtual Environments: A Cognitive Load Reduction Approach},
	volume = {19},
	issn = {{1093-023X}},
	shorttitle = {Guiding Exploration Through {Three-Dimensional} Virtual Environments},
	url = {http://editlib.org/p/24213},
	number = {4},
	journal = {Journal of Interactive Learning Research},
	author = {Chwen Jen Chen and Wan Mohd Fauzy Wan Ismail},
	pages = {579--596}
},

@article{hoyt_social_2003,
	title = {Social Inhibition in Immersive Virtual Environments},
	volume = {12},
	url = {http://dx.doi.org/10.1162/105474603321640932},
	doi = {10.1162/105474603321640932},
	abstract = {We assessed the utility of using immersive virtual environment {(IVE)} technology for social psychological research by attempting to replicate two classic social influence effects. Specifically, we sought to replicate the classic social facilitation/inhibition effects wherein individuals' performance on a task is affected by the presence of others. Within an {IVE,} participants mastered one of two tasks and subsequently performed the mastered or nonmastered task either alone or in the presence of a virtual human audience whom they were led to believe were either computer-controlled agents or human-controlled avatars. Those performing in the presence of avatars demonstrated classic social inhibition performance impairment effects relative to those performing alone or in the presence of agents. We discuss important elements involved in the experience of social influence within immersive virtual environments.},
	number = {2},
	journal = {Presence: Teleoperators and Virtual Environments},
	author = {Crystal L. Hoyt and Jim Blascovich and Kimberly R. Swinth},
	month = apr,
	year = {2003},
	pages = {183--195}
},

@book{hawkins_intelligence_2004,
	edition = {Adapted},
	title = {On Intelligence},
	isbn = {0805074562},
	publisher = {Times Books},
	author = {Jeff Hawkins and Sandra Blakeslee},
	month = oct,
	year = {2004}
},