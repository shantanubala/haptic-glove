\contentsline {figure}{\numberline {1.1}{\ignorespaces Relative importance of a) verbal vs. non-verbal cues, b) four channels of non-verbal cues, and c) visual vs. audio encoding and decoding of bilateral human interpersonal communicative cues.}}{3}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Relative communicative information plotted against its leakiness. Speech forms the verbal channel. Face, body and voice form the non-verbal communication channels.}}{5}
\contentsline {figure}{\numberline {1.3}{\ignorespaces Social Situational Awareness.}}{6}
\contentsline {figure}{\numberline {1.4}{\ignorespaces Social learning systems with continuous learning feedback loop.}}{8}
\contentsline {figure}{\numberline {3.1}{\ignorespaces Histogram of Responses grouped by Questions}}{30}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Response Ratio}}{31}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Rank average of the 8 questions}}{32}
\contentsline {figure}{\numberline {4.1}{\ignorespaces Training and testing phases of a typical learning framework found in literature.}}{40}
\contentsline {figure}{\numberline {4.2}{\ignorespaces The proposed hardware for use in the detection of body rocking stereotypic behavior. The accelerometer, in comparison with a US quarter, is shown in the inset. The three axes marked in the image shows the orientation of the accelerometer as it is placed on the head.}}{42}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Data stream for the tri-axial accelerometer. The three streams correspond to the three axes. The figure shows non-rocking events followed by rocking and then followed by non-rocking.}}{43}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Packet length to recognition rate comparison under the classic AdaBoost framework.}}{55}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Packet length to recognition rate comparison under the Modest AdaBoost framework.}}{55}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Piecewise performance analysis of the classic AdaBoost classifier framework; (a) Recognition rates under use of individual feature sets; (b) The Receiver Operating Characteristics (ROC) under the use of individual feature sets; (c) Area under the curve (AUC) for each feature set as estimated from the ROC; (d) The number of simple classifiers used by the aggregated AdaBoost classifier. Each set and each feature representation in the classifier pool are separately marked. In all the graphs Set 1 through 5 are as explained by Tables 4.1\hbox {} and 4.2\hbox {}. Set 6 represents a set containing all 14 features from Tables 4.1\hbox {} and 4.2\hbox {}.}}{58}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Piecewise performance analysis of the classic AdaBoost framework; (a) Recognition rates under use of individual feature sets; (b) The Receiver Operating Characteristics (ROC) under the use of individual feature sets; (c) Area under the curve (AUC) for each feature set as estimated from the ROC; (d) The number of simple classifiers used by the aggregated AdaBoost classifier; Each set and each feature representation in the classifier pool are separately marked. In all the graphs Set 1 through 5 are as explained by Tables 4.1\hbox {} and 4.2\hbox {}. Set 6 represents a set containing all 14 features from Tables 4.1\hbox {} and 4.2\hbox {}.}}{60}
\contentsline {figure}{\numberline {5.1}{\ignorespaces (a) 3D representation of a Gaussian mask; $\sigma _x = 10$, $\sigma _y = 15$ and $\theta =0$ \newline (b)Image of the Gaussian mask $\sigma _x = 10$, $\sigma _y = 15$ and $\theta =0$}}{75}
\contentsline {figure}{\numberline {5.2}{\ignorespaces (a)3D representation of a Sinusoid $S_{\omega ,\theta }$ \newline (b)Image representation of the real part of the complex Sinusoid $\Re \left \{S_{\omega ,\theta }\right \}$\newline (c)Image representation of the imaginary part of complex Sinusoid $\Im \left \{S_{\omega ,\theta }\right \}$}}{76}
\contentsline {figure}{\numberline {5.3}{\ignorespaces (a)3D representation of a Gabor filter $\Psi _{\omega ,\theta }$ \newline (b)Image representation of the real part of Gabor filter $\Re \left \{\Psi _{\omega ,\theta }\right \}$ \newline (c)Image representation of the imaginary part of Gabor filter $\Im \left \{\Psi _{\omega ,\theta }\right \}$}}{77}
\contentsline {figure}{\numberline {5.4}{\ignorespaces A typical chromosome used in the proposed method.}}{81}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Stages in the creation of the first generation of parents}}{82}
\contentsline {figure}{\numberline {5.6}{\ignorespaces Deriving newer parents from the current generation}}{83}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Typical crossing of two parents to create an offspring}}{85}
\contentsline {figure}{\numberline {5.8}{\ignorespaces Mutation of a newly created offspring}}{86}
\contentsline {figure}{\numberline {5.9}{\ignorespaces The data capture setup for FacePix(30)}}{87}
\contentsline {figure}{\numberline {5.10}{\ignorespaces Sample face images with varying pose and illumination from the FacePix(30) database}}{88}
\contentsline {figure}{\numberline {5.11}{\ignorespaces Sample frontal images of one person from the FacePix(30) Database}}{89}
\contentsline {figure}{\numberline {5.12}{\ignorespaces A face image marked with 5 locations where unique Gabor features were extracted}}{90}
\contentsline {figure}{\numberline {5.13}{\ignorespaces Distance Measure $D$ for the fitness function}}{93}
\contentsline {figure}{\numberline {5.14}{\ignorespaces The recognition rate versus the number Gabor feature detectors}}{96}
\contentsline {figure}{\numberline {5.15}{\ignorespaces Recognition rate with varying $w_D$}}{97}
\contentsline {figure}{\numberline {5.16}{\ignorespaces 10 and 20 person-specific features extracted for a particular individual in the database}}{98}
\contentsline {figure}{\numberline {6.1}{\ignorespaces An example false face detection.}}{104}
\contentsline {figure}{\numberline {6.2}{\ignorespaces Block diagram.}}{106}
\contentsline {figure}{\numberline {6.3}{\ignorespaces Skin pixels in nRGB space.}}{108}
\contentsline {figure}{\numberline {6.4}{\ignorespaces Extra region for background modeling.}}{109}
\contentsline {figure}{\numberline {6.5}{\ignorespaces Example of {\em true} and {\em false} face detection.}}{110}
\contentsline {figure}{\numberline {6.6}{\ignorespaces Pre-processing.}}{112}
\contentsline {figure}{\numberline {6.7}{\ignorespaces Neighborhood System.}}{113}
\contentsline {figure}{\numberline {6.8}{\ignorespaces Frontal face Local Conditional Probability Density (LCPD) models.}}{115}
\contentsline {figure}{\numberline {6.9}{\ignorespaces Skin-region masks.}}{115}
\contentsline {figure}{\numberline {6.10}{\ignorespaces Soft threshold.}}{116}
\contentsline {figure}{\numberline {6.11}{\ignorespaces An example of combining evidence from two experts under Dempster-Shafer Theory.}}{118}
\contentsline {figure}{\numberline {6.12}{\ignorespaces Coarse pose estimation.}}{120}
\contentsline {figure}{\numberline {7.1}{\ignorespaces Person of interest at a short distance from camera}}{122}
\contentsline {figure}{\numberline {7.2}{\ignorespaces Person of interest at a large distance from camera}}{122}
\contentsline {figure}{\numberline {7.3}{\ignorespaces Simple Background}}{123}
\contentsline {figure}{\numberline {7.4}{\ignorespaces Complex Background}}{123}
\contentsline {figure}{\numberline {7.5}{\ignorespaces Rigid, Homogeneous Object}}{124}
\contentsline {figure}{\numberline {7.6}{\ignorespaces Non-Rigid, Deformable, Non-Homogeneous Object}}{124}
\contentsline {figure}{\numberline {7.7}{\ignorespaces Static Camera}}{125}
\contentsline {figure}{\numberline {7.8}{\ignorespaces Mobile Camera}}{125}
\contentsline {figure}{\numberline {7.9}{\ignorespaces Changing Illumination, Pose Change and Blur}}{126}
\contentsline {figure}{\numberline {7.10}{\ignorespaces SMSPF - Step 1}}{131}
\contentsline {figure}{\numberline {7.11}{\ignorespaces SMSPF - Step 2}}{132}
\contentsline {figure}{\numberline {7.12}{\ignorespaces Structured Search}}{133}
\contentsline {figure}{\numberline {7.13}{\ignorespaces Sliding window of the Structured Search (Green: Estimate; Red: Sliding window).}}{134}
\contentsline {figure}{\numberline {7.14}{\ignorespaces Structured Search Matching Technique}}{136}
\contentsline {figure}{\numberline {7.15}{\ignorespaces Incorporating Chamfer Matching into Structured Search}}{137}
\contentsline {figure}{\numberline {7.16}{\ignorespaces SMSPF Results}}{139}
\contentsline {figure}{\numberline {7.17}{\ignorespaces AO (Dotted Line: Color PF; Solid Line: SMSPF)}}{142}
\contentsline {figure}{\numberline {7.18}{\ignorespaces DC(Dotted Line: Color PF; Solid Line: SMSPF)}}{142}
\contentsline {figure}{\numberline {7.19}{\ignorespaces Evaluation Measure for DataSet 1}}{143}
\contentsline {figure}{\numberline {7.20}{\ignorespaces Evaluation Measure for DataSet 2}}{143}
\contentsline {figure}{\numberline {7.21}{\ignorespaces Evaluation Measure for DataSet 3}}{144}
\contentsline {figure}{\numberline {8.1}{\ignorespaces Main Controller implementation}}{156}
\contentsline {figure}{\numberline {8.2}{\ignorespaces (a) 25\%; (b) 75\% Pulse-width modulation; (c) and (d) Vibration motor magnitudes of 25\% and 75\% achieved using duty cycles with 25 pulses over a 50 ms vibration period.}}{158}
\contentsline {figure}{\numberline {8.3}{\ignorespaces Sample Temporal Rhythm Sequences (TRS) with different magnitudes of vibration encoded on the Temporal Rhythm Units (TRU) (a) 100\% Magnitude, (b) 50\% Magnitude.}}{159}
\contentsline {figure}{\numberline {8.4}{\ignorespaces Tactor Controller implementation}}{160}
\contentsline {figure}{\numberline {8.5}{\ignorespaces a) Graphical User Interface on a Portable Platform. b) Temporal Rhythm Sequence (TRS) Design Interface.}}{161}
\contentsline {figure}{\numberline {8.6}{\ignorespaces System Architecture for Haptic Belt used as part of the Social Interaction Assistant}}{162}
\contentsline {figure}{\numberline {8.7}{\ignorespaces Experiment 1 Results: Mean Localization Accuracy for each Tactor, Averaged across Subjects, with 95\% Confidence Intervals}}{164}
\contentsline {figure}{\numberline {8.8}{\ignorespaces Mean Classification Accuracy of Duration, Averaged across Subjects and Tactors, with 95\% Confidence Intervals. Durations listed in figure correspond to 200 ms (\#1), 400 ms (\#2), 600 ms (\#3), 800 ms (\#4) and 1000 ms (\#5)}}{167}
\contentsline {figure}{\numberline {8.9}{\ignorespaces The on/off timing values of the four tactile rhythm designs, and corresponding distances, used in the experiment.}}{169}
\contentsline {figure}{\numberline {8.10}{\ignorespaces Overall direction recognition accuracy of each tactor location with standard deviations.}}{171}
\contentsline {figure}{\numberline {8.11}{\ignorespaces Overall distance recognition accuracy of each rhythm type with standard deviations.}}{172}
\contentsline {figure}{\numberline {8.12}{\ignorespaces Arrangement of 8 Tactors around the Waist.}}{174}
\contentsline {figure}{\numberline {8.13}{\ignorespaces Modified Box Dance.}}{176}
\contentsline {figure}{\numberline {8.14}{\ignorespaces Modified Electric Slide Dance.}}{177}
\contentsline {figure}{\numberline {8.15}{\ignorespaces Usability Results.}}{181}
\contentsline {figure}{\numberline {8.16}{\ignorespaces Functionality and Performance Results.}}{182}
\contentsline {figure}{\numberline {8.17}{\ignorespaces Pattern Recognition Results for Dance Experiment.}}{183}
\contentsline {figure}{\numberline {8.18}{\ignorespaces Questionnaire Results from Dance Experiment for Experienced Dance Participants (a) and Inexperienced Dance Participants (b). Responses from Q6-Q8 are excluded.}}{185}
