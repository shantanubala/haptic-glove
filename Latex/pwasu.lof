\contentsline {figure}{\numberline {1.1}{\ignorespaces Relative importance of a) verbal vs. non-verbal cues, b) four channels of non-verbal cues, and c) visual vs. audio encoding and decoding of bilateral human interpersonal communicative cues.}}{3}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Relative communicative information plotted against its leakiness. Speech forms the verbal channel. Face, body and voice form the non-verbal communication channels.}}{4}
\contentsline {figure}{\numberline {1.3}{\ignorespaces Social situational awareness in human social communications}}{6}
\contentsline {figure}{\numberline {1.4}{\ignorespaces Social learning systems with continuous learning feedback loop.}}{9}
\contentsline {figure}{\numberline {2.1}{\ignorespaces TeamSTEPPS: Team Strategies and Tools to Enhance Performance and Patient Safety}}{22}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Multi goal model of self regulation for effective team regulation.}}{25}
\contentsline {figure}{\numberline {3.1}{\ignorespaces Histogram of Responses grouped by Questions}}{37}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Response Ratio}}{38}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Rank average of the 8 questions}}{39}
\contentsline {figure}{\numberline {4.1}{\ignorespaces Training and testing phases of a typical learning framework found in literature.}}{47}
\contentsline {figure}{\numberline {4.2}{\ignorespaces The proposed hardware for use in the detection of body rocking stereotypic behavior. The accelerometer, in comparison with a US quarter, is shown in the inset. The three axes marked in the image shows the orientation of the accelerometer as it is placed on the head.}}{49}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Data stream for the tri-axial accelerometer. The three streams correspond to the three axes. The figure shows non-rocking events followed by rocking and then followed by non-rocking.}}{50}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Packet length to recognition rate comparison under the classic AdaBoost framework.}}{62}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Packet length to recognition rate comparison under the Modest AdaBoost framework.}}{62}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Piecewise performance analysis of the classic AdaBoost classifier framework; (a) Recognition rates under use of individual feature sets; (b) The Receiver Operating Characteristics (ROC) under the use of individual feature sets; (c) Area under the curve (AUC) for each feature set as estimated from the ROC; (d) The number of simple classifiers used by the aggregated AdaBoost classifier. Each set and each feature representation in the classifier pool are separately marked. In all the graphs Set 1 through 5 are as explained by Tables 4.1\hbox {} and 4.2\hbox {}. Set 6 represents a set containing all 14 features from Tables 4.1\hbox {} and 4.2\hbox {}.}}{65}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Piecewise performance analysis of the Modest AdaBoost framework; (a) Recognition rates under use of individual feature sets; (b) The Receiver Operating Characteristics (ROC) under the use of individual feature sets; (c) Area under the curve (AUC) for each feature set as estimated from the ROC; (d) The number of simple classifiers used by the aggregated AdaBoost classifier; Each set and each feature representation in the classifier pool are separately marked. In all the graphs Set 1 through 5 are as explained by Tables 4.1\hbox {} and 4.2\hbox {}. Set 6 represents a set containing all 14 features from Tables 4.1\hbox {} and 4.2\hbox {}.}}{67}
\contentsline {figure}{\numberline {5.1}{\ignorespaces (a) 3D representation of a Gaussian mask; $\sigma _x = 10$, $\sigma _y = 15$ and $\theta =0$ \newline (b)Image of the Gaussian mask $\sigma _x = 10$, $\sigma _y = 15$ and $\theta =0$}}{82}
\contentsline {figure}{\numberline {5.2}{\ignorespaces (a)3D representation of a Sinusoid $S_{\omega ,\theta }$ \newline (b)Image representation of the real part of the complex Sinusoid $\Re \left \{S_{\omega ,\theta }\right \}$\newline (c)Image representation of the imaginary part of complex Sinusoid $\Im \left \{S_{\omega ,\theta }\right \}$}}{83}
\contentsline {figure}{\numberline {5.3}{\ignorespaces (a)3D representation of a Gabor filter $\Psi _{\omega ,\theta }$ \newline (b)Image representation of the real part of Gabor filter $\Re \left \{\Psi _{\omega ,\theta }\right \}$ \newline (c)Image representation of the imaginary part of Gabor filter $\Im \left \{\Psi _{\omega ,\theta }\right \}$}}{84}
\contentsline {figure}{\numberline {5.4}{\ignorespaces A typical chromosome used in the proposed method.}}{88}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Stages in the creation of the first generation of parents}}{89}
\contentsline {figure}{\numberline {5.6}{\ignorespaces Deriving newer parents from the current generation}}{90}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Typical crossing of two parents to create an offspring}}{92}
\contentsline {figure}{\numberline {5.8}{\ignorespaces Mutation of a newly created offspring}}{93}
\contentsline {figure}{\numberline {5.9}{\ignorespaces The data capture setup for FacePix(30)}}{94}
\contentsline {figure}{\numberline {5.10}{\ignorespaces Sample face images with varying pose and illumination from the FacePix(30) database}}{95}
\contentsline {figure}{\numberline {5.11}{\ignorespaces Sample frontal images of one person from the FacePix(30) Database}}{96}
\contentsline {figure}{\numberline {5.12}{\ignorespaces A face image marked with 5 locations where unique Gabor features were extracted}}{97}
\contentsline {figure}{\numberline {5.13}{\ignorespaces Distance Measure $D$ for the fitness function}}{100}
\contentsline {figure}{\numberline {5.14}{\ignorespaces The recognition rate versus the number Gabor feature detectors}}{103}
\contentsline {figure}{\numberline {5.15}{\ignorespaces Recognition rate with varying $w_D$}}{104}
\contentsline {figure}{\numberline {5.16}{\ignorespaces 10 and 20 person-specific features extracted for a particular individual in the database}}{105}
\contentsline {figure}{\numberline {6.1}{\ignorespaces An example false face detection.}}{111}
\contentsline {figure}{\numberline {6.2}{\ignorespaces Block diagram.}}{113}
\contentsline {figure}{\numberline {6.3}{\ignorespaces Skin pixels in nRGB space.}}{115}
\contentsline {figure}{\numberline {6.4}{\ignorespaces Extra region for background modeling.}}{116}
\contentsline {figure}{\numberline {6.5}{\ignorespaces Example of {\em true} and {\em false} face detection.}}{117}
\contentsline {figure}{\numberline {6.6}{\ignorespaces Pre-processing.}}{119}
\contentsline {figure}{\numberline {6.7}{\ignorespaces Neighborhood System.}}{120}
\contentsline {figure}{\numberline {6.8}{\ignorespaces Frontal face Local Conditional Probability Density (LCPD) models.}}{122}
\contentsline {figure}{\numberline {6.9}{\ignorespaces Skin-region masks.}}{122}
\contentsline {figure}{\numberline {6.10}{\ignorespaces Soft threshold.}}{123}
\contentsline {figure}{\numberline {6.11}{\ignorespaces An example of combining evidence from two experts under Dempster-Shafer Theory.}}{125}
\contentsline {figure}{\numberline {6.12}{\ignorespaces Coarse pose estimation.}}{127}
\contentsline {figure}{\numberline {7.1}{\ignorespaces Person of interest at a short distance from camera}}{129}
\contentsline {figure}{\numberline {7.2}{\ignorespaces Person of interest at a large distance from camera}}{129}
\contentsline {figure}{\numberline {7.3}{\ignorespaces Simple Background}}{130}
\contentsline {figure}{\numberline {7.4}{\ignorespaces Complex Background}}{130}
\contentsline {figure}{\numberline {7.5}{\ignorespaces Rigid, Homogeneous Object}}{131}
\contentsline {figure}{\numberline {7.6}{\ignorespaces Non-Rigid, Deformable, Non-Homogeneous Object}}{131}
\contentsline {figure}{\numberline {7.7}{\ignorespaces Static Camera}}{132}
\contentsline {figure}{\numberline {7.8}{\ignorespaces Mobile Camera}}{132}
\contentsline {figure}{\numberline {7.9}{\ignorespaces Changing Illumination, Pose Change and Blur}}{133}
\contentsline {figure}{\numberline {7.10}{\ignorespaces SMSPF - Step 1}}{138}
\contentsline {figure}{\numberline {7.11}{\ignorespaces SMSPF - Step 2}}{139}
\contentsline {figure}{\numberline {7.12}{\ignorespaces Structured Search}}{140}
\contentsline {figure}{\numberline {7.13}{\ignorespaces Sliding window of the Structured Search (Green: Estimate; Red: Sliding window).}}{141}
\contentsline {figure}{\numberline {7.14}{\ignorespaces Structured Search Matching Technique}}{143}
\contentsline {figure}{\numberline {7.15}{\ignorespaces Incorporating Chamfer Matching into Structured Search}}{144}
\contentsline {figure}{\numberline {7.16}{\ignorespaces SMSPF Results}}{146}
\contentsline {figure}{\numberline {7.17}{\ignorespaces AO (Dotted Line: Color PF; Solid Line: SMSPF)}}{148}
\contentsline {figure}{\numberline {7.18}{\ignorespaces DC(Dotted Line: Color PF; Solid Line: SMSPF)}}{149}
\contentsline {figure}{\numberline {7.19}{\ignorespaces Evaluation Measure for DataSet 1}}{150}
\contentsline {figure}{\numberline {7.20}{\ignorespaces Evaluation Measure for DataSet 2}}{150}
\contentsline {figure}{\numberline {7.21}{\ignorespaces Evaluation Measure for DataSet 3}}{151}
\contentsline {figure}{\numberline {8.1}{\ignorespaces Main Controller implementation}}{163}
\contentsline {figure}{\numberline {8.2}{\ignorespaces (a) 25\%; (b) 75\% Pulse-width modulation; (c) and (d) Vibration motor magnitudes of 25\% and 75\% achieved using duty cycles with 25 pulses over a 50 ms vibration period.}}{165}
\contentsline {figure}{\numberline {8.3}{\ignorespaces Sample Temporal Rhythm Sequences (TRS) with different magnitudes of vibration encoded on the Temporal Rhythm Units (TRU) (a) 100\% Magnitude, (b) 50\% Magnitude.}}{166}
\contentsline {figure}{\numberline {8.4}{\ignorespaces Tactor Controller implementation}}{167}
\contentsline {figure}{\numberline {8.5}{\ignorespaces a) Graphical User Interface on a Portable Platform. b) Temporal Rhythm Sequence (TRS) Design Interface.}}{168}
\contentsline {figure}{\numberline {8.6}{\ignorespaces System Architecture for Haptic Belt used as part of the Social Interaction Assistant}}{169}
\contentsline {figure}{\numberline {8.7}{\ignorespaces Experiment 1 Results: Mean Localization Accuracy for each Tactor, Averaged across Subjects, with 95\% Confidence Intervals}}{171}
\contentsline {figure}{\numberline {8.8}{\ignorespaces Mean Classification Accuracy of Duration, Averaged across Subjects and Tactors, with 95\% Confidence Intervals. Durations listed in figure correspond to 200 ms (\#1), 400 ms (\#2), 600 ms (\#3), 800 ms (\#4) and 1000 ms (\#5)}}{174}
\contentsline {figure}{\numberline {8.9}{\ignorespaces The on/off timing values of the four tactile rhythm designs, and corresponding distances, used in the experiment.}}{176}
\contentsline {figure}{\numberline {8.10}{\ignorespaces Overall direction recognition accuracy of each tactor location with standard deviations.}}{178}
\contentsline {figure}{\numberline {8.11}{\ignorespaces Overall distance recognition accuracy of each rhythm type with standard deviations.}}{179}
\contentsline {figure}{\numberline {8.12}{\ignorespaces Arrangement of 8 Tactors around the Waist.}}{181}
\contentsline {figure}{\numberline {8.13}{\ignorespaces Modified Box Dance.}}{183}
\contentsline {figure}{\numberline {8.14}{\ignorespaces Modified Electric Slide Dance.}}{184}
\contentsline {figure}{\numberline {8.15}{\ignorespaces Usability Results.}}{188}
\contentsline {figure}{\numberline {8.16}{\ignorespaces Functionality and Performance Results.}}{189}
\contentsline {figure}{\numberline {8.17}{\ignorespaces Pattern Recognition Results for Dance Experiment.}}{190}
\contentsline {figure}{\numberline {8.18}{\ignorespaces Questionnaire Results from Dance Experiment for Experienced Dance Participants (a) and Inexperienced Dance Participants (b). Responses from Q6-Q8 are excluded.}}{192}
\contentsline {figure}{\numberline {8.19}{\ignorespaces (a) Typical use of the social interaction assistant, a third person perspective on the use case scenario, (b) An example of face detection being translated to vibrations on the haptic belt.}}{193}
\contentsline {figure}{\numberline {8.20}{\ignorespaces Application of haptic belt as a navigational aid.}}{194}
\contentsline {figure}{\numberline {9.1}{\ignorespaces Typical use of the dyadic interaction assistance scenario, a third person perspective on the use case scenario.}}{198}
\contentsline {figure}{\numberline {9.2}{\ignorespaces Typical use of the dyadic interaction assistance scenario, a third person perspective on the use case scenario.}}{199}
\contentsline {figure}{\numberline {9.3}{\ignorespaces Example Bayesian Network.}}{204}
\contentsline {figure}{\numberline {9.4}{\ignorespaces Temporal Exemplar-based Bayesian Network for facial expression recognition.}}{206}
\contentsline {figure}{\numberline {9.5}{\ignorespaces 36 Facial fiducial points tracked with FaceAPI software. Both $x$ and $y$ coordinates from all 36 points are used for facial expression recognition.}}{206}
\contentsline {figure}{\numberline {9.6}{\ignorespaces Deriving the exemplar layer of the TEBN based on every test point $X_t(t)$.}}{208}
\contentsline {figure}{\numberline {9.7}{\ignorespaces Comparison of Fear and Surprise facial expressions.}}{211}
\contentsline {figure}{\numberline {10.1}{\ignorespaces The Vibrotactile Glove.}}{226}
\contentsline {figure}{\numberline {10.2}{\ignorespaces Localization and spatio-temporal cueing software used for the vibrotactile glove.}}{227}
\contentsline {figure}{\numberline {10.3}{\ignorespaces Phalange naming convention and grouping based on the anatomical distances.}}{228}
\contentsline {figure}{\numberline {10.4}{\ignorespaces Mapping of Group 1 and Group 2 haptic expression icons to the central three fingers (9 Phalanges) of the vibrotactile glove. In the expression mapping chart, Columns 1 to 3 represent the expression. Column 4 shows the spatial mapping of vibrations. Column 5 shows the temporal mapping of the vibrations.}}{230}
\contentsline {figure}{\numberline {10.5}{\ignorespaces (a) Recognition Accuracies; (b) ANOVA; (c) HSD}}{236}
\contentsline {figure}{\numberline {10.6}{\ignorespaces (a) Recognition Accuracies; (b) ANOVA; (c) HSD}}{237}
\contentsline {figure}{\numberline {10.7}{\ignorespaces (a) Recognition Accuracies; (b) ANOVA; (c) HSD}}{238}
\contentsline {figure}{\numberline {10.8}{\ignorespaces (a) Recognition Accuracies; (b) ANOVA; (c) HSD}}{239}
\contentsline {figure}{\numberline {10.9}{\ignorespaces (a) Recognition Accuracies; (b) ANOVA; (c) HSD}}{240}
\contentsline {figure}{\numberline {10.10}{\ignorespaces Confusion Matrix across the 12 participants. The rows are the stimulation and the columns are the responses of the participants. Each row adds to 100\% (rounding error of 1\%).}}{242}
\contentsline {figure}{\numberline {10.11}{\ignorespaces Average recognition rate and response time for the subject who is blind, for over 70 trails.}}{243}
\contentsline {figure}{\numberline {10.12}{\ignorespaces Average response time for all 12 participants. Four important results are shown above, 1) Avg. correct response time per expression (Cyan), 2) Avg. incorrect response time per expression (Red), 3) Avg. correct response time for Group 1 (Blue), and 4) Avg. correct response time for Group 2 (Magenta).}}{244}
\contentsline {figure}{\numberline {11.1}{\ignorespaces Depression and Loneliness of students plotted against stress levels in high, medium and low social skilled undergraduate students. (Please see text for the scales used for the measurement.)}}{247}
\contentsline {figure}{\numberline {11.2}{\ignorespaces Comparison of annual wage of the visually impaired, physically disabled and non-disabled population. (a) Compared by age group. (b) Compared by education level.}}{253}
\contentsline {figure}{\numberline {B.1}{\ignorespaces Find x and y to maximize $f(x,y)$ subject to a constraint $g(x,y) = c$}}{288}
