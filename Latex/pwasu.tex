

%Peter W.
%Requires the memoir class (as of this date v1.6180339e 2009/02/17)
%I suggest
\documentclass[oneside,11pt]{memoir}
%%% with the wide textblock, 12pt is too small for reading ease, so best not
%%% to use 11pt or 10pt.

%%% Arial
%\usepackage[T1]{fontenc}
%\usepackage[scaled]{uarial}
%\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif

%%% Garamond
%\usepackage[T1]{fontenc}
%\usepackage{lmodern}
%\usepackage{garamond}

%%% MS San Serif
%\usepackage[T1]{fontenc}
%\usepackage[scaled]{helvet}
%\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif

%%% Times
\usepackage{mathptmx}  % Times New Roman, but if you have Garamond then use it;
                       % you are writing a book, not a newspaper column
\DoubleSpacing         % memoir's double spacing
\usepackage{pwasu}     % this package

%%%%%%Added by Craig Picone to meet ASU's margin requirements
\usepackage{graphicx}    % needed for including graphics e.g. EPS, PS
\topmargin -0.2in        % read Lamport p.163
\oddsidemargin 0.5in   % read Lamport p.163
\evensidemargin 0in  % same as oddsidemargin but for left-hand pages
\textwidth 5.5in
\textheight 8.83in
%\pagestyle{empty}       % Uncomment if don't want page numbers
\parskip 7.2pt           % sets spacing between paragraphs
%\renewcommand{\baselinestretch}{1.5} % Uncomment for 1.5 spacing between lines
\parindent 0pt          % sets leading space for paragraphs
 %%%%%%%%

%    The general sequence in your document, after you have set the data for
%the TITLE and APPROVAL pages, and any other specifics in the preamble is:
\DoubleSpacing
\begin{document}
\maxtocdepth{subparagraph} % put everything into the ToC
\pagestyle{plain}  % pagestyle for the prelims
\frontmatter
\thetitlepage
%%\approvalpage

%% Added by bbailey1
% Macro for List of Symbols
\def\listofsymbols{\input{memoir/symbols} \clearpage}
\def\addsymbol #1: #2#3{$#1$ \> \parbox{5.45in}{#2 \dotfill \pageref{#3}}\\}
\def\newnot#1{\label{#1}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% here is the main part of your dessertation

% put your abstract here

\asuabstract
\setlength{\parindent}{.5in}
This is a sample abstract

% your acknowledgement

\setdedication{ Your dedication goes here. } % if you want a dedication

\asudedication

\asuacknowledgements
[Enter your text here]

\tableofcontents
\listoftables   % if you have any tables

\listoffigures  % if you have any figures

%% Added by bbailey1
%% Uncomment the next 3 lines for List of Symbols
% \newpage
% \chapter*{List of Symbols\hfill} \addcontentsline{toc}{chapter}{LIST OF SYMBOLS}
% \listofsymbols

%%
% Mark your variables in your source code with \newnot{YOUR_SYMBOL_LABEL}.
% Example:
% ...Here, if the dimensions of A \newnot{sybmol:A}, B \newnot{symbol:B}, and C \newnot{symbol:C} are
% nxn, nxm and lxn \newnot{symbol:nml} respectfully; then ...
%%

%\newpage
%\chapter*{PREFACE\hfill} \addcontentsline{toc}{chapter}{PREFACE}
%[Enter your text here]
%\clearpage

%% if you have more prelim sections, then
%%%\clearpage
%%%%%\pagestyle{plain}
%%%%%\prelimtitle   text % for sections after the ToC, etc, before main text
\mainmatter
\pagestyle{asu}

\addcontentsline{toc}{chapter}{CHAPTER}

\pagestyle{plain}
% finally, start of your main text

\chapter{INTRODUCTION}

\DoubleSpacing
\setlength{\parindent}{.5in}
Human interpersonal interactions are socially driven exchanges of verbal and non-verbal communicative cues. The essence of humans as social animals is very well exemplified in the way humans interact face-to-face with one another. Even in a brief exchange of eye gaze, humans communicate a lot of information about themselves, while assessing a lot about others around them. Though not much is spoken, plenty is always said. We still do not understand the nature of human communication and why face-to-face interactions are so significant for us.

Social interaction refers to any form of mutual communication between two individuals or between an individual and a group \cite{riggio_assessment_1986}. Such communications involve any or all forms of sensory and motor activities as deemed necessary by the participants of the interaction. Social, Behavioral and Developmental Sociologists emphasize that the ability of individuals to effectively control expressive behavior is essential for the social and interpersonal functioning of our society. Such social interactions are the aggregate cause of social behaviors, social actions and social contact that helps not only in effective bilateral communication, but also in forming an efficient feedback driven behavioral learning loop. It is this feedback (termed as social feedback) that children use towards developing good social and communicative skills.

Recent studies in behavioral psychology are furthering our understanding of the importance of social behaviors and social actions in everyday context. Researchers have revealed an unconscious need in humans to mimic and imitate the mannerisms of their interaction partners. An increasing number of experiments have highlighted this need for imitation to be very primeval and that they offer an elegant channel for building trust and confidence between individuals.

\section{Components of Social Interactions}
From a neurological perspective, social interactions result from the complex interplay of cognition, action and perception tasks within the human brain. For example, the simple act of shaking hands involves interactions of sensory, motor and cognitive events. Two individuals who engage in the act of shaking hands have to first make eye contact, exchange emotional desire to interact (this usually happens through a complex set of face and body gestures, such as smile and increased upper body movements), determine the exact distance between themselves, move appropriately towards each other maintaining Proxemics (interpersonal distance) that are befitting of their cultural setting, engage in shaking hands, and finally, move apart assuming a conversational distance which is invariably wider than the hand shake distance. Verbal exchanges may occur before, during or after the hand shake itself. This example shows the need for sensory (visual senses of face and bodily actions, auditory verbal exchange etc.), perceptual (understanding expressions, distance between individuals etc.), and cognitive (recognizing the desire to interact, engaging in verbal communication etc.) exchange during social interactions. Further, though social interactions display such complex interplay, they have been studied in the human communication literature under two important categories \cite{brent_d._ruben_human_1975}, namely,

\begin{itemize}
\item \emph {Verbal communication}: Explicit communication through the use of words in the form of speech or transcript.
\item \emph {Non-verbal communication}: Implicit communication cues that use prosody, body kinesis, facial movements and spatial location to communicate information that may be unique or overlapping with verbal information.
\end{itemize}

While the spoken language plays an important role in communication, speech accounts for only 35\% of the interpersonal exchanges. Nearly 65\% of all information communication happens through non-verbal cues \cite{knapp_nonverbal_1996}. Out of this large chunk, 48\% of the communication, is through visual encoding of face and body kinesis and posture, while the rest is encoded in the prosody (intonation, pitch, pace and loudness of voice) \cite{borkenau_thin_2004}. A closer look at the various non-verbal communication modes can highlight the importance of the multi-modality of social exchanges (See Figure \ref{Fig:Figure1}).

\subsection{Non-verbal communication cues}
Speech, voice, face and body form the primary channels of communication in any social interaction. Speech forms the primary channel for verbal communication, while prosody (intonation, pace and loudness of one's voice), face, and body (posture, gesture and mannerisms) form the medium for nonverbal communication. In everyday social interactions, people communicate so effortlessly through both verbal and non-verbal cues that they are not cognizant of the complex interplay of their voice, face and body in establishing a smooth communication channel.

\begin{figure}[h]
\begin{center}
 \includegraphics[width=3in]{NVCEncodings.jpg}
\end{center}
\caption{Relative importance of a) verbal vs. non-verbal cues, b) four channels of non-verbal cues, and c) visual vs. audio encoding and decoding of bilateral human interpersonal communicative cues.}
\label{Fig:Figure1}
\end{figure}

\subsubsection{Social Sight and Social Hearing}
Unlike speech, which is mostly under the conscious control of the user, the non-verbal communication channels are engaged from a subconscious level. Though people can increase their control on these channels through training, innately, individuals demonstrate certain inability to control their non-verbal cues. This inability to control non-verbal channels is referred to as the leakiness \cite{brown_social_1986} and humans (evolutionarily) have learnt to pick up these leaked signals during social interactions. For example, people can read very subtle body mannerisms very easily to determine the mental state of their interaction partner. Eye Gaze is a classic example of such subtle cues where interaction partners can detect interest, focus, involvement and role play, to name a few.  On this leakiness scale, it has been found that the voice is the leakiest of all channels, implying that emotions of individuals are revealed first in their voice before any of the other channels are engaged. The voice is followed by body, face and finally the verbal channel, speech. The leakiness is plotted on the abscissa of Figure \ref{Fig:Figure2} with the ordinate showing the amount of information encoded in the other three non-verbal communication channels. It can be seen that the face communicates the most amount of non-verbal cues, while the prosody (voice) is the first channel to leak emotional information.

\begin{figure}[h]
\begin{center}
 \includegraphics[width=4in]{Leakiness.jpg}
\caption{Relative communicative information plotted against its leakiness. Speech forms the verbal channel. Face, body and voice form the non-verbal communication channels.}
\label{Fig:Figure2}
\end{center}
\end{figure}

\subsubsection{Social Touch}
Apart from visual and auditory channels of social stimulation, humans increasingly rely on social touch during interpersonal interactions. For example, hand shake represents an important aspect of social communication conveying confidence, trust, dominance and other important personal and professional skills \cite{burgoon_relational_1984}. Social touch has also been studied by psychologists in the context of emotional gratification. Wetzel \cite{wetzel_midas_1984} demonstrated patron gratification effects through tipping behavior when waitresses touched their patrons. Similar studies have revealed the importance of social touch and how conscious decision making is connected deeply with the human affect system. In the recent years social touch has gained a lot of interest in the area enriching remote interactions \cite{haans_mediated_2006} \cite{bailenson_virtual_2008} to help better understand an individual's  social awareness and social presence. In the next section, we describe the term \emph{Social Situational Awareness} as seen pertinent to this report and emphasize the importance of any individual being aware of his/her social situational awareness.

\section{Social Situational Awareness}
We refer to the term Social Situational Awareness (SSA) as the ability of individuals to receive the visual, auditory and touch based non-verbal cues and respond appropriately through their voice, face and/or body (touch and gestures). Figure \ref{Fig:Figure3} represents the concept of consuming social cues and reacting accordingly to the needs of social interaction. Social cognition bridges stimulation and reciprocation and allows individuals to interpret and react to the non-verbal cues.

\begin{figure}[h]
\begin{center}
 \includegraphics[width=4.5in]{SSA.jpg}
 \includegraphics[width=4.5in]{SSA2.jpg}
\caption{Social Situational Awareness.}
\label{Fig:Figure3}
\end{center}
\end{figure}

The Transactional Communication Model \cite{sameroff_reproductive_1975} suggests that during any face-to-face interaction, the interpretation of the social stimulation and the corresponding social response are under the control of various factors including the culture, physical and emotional state, experience, memory, expectation, self concept and attitude of the individuals involved in the interaction. In order to effectively cognize and react to the social stimulation, it is necessary that individuals be able to receive and synthesize these above factors. Enriching social situational awareness then represents the ability of a mediator (telecommunication technology for remote interactions; social assistive technologies for the disabled population) to allow the social cognition of an individual to have access to the above mentioned factors and thereby evoking appropriate social reciprocation.

\subsection{Social Situational Awareness in Everyday Social Interactions}
\subsubsection{SSA in Dyadic Interactions}
Human communication theories have studied dyadic or bilateral interaction between individuals as the basis of most communication models. Theories of leadership, conflict and trust base their findings on dyadic interaction primitives where the importance of the various non-verbal cues is heightened due to the one-on-one nature of dyadic interactions. Eye contact, head gestures (nod and shake), body posture (conveying dominance or submissiveness), social touch (hand shake, shoulder pat, hug, etc.), facial expressions and mannerisms (smile, surprise, inquiry, etc.), eye gestures (threatened gaze, inquisitive gaze, etc.) are some of the parameters that are studied closely in dyadic understanding of human bilateral communication \cite{altmann_analysis_2007}. Enriching SSA in dyadic communication thus focuses on appropriate extraction and delivery of communicator's face, body and voice based behaviors to a remote participant or to a person who is disabled.

\subsubsection{SSA in Group Interactions}
Group dynamics refer to the interactions between members of a team assembled together for a common purpose. For example, teams of medical professionals operating on a patient, a professional team meeting for achieving a certain goal, a congressional meeting on regulations, etc. represent groups of individuals with a shared mental model of what needs to be accomplished. Within such groups, communication behaviors play a vital role in determining the dynamics and outcome of the meeting. Zancanaro et. al. \cite{zancanaro_automatic_2006} and Dong et. al.  \cite{dong_using_2007} presented one model of identifying role-play of participants in a group discussion. They identified two distinct categories of roles for the individuals within the group, namely, the socio-emotion roles and the task roles. The socio-emotional roles included the protagonist, attacker, supporter and neutral, and the task roles included the orienteer, seeker, follower and giver. These roles were dependent heavily on the emotional state (affect) of the individuals participating in the group interaction. Good teams are those where individual team members and their leaders are able to compose and coordinate their affect towards a smooth and conflict free group interaction. And effective leaders are those who can read the affect of their group member, make decisions on individual's roles and steer the group towards effective and successful decisions. Inability to access the affective cues of team members has significant consequences to team leaders leading to unresolved conflict situations and underproductive meetings, or in the worst case, the death of a patient. Thus, enriching SSA in group settings correspond to the extraction and delivery of team's interaction dynamics (which are in turn modulated in their mutual and group affect) to a remotely located team member or to a co-located individual who is disabled.

In essence, SSA enrichment technologies provide for a richer interaction experience for individuals involved either in a dyadic or group interaction. It is well established that in teams comprising of good communication strategies a shared mental model towards effective decision is achieved faster with little or no emotional stress on the team members. The lack of social awareness can lead to interactions where individuals are not committed cognitively and find it very difficult to focus their attention on the communication. This is true in the case of remote interactions, disability and situations where doctors, nurses and other medical professionals are operating simultaneously on a patient.

\subsection{Learning Social Awareness}
Figure \ref{Fig:Figure3} represents a simple unidirectional model of social stimulation and reciprocation. In reality, social awareness is a continuous feedback learning system where individuals are learning through observing, predicting, enacting and correcting themselves. It is this learning mechanism that allows people to adapt easily from one culture to another with ease - here we refer to term culture in very broadly encompassing work culture, social culture in a new environment and culture of a new team, etc. Figure \ref{Fig:Figure4} shows the continuous feedback loop involved in social learning systems, based on the model of human cognition as proposed by Hawkins \cite{hawkins_intelligence_2004}.

\begin{figure}[h]
\begin{center}
 \includegraphics[width=4.5in]{SSALearning.jpg}
\caption{Social learning systems with continuous learning feedback loop.}
\label{Fig:Figure4}
\end{center}
\end{figure}

People exposed to everyday social interactions learn social skills from the three different social stimulations (social sight, social hearing and social touch) effortlessly. When faced with a new environment, individuals exercise their learned social skills to predict what social actions are appropriate in the setting. Once executed, they observe and assess their counterparts to determine if their new behavior is appropriate or not for the new setting. Such learning continues until their social rule set adapts to the new environment. Psychologists have been studying the nature of learning that happens in individuals who move from Western to Eastern cultures and vice versa. Largely, USA and Japan have been the countries of choice based on their economic equality and cultural diversity \cite{rogers_edward_2002}. In the West, large body movements and excitement in the voice are considered to be typical and to a large part encouraged as a good social skill. Similar attitudes in the East are considered to be inappropriate in professional settings and to a large extent considered indecent. An individual displaying any such inappropriate mannerisms or gestures will receive social feedback from his counterparts (everyone staring at the individual, reduced interaction with the individual, etc.).  Thus, social awareness is a learned set of rules about the environment within which the individual is present and this requires continuous monitoring of the various social channels of stimulation. Deprivation of any one of these channels can in turn affect the ability of the individual to learn social actions and responses that are pertinent to a social situation. Thus, enriching SSA not only offers the means for individuals to make appropriate social decisions, but also cognitively trains them towards effective social judgments.

-------------------------------------
In this paper, we advocate that the social separation induced by remote interactions in physically separated partners is similar to the social separation resulting from information impoverishment induced by sensory/physical disabilities in co-located interaction partners and propose technologies targeted at enriching social interactions.
--------------------------------------

\section{Components of Non-verbal Communication}
Non-verbal communications are inherently complex in nature. In order to understand the nature of these cues, psychologists have been studying these cues under three subdivisions based on what affects individualâ€™s non-verbal cueing \cite{knapp_nonverbal_1996}. These subdivisions include,
\begin{enumerate}[(a)]
\item The communication environment
\item  The physical characteristics of the communicators
\item The behaviors of the communicators
\end{enumerate}
Below, these three items are discussed in detail providing a highlevel discussion on the nature of their influence on the non-verbal communication between individuals.

\subsection{The Communication Environment}
The communication environment or surroundings where the interactions are taking place make a huge difference of how humans respond or react \cite{hargie_social_1994} \cite{walsh_person-environment_2000}. For example, lengthy periods of extreme heat \cite{kenrick_ambient_1986} are known to increase discomfort, irritability, reduced work output and unfavorable evaluations of other. Along with the interaction partners, the environment either reinforces or depreciates the emotional experience of an individual. For example, wide open spaces and natural environments are known to be conducive for psychological stability \cite{krupat_people_1985}. Though the environmental factors just perceptual, they impose a lot of control on how humans react towards them. Some of the important environmental factors that affect interpersonal communication and non-verbal cueing are shown in the Table \ref{Tab:Tabel1}. **These are some of the well identified factors towards which psychologists and sociologists are working towards.**

\begin{table}[hpdf]
\begin{center}
\caption{The various factors of the communicator's environment that can affect interpersonal communication.}
\label{Tab:Tabel1}
\begin{tabular}{|l|l|}
\hline
\multicolumn{2}{|c|}{The Communication Environment} \\
\hline
Familiarity of the environment & \cite{sommer_personal_1969} \cite{sommer_tight_1974} \\
Colors in the environment & \cite{schauss_psysiological_1985} \cite{bottomley_interactive_2006} \\
Other people in the environment	& See next two subsections. \\
Architectural Designs & \cite{farrenkopf_university_1980} \\
Objects in the environment & \cite{moos_human_1985} \\
Sounds  & \cite{manusov_attribution_2001} \cite{north_-store_1997} \\
Lighting & \cite{meer_light_1985} \\
Temperature & \cite{kenrick_ambient_1986} \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{The Physical Characteristics of the communicators}
The physical appearance of a person is very important aspect of non-verbal cueing. People draw impressions of their communication partner as soon as they see them. The human body acts like means for communicating important sociological parameters like status, interest, dominance etc. Researchers have found cultural and global preferences in overall body image and any deviations from the norm affects interactions between people. For example, facial babyishness \cite{berry_attractive_1991} has been found affect judgment of facial attractiveness, honesty, warmth and sincerity. Any deviation from the babyishness has been correlated to immediate reduction in the judgment of these traits. A similar such example is the clothing that people wear. It has been found that first impressions are positive if the interviewer and interviewee are clothed similarly \cite{johnson_clothing_1977}. Table \ref{Tab:Table2} shows the important aspects of a person's physical appearance that affects the interpersonal interaction. Various psychological studies have been conducted towards understanding the model of human perception of character. Very little is known on the reasons for some of the human norms, but it is an active area of research that is being explored rigorously, especially, in the context of group behaviors and personal mannerisms with work environments \cite{helen_h._jennings_sociometry_1959}.

\begin{table}[hpdf]
\begin{center}
\caption{The physical characteristics of a communicator that can affect interpersonal communications.}
\label{Tab:Table2}
\begin{tabular}{||l||l||}
\hline
\hline
\multicolumn{2}{||c||}{The Physical Characteristics} \\
\hline
\hline
The human facial attractiveness	& \cite{berry_attractive_1991} \cite{zebrowitz_reading_1997} \cite{berry_perceiving_1986}\\
\hline
Body shape & \cite{cortes_physique_1965} \cite{tucker_physical_1984}\\
\hline
Height of a person & \cite{cameron_courtship_1977}\\
\hline
Self image & \cite{ogden_prevalence_2002}\\
\hline
Body color & \cite{griffin_black_1996}\\
\hline
Body smell & \cite{porter_olfaction_1998} \cite{lord_identification_1989} \cite{russell_human_1976}\\
\hline
Body hair & \cite{barber_mustache_2001}\\
\hline
Clothing & \cite{johnson_clothing_1977} \cite{hensley_effects_1981}\\
\hline
Personality	& \cite{joseph_uniforms_1986} \cite{rosenfeld_clothing_1977}\\
\hline
Body decoration or artifacts & \cite{sanders_customizing_2008}\\
\hline
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Physical Characteristics that affect interpersonal communication}
\subsubsection{Behavior of the Communicator}
The last of the three units of non-verbal communication is the behavior of the communicators. While the term behavior is used loosely in defining this unit, this encompasses both static posture and dynamic movements demonstrated by communicators. Of the three units of non-verbal communication, the behavior forms the most important aspect. Most part of the emotional information encoded by humans is delivered through the behavior of individuals during social interactions. Gestures, Posture, Touch and Voice form the basic subdivisions in behavioral non-verbal cueing. While the entire human body is important for the communication of these cues, the face and eyes play a major role.

\subsubsection{Gesture}
Gestures are dynamic movement of face and limbs displayed during interpersonal communication. Together, they convey a lot of information that is sometimes redundant (with speech) while other times deliver emotional information about the enactor. Most often gestures are classified based on their occurrence with speech. Accordingly, there are
\begin{enumerate} [(a)]
\item Speech-independent gestures, or emblems (like shrug, thumbs up, victory sign etc), that are mostly visual in nature and convey the user's response to the situation \cite{ekman_nonverbal_1976} \cite{wagner_field_2003}.
\item Speech-related gestures, or illustrators (pointing to a thing, drawing a shape while describing etc) \cite{efron_gesture_1972}.
\item Punctuation gestures, that emphasize, organize and accent important segments of a communication, like pounding the hand, raising a fist in the air etc.
    \end{enumerate}

\subsubsection{Posture}
Posture refers to the temporary limb and body positions assumed by individuals during interpersonal interactions.  Posture is a very effective medium for communicating some of the important non-verbal cues like leadership, dominance \cite{weisfeld_erectness_1982}, submissiveness and social hierarchy \cite{grant_comparison_1963}. For example, people who show a tendency of dominance tend to extend their limbs out while sitting thereby displaying an overall larger body size. Similarly, submissiveness seems to be correlated to reducing the overall body size by keeps the limbs together.

Both gestures and postures are influenced heavily by the cultural background of the individual and also varied with the geographical location \cite{kleinsmith_cross-cultural_2006}. Though the cultural influence if true with other non-verbal and verbal cues, the perceived difference is the highest in gestures and posture displayed by individuals.

\subsubsection{Touch}
Social touch has been a very important aspect of non-verbal communication in humans. Developmental biologists believe that the first set of sensory responses in a human fetus is touch \cite{montagu_touching:_1986}. From a social context this sensory channel is very well used in conveying important interpersonal cues such as interest, intimacy, warmth, confidence, leadership and sympathy \cite{afifi_use_1999}. Touch is a powerful means of unconscious interaction and it is believed that people who are very good in their social skills rely upon touch a lot \cite{hertenstein_communicative_2006}. Historically, the sense of touch (Haptics Communication \cite{hertenstein_touch_2006}) has been studied by psychologists in the perspective of understanding the human sensory system, but recently, haptics has grown out into the technology front providing human machine interfaces that augment or replace visual and auditory interfaces \cite{robles-de-la-torre_principles_2008}.

\subsubsection{Face}
The face is the primary channel for non-verbal communication. Humans are efficient in conveying and receiving plethora of information through subtle movements of their face and head. This focus on the face develops from a very young age and it has been shown that by 2 months, infants are adept in understanding facial gestures and mannerisms \cite{carver_development_2002}. The human face has very fine muscular control allowing it to perform complex patterns that are common to humans, while at the same time being vastly individual \cite{rinn_neuropsychology_1984}. The facial appearance of an individual is due to their genetic makeup, transient moods that stimulate the facial muscles and due to chronically held expressions that seem to set in and become permanent. Human visual system has developed the ability to read these subtleties on people's faces and interpret all the three aspects of the face - genetic makeup (person's identity through face recognition), transient mood (facial expression and emotion recognition), and permanent expression on the face (default neutral face of individuals). While the aspects of permanent facial appearance are important in the recognition of the individual, from a non-verbal communication perspective, the primary function of the face is directed towards communicating emotions and expressions.

The understanding of the human facial expression space was immensely increased by the work of Ekman, Frisen \cite{ekman_facial_1978} and Izard \cite{izard_maximally_1983} in the late 1970s. They independently measured precise facial movement patterns and correlated these individual movements with facial expressions on the human face. While Izard developed these patterns on infants, the Facial Action Coding System (FACS) developed by Ekman and Frisen has become the de facto standard for measuring facial expressions and emotions. FACS allow expression and emotion researchers to encode facial movements into accurate contraction and relaxation of facial muscles. Based on these facial actions, Ekman and Frisen discovered the global occurrence of seven basic judged emotions. As psychologists have started to master the FACS system of analyzing facial actions, human computer interaction specialists have started to use the same FACS encodings for building better interfaces that can determine human affect and respond accordingly.

\emph{Facial Action Coding System (FACS):}
FACS defines all possible facial feature movements into Action Units (AU) which represent movement of facial features (like lips, eye brow, chin etc). The AUs are the net effect of facial muscle contraction and relaxation, though they are not directly related to the muscles. Table below shows the different AUs that form the basis of FACS based facial coding with the appropriate number and the associated facial feature movement.

\begin{table}[hpdf]
\begin{center}
\caption{FACS communicative actions on the human face}
\label{Tab:Table2}
\begin{tabular}{|l|l||l|l|}
\hline
1 & Inner Brow Raiser & 24 & Lip Pressor\\
2 & Outer Brow Raiser & 25 & Lips part\\
4 & Brow Lowerer & 26 & Jaw Drop\\
5 & Upper Lid Raiser & 27 & Mouth Stretch \\
6 & Cheek Raiser & 28 & Lip Suck\\
7 & Lid Tightener & 29 & Jaw Thrust \\
9 & Nose Wrinkler & 30 & Jaw Sideways \\
10 & Upper Lip Raiser & 31 & Jaw Clencher\\
11 & Nasolabial Deepener & 32 & Lip Bite \\
12 & Lip Corner Puller & 33 & Cheek Blow \\
13 & Cheek Puffer & 34 & Cheek Puff \\
14 & Dimpler & 35 & Cheek Suck \\
15 & Lip Corner Depressor & 36 & Tongue Bulge \\
16 & Lower Lip Depressor & 37 & Lip Wipe\\
17 & Chin Raiser & 38 & Nostril Dilator \\
18 & Lip Puckerer & 39 & Nostril Compressor\\
19 & Tongue Out & 41 & Lid Droop \\
20 & Lip stretcher & 42 & Slit\\
21 & Neck Tightener & 43 & Eyes Closed\\
22 & Lip Funneler & 44 & Squint\\
23 & Lip Tightener & 45 & Blink\\
& & 46 & Wink\\
\hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{Eye}
Like the human face, eyes are very important for the control of non-verbal communication. This involvement of human eyes comes from the functions that gaze and mutual gaze play in everyday human interpersonal communication \cite{argyle_gaze_1976}. People use their gaze to convey subtle information that enables smooth verbal interaction which eventually leads to information exchange \cite{kleinke_gaze_1986}. From a research perspective, the function of gaze has been classified into four important functional categories \cite{kendon_functions_1967}. These include

\begin{table}[hpdf]
\begin{center}
\caption{The role of human eye in interpersonal communications.}
\label{Tab:Table4}
\begin{tabularx}{5.5in}{|X|X|}
\hline
Regulating the flow of communication & One of the most important functions of gaze is the regulation of verbal communication in bilateral and group communications. People use gaze to shift focus, bring the attention of a group of people to one thing, turn taking in group conversations \cite{mast_dominance_2002} and eliciting response from communication partners \cite{bavelas_listener_2002}. \\
\hline
Monitoring feedback & Gaze provides a means for individuals to get feedback during conversations and communications. Feedback is a very important tool while people converse. Humans study the eyes of the listener to cognitively inject or eliminate more verbal information into the conversation \cite{van_dulmen_shifts_1997}. \\
\hline
Reflective of cognitive activity & Both listeners and speakers tend not to gaze at others when they are processing complex ideas or tasks. Studies have shown that people can answer better when they close their eyes and are allowed to process their thoughts \cite{glenberg_averting_1998}. Thus, cognitive processing is displayed very elegantly by monitoring eye gaze patterns. \\
\hline
Expressing emotions & Along with the facial muscular movements, the eyes play a vital role in the expression of emotions. In fact, in human computer interaction research, it has been found that relying on the eyes and the eyelids alone can provide more accurate delivery of affect information when compared to the entire face \cite{orozco_confidence_2008}. Verbal communication tends to move the lips and mouth quickly and randomly that can make image and video processing of expressions very tough. Some of the more recent spontaneous expression recognition research is focusing on the eyes for this very reason. \\
\hline
\end{tabularx}
\end{center}
\end{table}



\chapter{MOTIVATION}
\DoubleSpacing
\setlength{\parindent}{.5in}
In this chapter we discuss three important problems that highlight the need to communicate social situational awareness to individuals involved in interpersonal interactions.

\section{Assistive Technology}
most part of the non-verbal encoding happens through visual media. While some parts of these cues are delivered along with speech, most part of the nonverbal communication is inaccessible to someone with visual impairment or blindness. This disconnect from the visual stimulations deprive the individuals of vital communicative cues that enrich the experience of social interactions.  People who are blind cannot independently access this visual information, putting them at a disadvantage in daily social encounters.  For example, during a group conversation it is common for a question to be directed to an individual without using his or her name-instead, the gaze of the questioner indicates to whom the question is directed. In such situations, people who are blind find it difficult to know when to speak because they cannot determine the direction of the questioner's gaze. Consequently, individuals who are blind might be slow to respond or talk out of turn, possibly interrupting the conversation. As another example, consider that people who are blind cannot use visual cues to determine when their conversation partners change positions (e.g., pacing the floor or moving to a more comfortable chair). In this scenario, an individual who is blind might inadvertently create a socially awkward situation by speaking in the wrong direction.

To compound these problems, sighted individuals are often unaware of their non-verbal cues and often do not (or cannot) make appropriate adjustments when communicating with people who are blind. Also, people who are blind often do not feel comfortable asking others to interpret non-verbal information during social encounters because they do not want to burden friends and family.  The combination of all these factors can lead people who are blind to become socially isolated \cite{segrin_poor_2000}, which is a major concern given the importance of social interaction. While people who are blind and visually impaired face a difficulty in social interactions, research in rehabilitation training for these populations recommends that the social involvement for these individuals have to substantially increase in order to enable their acceptance of the society.

National Center for Health Statistics reported in $2007$ that the estimated number of visually impaired and blind people totals up to $21.2$ million in the United States alone . Global numbers are daunting. In $2002$ more than $161$ million people were visually impaired, of whom $124$ million people had low vision and $37$ million were blind . World Health Organization reports that more than $82$\% of the populations who are blind or visually impaired are of age $50$ or older. With the life expectancy going up in most developing countries, the percentage of general population entering into some sort of visual impairment is going to increase in the coming years.

Recently, Jindal-Snape \cite{jindal-snape_generalization_2004} \cite{jindal-snape_use_2005} \cite{jindal-snape_using_1998} carried out extensive research in understanding social skill development in the blind and visually impaired. She has studied individual children (who are blind) from India where the socio-economic conditions do not provide for trained professionals to work with children with disabilities. Her seminal work in understanding social needs of children who are blind have revealed two important aspects of visual impairment that restricts seamless social interactions.

While most persons who are blind or visually impaired eventually make accommodations for the lack of visual information, and lead a healthy personal and professional life, the path towards learning effective accommodations could be positively effected through the use of assistive aids. Specifically, children with visual disabilities find it very difficult to learn social skills while growing amongst sighted peers, leading to social isolation and psychological problems \cite{jindal-snape_generalization_2004}. Social disconnect due to visual disability has also been observed at the college level \cite{shinohara_blind_2009} where students start to learn professional skills and independent living skills. Any assistive technology aid that can enrich interpersonal social interactions could prove beneficial for persons who are visual disabled.

\section{Remote Interactions}
An industry survey \cite{solomon_challenges_2010} of $1592$ individuals who collaborated remotely, carried out by RW3 CultureWizard - a company focused on improving international collaborations - reported difficulties similar to what was faced by the individuals who are blind. "Respondents found virtual teams more challenging than face-to-face teams in managing conflict ($73$\%), making decisions ($69$\%), and expressing opinions ($64$\%). The top five challenges faced during virtual team meetings were insufficient time to build relationships ($90$\%), speed of decision making ($80$\%), different leadership styles ($77$\%), method of decision making ($76$\%), and colleagues who do not participate ($75$\%)." These results can be correlated to the need for Social Situational Awareness in group settings, specifically one that can promote leadership and personal understanding of each other as indicated in Section 2.1.2.

Further, when the participants were asked about the personal challenges faced during virtual team meetings, they reported inability to read non-verbal cues ($94$\%), absence of collegiality ($85$\%), difficulty establishing rapport and trust ($81$\%), difficulty seeing the whole picture ($77$\%), reliance on email and telephone ($68$\%), and a sense of isolation ($66$\%)." Delivering non-verbal cues, establishing trust and rapport, and easing isolation are all derivatives of increasing one's social connection to their interaction partners, be it remote or face-to-face. Observing people who are disabled and the way they communicate with their co-located partners, it is possible to derive inspirations for novel social mediation technologies. The following subsection discusses one example of how to develop an evidence-based social situational awareness model based on hand shaking in the blind population as an example of social interaction between participants.

\begin{table}
\caption{Survey on the challenges of remote interaction \cite{solomon_challenges_2010}}
\label{Table:Tab3}
\begin{tabular}{|l|}
\hline
Challenges in virtual teams compared to face-to-face teams \\
\hline
\includegraphics[width=3in]{Suggestion1.jpg}\\
\hline
Top five challenges faced during virtual team meetings \\
\hline
\includegraphics[width=3in]{Suggestion2.jpg}\\
\hline
Personal challenges during virtual team meetings\\
\hline
\includegraphics[width=4.5in]{Suggestion3.jpg}\\
\hline
\end{tabular}
\end{table}





\section{Medical Teams}






\chapter{ASSISTIVE TECHNOLOGY DESIGN}
\DoubleSpacing
\setlength{\parindent}{.5in}
Affective Computing research has employed algorithmic framework to quantitatively study both verbal and non-verbal cues displayed by the humans during social communication.  Signal streams from various sensors, including visual sensors (e.g. cameras), audio sensors (e.g. microphones) and various physiological sensors (such as EEG, EMG, and galvanic skin resistance sensors) have been used to evaluate human emotional states.  A good review of research work in Affective Computing can be found in \cite{zhihong_zeng_survey_2009}.  This research has enabled a better understanding of human physiological signals, with respect to emotional states, and the results have been used to facilitate human-computer interaction (HCI). In theory, a system that can detect non-verbal social cues could also be used as an assistive device to provide social feedback to people with disabilities.  The emphasis here would not be so much on interpreting these cues as on presenting social cue information to the user, and allowing the user to interpret them.  However, very little research has been done towards finding intuitive methods for presenting social cue information to humans.  \cite{ur_rehman_manifold_2007} developed a haptic chair for presenting facial expression information.  It was equipped with vibrotactile actuators on the back of the chair that represented some specific facial feature. Experiments conducted by the researchers showed that people were able to distinguish between six basic emotions.  However, this solution had the obvious limitation that the user needed to be sitting in the chair to use the system.

\emph{Observation 1: Assistive technology designed towards social assistance should be portable and wearable so that the users can use them at various social circumstances without any restriction to their everyday life.}

People with disabilities are not always able to perceive or interpret implicit social feedback as a guide to improving their communication competence.  However, they might be able to use explicit feedback provided by a technological device.  Rana and Picard \cite{teeters_self-cam:_2006} developed a device called Self Cam, which provides explicit feedback to people with Autism Spectrum Disorder (ASD).  The system employs a wearable, self-directed camera that is supported on the users own shoulder to capture the user's facial expressions. The system attempts to categorize the facial expressions of the user during social interactions to evaluate the social interaction performance of the ASD user.  Unfortunately, the technology does not take into account the social implication of assistive technologies. Since the technology is being developed to address social interactions, it is important to take into account the social artifacts of technology. A device that has unnatural extensions could become more of a social distraction for both the participants and users than as an aid.


\emph{Observation 2: Assistive technology designed towards social assistance should allow seamless and discrete embodiment of sensors or actuators making sure the device does not become a social distraction.}

Vinciarelli et. al. \cite{vinciarelli_social_2008} have described the use of discrete technologies for understanding social interactions within groups, specifically targeting professional environments where individuals take decisions as a group. They analyze the use of bodily mannerisms and prosody to extract nonverbal cues that allow group dynamics analysis. They rely on simple sensors in the form of wearable tags \cite{kim_meeting_2008} which detect face to face interaction events along with prosody analysis to determine turn taking, emotion of the speaker, distance to an individual etc. Pentland describes these signals captured during group interactions as \cite{pentland_honest_2008} honest signals. Some of his recent works \cite{vinciarelli_social_2008-1} in the area of social monitoring hopes to capture these signals and provide feedback to individuals about their social presence within a group. The use of social feedback is illustrated elegantly in their work but their findings relied on sensors carried by all individuals involved in the study. Having everyone in a group wear sensors has proved to be a viable and productive approach for studying group dynamics.  However, this approach is not viable as a strategy for developing an assistive technology, as it is not realistic to assume that everyone who interacts with a person with a disability will wear sensors.

\emph{Observation 3: Assistive technology designed towards social assistance should incorporate mechanisms embodied on the user to determine both self and other's social mannerism.}

In two independent experiments \cite{transon_using_1988} and \cite{felps_modification_1988}, researchers developed a social feedback device that provides intervention when a person with visual impairment starts to rock their body displaying a stereotypy. \cite{transon_using_1988} designed a device that consisted of a metal box with a mercury level switch that detects any bending actions. The feedback was provided with a tone generator that was also located inside the metal box.  The entire box was mounted on a strap that the user wears around his/her head. The authors tested it on a congenitally blind individual who had severe case of body rocking and they conclude that the use of any assistive technology is useful only temporarily while the device is in use. They state that the body rocking behavior returned to baseline levels as soon as the device was removed. Since the time of this experiment, behavioral psychology studies have explored short term feedback for rehabilitation \cite{jindal-snape_use_2005}, and these studies support the above observation that short term feedback is often detrimental to rehabilitation and subject's case invariably worsens. Unfortunately, due to the prohibitively large design of the device developed by these researchers, it was impossible to have the individual wear the device over long durations.

\emph{Observation 4: Assistive technology designed towards social assistance and behavioral rehabilitation should be used over long durations in such a way that the feedback is slowly tapered off over a significantly longer duration of time.}


In \cite{felps_modification_1988} researchers used a 'Drive Alert' (driver alerting system that monitors head droop) to detect body rocking and provide feedback to a congenitally blind 21 year old student. The research concludes that they were able to control body rocking effectively, but the device could not differentiate between body rocks from any other functional body movements. This device, primarily built to sense drooping in drivers provides no opportunity to differentiate between a body rock and a functional droop. Use of such devices could only be negative on the user as a large number of false alarms would only discourage an individual from using any assistive technology.

\emph{Observation 5: Assistive technology designed towards social assistance and behavioral rehabilitation should be effective in discriminating social stereotypic mannerisms from other functional movements to keep the motivation of device use high.}


\section{Conceptual Framework}
\subsection{Design principles for social assistive and rehabilitative devices}
A device that is developed to facilitate the social interactions of people with sensory, or cognitive disabilities might do so by, (a) detecting social cues during social interactions and delivering that information to the user in real time to enable empathy, or (b) detecting the user's stereotypic behaviors during social interactions and communicating that information to the user in real time to provide social feedback.  The first device might be classified as an assistive technology, while the second might be classified as a rehabilitative technology.  Ideally, such a device would be based on the following design principles:

\begingroup
\leftskip0.5in
\setlength{\parindent}{0in}
\emph{Design principle 1:} The device should be portable and wearable so that it can be used in any social situation, and without any restriction on the user's everyday life.

\emph{Design principle 2:} The device should employ sensors and personal signaling devices that are unobtrusive, and do not become a social distraction.

\emph{Design principle 3:} The device should include sensors that can detect the social mannerisms of both the user and other people with whom the user might communicate.

\emph{Design principle 4:} The device should be comfortable enough to be worn repeatedly for extended periods of time, to allow it to be used effectively for rehabilitation.

\emph{Design principle 5:} The device should be able to reliably distinguish between the user's problematic stereotypic mannerisms and normal functional movements, to ensure that it will be worn long enough to achieve rehabilitation.

\endgroup

\section{Requirements Analysis for a Social Assistive Technology for Individuals who are Blind and Visually Impaired}
In order to identify the unmet needs of the visually impaired community, two focus groups consisting primarily of people who are blind, as well as disability specialists and parents of students with visual impairment and blindness where conducted \footnote{ In  order  to  understand  the  assistive  technology  requirements  of  people  who  are  blind, we conducted two focus group studies (one in Tempe, Arizona USA - $9$ participants, and another in Tucson, Arizona USA - $11$ participants) which included:
\begin{enumerate}[1.]
\item Students and adult professionals who are blind,
\item Parents of individuals who are blind
\item Professionals who work in the area of blindness and visual impairments.
\end{enumerate}
There was unanimous agreement among participants that a technology that would help people with visual impairment to recognize people or hear them described would significantly enhance their social life.}. Members of these focus groups who were blind or visually impaired were encouraged to speak freely about their challenges in coping with daily living. During these focus groups, the participants agreed on many issues as being important problems. However, one particular problem - that of engaging freely with their sighted counterparts - was highlighted as a particularly important problem that was not being addressed by technology specialists \footnote{  To quote some candidate's opinion about social assistance technology in a everyday setting:
\begin{itemize}
\item \"It would be nice to walk into a room and immediately get to know who are all in front of me before they start a conversation\".
\item One young man said, \"It would be great to walk into a bar and identify beautiful women\".
\end{itemize}}.

While various other examples were cited by individuals during these focus group studies, the inability to access non-verbal cues were considered of highest priority. Based on these discussions, a list of needs was complied that characterized social needs often experienced by people with visual impairments. In doing so, two important aspects of social interaction were identified. These included
\begin{enumerate}[1.]
\item Access to the non-verbal cues of others during social interactions, and
\item How one is perceived by others during social interactions.
\end{enumerate}

These needs correlated with the psychology studies conducted by Jindal-Snape with children who were visually impaired. She identifies these two needs under the \emph{Social Learning} and \emph{Social Feedback}. While these two important categories were identified, for simplification, the non-verbal cue needs were reduced to $8$ aspects of social interactions that focused primarily on the physical characteristics of the interaction partner and the behaviors of the
interaction partner. These questions were developed with the help of visually impaired professionals and students:

\begin{enumerate}[1.]
\item Knowing how many people are standing in front you, and where each person is standing.
\item Knowing where a person is directing his/her attention.
\item Knowing the identities of the people standing in front of you.
\item Knowing something about the appearance of the people standing in front of you.
\item Knowing whether the physical appearance of a person who you know has changed since the last time you encountered him/her.
\item Knowing the facial expressions of the person standing in front of you.
\item Knowing the hand gestures and body motions of the person standing in front of you.
\item Knowing whether your personal mannerisms do not fit the behavioral norms and expectations of the sighted people with whom you will be interacting.
\end{enumerate}

Further, in order to understand the importance of these non-verbal communication primitives an online survey was carried out to determine a self-report importance map of the various non-verbal cues. This list of questions included both the importance from the perspective of allowing access to the non-verbal cues of the interaction partner (for enabling Social Learning), while also focusing on the personal body mannerism (for enabling Social Feedback) of the individual.The online survey was anonymously completed by $28$ people, of whom $16$ were blind, $9$ had low vision, and $3$ were sighted specialists in the area of visual impairment and vocational training. The online survey consisted of eight questions that corresponded to the previously identified list of needs. Respondents answered each question using a Five-point Likert scale, the metrics being (1) Strongly disagree, (2) Disagree, (3) Neutral, (4) Agree, and (5) Strongly agree.

\section{Results from the Online Survey}
\subsection{Average Response}
Table \ref{Tab:Table5} shows the eight aspects of social interactions that were investigated with the individuals who are blind and visually impaired. The results are sorted by descending importance, as indicated by the survey respondents (the question numbers correspond to the need listed in the previous section). The mean score is the average of the respondents on the $5$ point scale that was used to capture the opinions.  A score closer to $5$ implies that the respondents strongly agree with a certain question and that they consider inaccessibility to that particular non-verbal cue to be important deterrent to their social interactions. On the other hand, a score closer to $1$ represents the respondent did not consider the access to a specific non-verbal cue to be important during their social interactions.

\begin{table}[h]
\caption{Average Score on the 8 Questions obtained through an Online Survey.}
\label{Tab:Table5}
\begin{center}
\begin{tabularx}{5in}{|l|X|c|}
\hline
Question No. & Question & Mean Score\\
\hline
8. & I would like to know if any of my personal mannerisms might interfere with my social interactions with others. & 4.5 \\
\hline
6. & I would like to know what facial expressions others are displaying while I am interacting with them. & 4.4 \\
\hline
3. & When I am standing in a group of people, I would like to know the names of the people around me. & 4.3 \\
\hline
7. & I would like to know what gestures or other body motions people are using while I am interacting with them. & 4.2 \\
\hline
1. & When I am standing in a group of people, I would like to know how many people there are, and where each person is. & 4.1 \\
\hline
2. & When I am standing in a group of people, I would like to know which way each person is facing, and which way they are looking. & 4.0 \\
\hline
5. & I would like to know if the appearance of others has changed (such as the addition of glasses or a new hair-do) since I last saw them. & 3.5 \\
\hline
4. & When I am communicating with other people, I would like to know what others look like. & 3.4 \\
\hline
\end{tabularx}
\end{center}
\end{table}

\subsection{Response on Individual Questions}
Figure \ref{Fig:Figure6} shows the histogram of responses for the 8 Questions that were asked as part of the survey. Each subplot refers to a single question and shows the number of times users responded to that particular question with answers from 1 to 5 on the Lickert Scale. Each histogram adds up to a total of 28 that corresponds to the 28 participants that took part in the online survey.

\begin{figure}[h]
\includegraphics[width=5.5in]{histogram.jpg}
\caption{Histogram of Responses grouped by Questions}
\label{Fig:Figure6}
\end{figure}

Some of the observations from the important histograms include,
\begin{itemize}
\item Respondents are highly concerned about how their body mannerisms are perceived by their sighted peers (based on the response to Question 8 on the survey).
\item Facial expressions form the most important visual non-verbal cue that individuals who are blind or visually impaired feel they do not have access to (based on Question 6 on the survey). This correlates with the studies into non-verbal communication that highlights the importance of facial mannerisms and gestures, which are mostly visual in their decoding.
\item Followed by facial expressions, body mannerisms seem to be of higher importance for individuals who are blind and visually impaired (based on Question 3 of the survey).
\item The responses to questions 7, 1 and 2 suggest that respondents would like to know the identities of the people with whom they are communicating, relative location of these people and whether their attentions are focused on the respondent. This corresponds to knowing the position of their interaction partners when they are involved in a bilateral or group communication. People tend to move around, especially when they are standing, causing people who are blind to lose their bearing on where people were standing. This can result in individuals addressing an empty space assuming that someone was standing there based on their memory.
\item The responses to questions 4 and 5 indicate that there was a wide variation in respondents' interest in (4) knowing the physical appearance of people with whom they are communicating and (5) knowing about changes in the physical appearance of people with whom they are communicating. Many respondents indicated moderate, little, or no interest in either of these areas.
\end{itemize}

\subsection{Response Ratio}
Figure \ref{Fig:Figure7} shows the number of times the respondents chose to answer the 8 questions with their agreement or disagreement. The y-axis has been normalized to 100 points. The graph shows that respondents chose to answer the most by agreeing (Likert Scale 4) with the 8 questions. Followed closely behind was the strong agreement (Likert Scale 5) with the questions asked in the survey. The respondents chose to answer the least through strong disagreement (Likert Scale 1) to what was asked in the survey.

\begin{figure}
\begin{center}
\includegraphics[width=5in]{responseratio.jpg}
\caption{Response Ratio}
\label{Fig:Figure7}
\end{center}
\end{figure}

As described earlier, the 8 questions corresponding to the social needs of the individuals were identified from the focus group survey that was conducted. Thus, the questions presented in the online survey questions were biased towards the needs of everyday social interactions of individuals who are blind and visually impaired. Thus, the implicit assumption while preparing this survey itself is that most of these items have been identified as being important and that only a priority scale needs to be extracted. This implicit assumption is immediately brought out by looking at the frequency with which the respondents answer with their agreement (Likert Scale 4) and strong agreement (Likert Scale 5).

\subsection{Rank Average Importance Map for Various Non-verbal Cues}
As can be seen from Figure \ref{Fig:Figure7}, the questionnaires were biased and the frequency of the responses is not Gaussian. This bias implies that using sample mean of the Lickert Scale responses will immediately show the same bias. This is due to the Gaussian iid assumption that is made while extracting the mean for the answers. In order to overcome this non-Gaussianity, we resort to non-parametric mean for the responses. Rank average of the responses is estimated instead of the typical mean of the responses for each of the question. Please see Appendix \ref{AppendixA} for the algorithm to determine the Rank Average. Since no assumptions on the distribution of the response are made, unlike the mean, the rank average gives a non-parametric method for comparing the responses of the individuals. The ranks can be either assigned ascending or descending with respect to the responses, i.e. rank 1 could mean all responses that were answered with strongly disagree (numeral 1), or rank 1 could mean all responses that were answered with strongly agree (numeral 5).

\begin{figure}
\includegraphics[width=5in]{rankaverage.jpg}
\caption{Rank average of the 8 questions}
\label{Fig:Figure8}
\end{figure}


In the Figure \ref{Fig:Figure8}, we have assigned rank 1 to strongly disagree. This is for the sake of visual convenience. Thus, higher the average rank, higher is that group's response from the respondents. Comparing Figure \ref{Fig:Figure8} to Table \ref{Tab:Table5}, it can be seen that the same ordering of priority can be seen through mean and rank average. But the mean tends to show very little variation between responses due to the bias that is present in the questions. On the other hand the rank average provides a good comparison scale.

\chapter{Person-Specific Face Recognition}
face recognition has the potential for recognizing
people at a distance, without their knowledge or cooperation.  For
decades, banking, retail, commercial, and industrial buildings have
been populated with surveillance cameras that capture video streams
of all people passing through critical areas.  More recently, as a
result of threats to public safety, some public places (such as in
Glasgow and London) have been heavily populated with video
surveillance cameras.  On average, a person moving through London is
captured on video over 5 times a day. This offers an unprecedented
basis for developing and testing face recognition as a biometric for
security and surveillance.

Given this great potential, it is not surprising that many private
corporations have attempted to develop and deploy face recognition
systems, as an adjunct to existing video security and surveillance
systems.  However, the performance of these systems has been
disappointing. Depending on how such a system is adjusted,
miscreants might easily pass through the system undetected, or
innocent people might be incessantly inconvenienced by false alarms.

One of the most difficult problems that face recognition researchers
encounter in surveillance applications is that face databases of
miscreants typically contain only frontal and profile views of each
person's face, with no intermediate views.  Surveillance videos
captured of the same person with the same camera in the same
lighting conditions might have face images that look quite
different, due to pose angle variations, making it very difficult to
compare captured face images to those in a database.  Combine this
problem with the fact that miscreants are highly motivated to
disguise their identity, and the fact that face databases often
contains thousands of faces, and the problem seems insurmountable.

Given all of these complicating factors, it is premature to rely
upon face recognition systems for detecting miscreants in public
places.  On the other hand, the use of face recognition in
controlled access applications (where users are highly motivated to
cooperate, and where face database images can be both captured and
tested with the same camera under the same illumination conditions)
is certainly within the limitations of current face recognition
algorithms.

\subsection{Employing face recognition to facilitate social interactions}
However, there is a real-world application for face recognition that
is moderately challenging, but still potentially within the realm of
possibility. When people who are blind enter a room, they might find
it awkward to initiate social interactions because they don't know
how many people are in the room, who those people are, or where they
are standing \footnote{In order to understand the assistive
technology requirements of people who are blind, we conducted two
focus group studies (one in Tempe, Arizona USA - 9 participants, and
another in Tucson, Arizona USA - 11 participants) which included:
\begin{enumerate} \item students and
adult professionals who are blind,
\item parents of individuals who are blind \item professionals
who work in the area of blindness and visual
impairments\end{enumerate}There was unanimous agreement among
participants that a technology that would help people with visual
impairment to recognize people or hear them described would
significantly enhance their social life.}\hspace*{0.1in}\footnote{To
quote some candidates opinion about face recognition technology in a
social setting:
\begin{itemize}
\item ``It would be nice to walk into a room and immediately get to
know who are all in front of me before they start a conversation''.
\item One young man said, ``It would be great to walk into a bar
and identify beautiful women''.\end{itemize}}. A robust, wearable
face recognition device could solve this problem.

This problem is simplified considerably by the fact that, on a
day-to-day basis most people encounter a limited number of people
whom they need to recognize.  It is further simplified by the fact
that people typically don't attempt to disguise their appearance in
social situations.  When a new person is encountered, the system
could employ face detection to extract and save a sequence of face
images captured during a conversation.  This would provide a wide
variety of facial expressions and pose angles, that could be stored
in a database, and used for training a face recognition algorithm.

As people use such an assistive device over an extended period of
time, they will learn both its abilities and its limitations.
Conjectural information from the system can then be combined with
the user's other sensory abilities (especially hearing) to jointly
ascertain the identity of the person.  This synergy between the user
and the system relaxes some of the stringent requirements normally
placed on face recognition systems.

However, such an assistive technology application still poses some
significant challenges for researchers.  One problem is the extreme
variety of in lighting conditions encountered during normal daily
activities. While there are standards for indoor office lighting
that tend to provide diffuse and adequate lighting, lighting in
other public places might vary considerably. For example, large
windows can significantly alter lighting conditions, and
incandescent lighting is much more yellow than florescent lighting.
Outdoor lighting can be quite harsh in full sunlight, and much more
blue and diffuse in shadows. A person who is blind might not be
aware of extreme lighting conditions, so the system would need to
either (1) be tolerant of extreme variations or (2) recruit the user
to ameliorate those extreme conditions.

In summary, the development of an assistive face recognition system
for people who are blind provides a more tractable problem for face
recognition researchers than security and surveillance applications.
It imposes a somewhat less stringent set of requirements because (1)
the number of people to be recognized is generally smaller, (2)
facial disguise is not a serious concern, (3) multiple pose angles
and facial expressions of a person can be captured as training
images, and (4) the person recognition process can be  a
collaborative process between the system and the user.

In an attempt to provide such an assistive face recognition system,
we have developed a new methodology for face recognition that
detects and extracts unique features on a person's face, and then
uses those features to recognize that person. Contrast this with
conventional face recognition algorithms that might avoid the use of
a few distinguishing features because that approach might make the
system very vulnerable to disguise.

\section{Face Recognition in Humans}
For decades, scientists in various research areas have studied how
humans recognize faces. Developmental psychologists have studied how
human infants start to recognize faces, cognitive psychologists have
studied how adolescents and adults perform face recognition;
neuroscientists have studied the visual pathways and cortical
regions used for recognizing faces, and neuropsychologists have
attempted to integrate knowledge from neurobiological studies with
face recognition research. Computer vision researchers are
relatively new to this area, and have attempted to develop face
recognition algorithms using image processing methods.  Only
recently have computer vision researchers been motivated to better
understand the process by which humans recognize faces, in order to
use that knowledge to develop robust computational models. Their new
interest has lead to more inter-disciplinary face recognition
research, which will likely aid our understanding of face
recognition.

New studies have shown that humans, to a large extent, rely on both
the featural and configural information in face images to recognize
faces ~\cite{Schw2003}. Featural information provides details about
the various facial features, such as the shape and size of the nose,
the eyes, and the chin.  Configural information defines the
locations of the facial features, with respect to each other.
Psychologists Vicki Bruce and Andrew Young ~\cite{Bruce2006} agree
with this dual representation, saying that humans create a
view-centric description of a human face by relying upon
feature-by-feature perceptual input, which is then combined into a
structural model of the face.

Sadar et al ~\cite{Sadr2003} showed that characteristic facial
features are important for recognizing famous faces. For example,
when they erased eye-brows from famous people's faces, face
recognition by human participants was adversely affected.  Young
~\cite{Young1987} showed that human participants were confused when
asked to recognize faces that combined facial features from
different famous faces. These studies suggest that the details of
facial features are important in the recognition of faces.

However, ~\cite{Sinha2006} showed that the relative locations of the
facial features was also very important for the recognition of
faces.  They collected face images of famous personalities, and then
changed the aspect ratio of those images, such that the height was
greatly compressed, while the width was emphasized.  Surprisingly,
all the resulting face images were still recognizable, despite their
contorted appearance, as long as the relative locations of the
features were maintained within the distorted image.  This study
suggests that humans can flexibly use the configural information
when recognizing faces.

Another important area of research in the human perception of faces
has been in understanding the medical condition of face blindness,
called {\it prosopagnosia}.  People with prosopagnosia are unable to
recognize faces including their own. Until recently it was assumed
that prosopagnosia was acquired often as a result of a localized
stroke. However new evidence suggests that a substantial portion of
the general population have a congenital form of prosopagnosia
~\cite{Mccon1976}. Kennerknecht et al ~\cite{Kenner2006} conducted a
survey of 789 students in 2006 which showed that 17 (2.5\%) suffered
from congenital prosopagnosia. These students went about their daily
life without realizing their disorder in face recognition.

Other studies at the Perception research centers at Harvard and Univ
College of London have shown that prosopagnosics recognize people
using unique personal characteristics, such as hair style, gait,
clothing, and voice. These findings suggest that the detection of
unique personal characteristics might provide a basis for face
recognition systems to better recognize people. Since current
methods of face recognition have met with only limited success, it
makes sense to explore the use of this alternative approach.

Research in Own-Race Bias (ORB) in face recognition ~\cite{Turk2005}
has also revealed some interesting results regarding human face
recognition capabilities. David Turk et al. found that, when humans
are presented with new objects or new faces, they initially learn to
recognize those objects and faces based on their distinctive
features.  Then, as familiarity increases, they incorporate
configural information, moving towards holistic recognition. This
study suggests that distinctive features are important during the
initial stages of face recognition, and that configural information
subsequently provides additional useful information.

Distinctive facial features can take many different forms.  For
example, after a first encounter with a person who has a handlebar
moustache, we readily recognize that person by the presence of his
distinctive feature.  Similarly, a person with a large black mole on
her face will be remembered by first-time acquaintances by that
feature.  Given the current limited understanding of how humans
recognize faces, it makes sense to use these observations as the
basis for a new approach to face recognition.

The research described in this chapter is based on the approach of
identifying distinctive facial features that can be used to
distinguish each person's face from other faces in a face database.
In recognition of the role played by configural information in the
later stages of face recognition, it also takes into account the
location of these features with respect to each other.  The results
of our research suggest that this approach can be very effective for
distinguishing one person's face from other faces.

\section{Our Approach to Face Recognition}
Having introduced the potential for using characteristic
person-specific features for face recognition, we now turn our
attention towards the development of a method for discovering such
features, and for using them to index face images. Then we propose a
novel methodology for face recognition, using person-specific
feature extraction and representation. For each person in a face
database, a learning algorithm discovers a set of distinguishing
features (each feature consisting of a unique local image
characteristic, and a corresponding face location) that are unique
to that person. This set of characteristic facial features can then
be compared to the normalized face image of any person, to determine
the presence or absence of those features. Because a unique set of
features is used to identify each person in the database, this
method effectively employs a different feature space for each
person, unlike other face recognition algorithms that assign all of
the face images in the database to a locality in a shared feature
space. Face recognition is then accomplished by a sequence of steps,
in which  query face images is mapped into a locality within the
feature space of each person in the database, and its position is
compared to the cluster of points in that space that represents that
person. The feature space in which the query face images are closest
to the cluster is used to identify the query face images.

Having introduced the conceptual theory behind a person-specific
characteristic feature extraction approach to face recognition, we
now propose in the subsequent sections a method for detecting and
extracting such features from face images, and for constructing a
feature space that is unique to each person in the database.

\section{Feature Extractors}
\subsection{What is a Feature?}
The task of face recognition is inherently a multi-class
classification problem. For every face image $X$, there is an
associated label $y$ that is the name of the class, i.e. the name of
the person depicted in the image. While $X$ represents the image of
the person, there is no inherent constraint on whether the image is
a color RGB, HUV or YCbCr image, or a gray-scale image with a
gray-scale range of 0 to 255, or even spectral representation that
is extracted from the face image using Fourier transform or
Wavelets. Irrespective of the image representation, the basis
vectors spanning that representation are called features. The
feature space spanned by these basis vectors is partitioned  by the
decision boundaries that ultimately define the different classes in
the multi-class problem of face recognition. In this work, we choose
a particular set Gabor filters as feature detectors, and each of
those feature detectors for each person in the database, and that
set of Gabor filters spans a unique feature space for that person.

\subsection{Gabor Features}
\label{subsecGF} Gabor filters are a family of functions (sometimes
called Gabor Wavelets) that are derived from a mother kernel (a
Gabor Function) by varying the parameters of the kernel. As with any
wavelet filters, the Gabor filters extract local spatial frequency
content from the underlying image. Gabor Filters specifically
capture the spatial location and spatial orientation of the
intensity variations in the image underneath the filter's location.
By varying the spatial frequency and the spatial scope of the
filters, it is possible to extract a Gabor coefficient that
partially describes the nature of the image underneath it. The
coefficients obtained by filtering a locality in a face image with a
set of different Gabor Filters are called Gabor Features.

\subsubsection{Use of Gabor Filters in Face Recognition}
Gabor filters have been widely used to represent the receptive field
sensitivity of simple cell feature detectors in the human primary
visual cortex. Recognizing this fact, Gabor features have been
widely used by face recognition researchers. Over the last few
years, the extensive use of Gabor wavelets as generators of feature
spaces for face recognition, has led to objective studies of the
strength of Gabor features for this application. For example, Shan
et al [Shan2004] reviewed the strength of Gabor features for face
recognition using an evaluation method that combined both alignment
precision and recognition accuracy. Their experiments confirmed that
Gabor features are robust to image variations caused by the
imprecision of facial feature localization. As indicated by Gökberk
et al ~\cite{Gokberk2007}, several studies have concentrated on
examining the importance of the Gabor kernel parameters for face
analysis. These include: the weighting of Gabor kernel-based
features using the simplex algorithm for face recognition
~\cite{Wiskott1997}, the extraction of facial subgraphs for head
pose estimation ~\cite{Kruger1997}, the analysis of Gabor kernels
using univariate statistical techniques for discriminative region
finding ~\cite{Kalocsai2000}, the weighting of elastic graph nodes
using quadratic optimization for authentication ~\cite{Tefas2001},
the use of principal component analysis (PCA) to determine the
importance of Gabor features ~\cite{Liu2004}, boosting Gabor
features ~\cite{Yang2004} and Gabor frequency/orientation selection
using genetic algorithms ~\cite{ Wang2002}.

 A relevant work on Gabor Filters for face recognition that is closely related to the
research presented here is by Wiskott and von der Malsburg
~\cite{wiskott_face_1997}. Their work
~\cite{von_der_malsburg_nervous_1985}
~\cite{bienenstock_neural_1987} ~\cite{wiskott_face_1995}
~\cite{wiskott_face_1996},~\cite{wiskott_face_1997} proposes a
framework for face recognition that is based on modeling human face
images as labeled graph. Termed {\em Elastic Bunch Graph Matching}
(EBGM), the technique has become a cornerstone in face recognition
research. Each node of the graph is represented by a group of Gabor
filters/wavelets (called "jets") which are used to model the
 intensity variations around their locations. The edges
of the graph are used to model the relative location of the various
jets. Since the jets represent the underlying image characteristics,
it is desirable to place them on fiducial points on the face. This
is achieved by {\em manually} marking the locations of the facial
fiducial points using a small set of controlled graphs that
represent ``general face knowledge'', which represents an average
geometry for the human face. In our work, a genetic algorithm is
used to obtain the spatial location of the fiducial points. Besides
automating the process of locating these points, our work identifies
spatial locations on the face image that are unique to every single
person, rather than relying on an average geometry.

Closely following the work of Wiskott et. al., Lyons et. al.
~\cite{lyons_automatic_1999} proposed a technique that uses Gabor
Filter coefficients extracted at 1) automatically located
rectangular grid points or 2) manually selected image feature
points. These coefficients are then used to bin face images based on
sex, race and expression. The technique relies on a combined
Principal Component Analysis (PCA) dimensionality reduction and
Linear Discriminant Analysis (LDA) classification over the extracted
Gabor coefficients, to achieve a pooling of images. While the
classification task is not related directly to {\em identifying}
individuals from face images, this technique also demonstrates the
ability of Gabor Filters to extract features that can encode subtle
variations on facial images, providing a basis for face
identification.

\subsubsection{Gabor Filters}
Mathematically, Gabor Filters can be defined as follows:
\begin{equation}
\Psi_{\omega,\theta}  \left(x,y\right) =
\frac{1}{2\pi\sigma_x\sigma_y}\cdot G_\theta\left( x,y\right)\cdot
S_{\omega,\theta}\left(x,y\right) \label{Eqn:GF}
\end{equation}
\begin{equation}
G_\theta\left( x,y\right) = \exp \left\{ - \left( \frac{\left(x \cos
\theta + y \sin \theta \right)^2}{2\sigma_x^2}+\frac{\left(-x \sin
\theta + y \cos \theta \right)^2}{2\sigma_y^2}\right)\right\}
\label{Eqn:G}
\end{equation}
\begin{equation}
S_{\omega,\theta}\left(x,y\right) = \left[\exp \left\{i\left(\omega
x \cos \theta + \omega y \sin \theta \right)\right\} - \exp \left\{-
\frac{\omega^2 \sigma^2}{2} \right\}\right] \label{Eqn:S}
\end{equation}

where,
\begin{itemize}
\item $G_\theta\left( x,y\right)$ represents a Gaussian Function.
\item $S_{\omega,\theta}\left(x,y\right)$ represents a Sinusoid Function.
\item $\left(x,y\right)$ is the spatial location where the filter is centered with respect to the image axis.
\item $\omega$ is the frequency parameter of a 2D Sinusoid.
\item $\sigma^2_{dir}$ represents the variance of the Gaussian (and thus the filter) along the specified direction.
$dir$ can either be $x$ or $y$. The variance controls the region
around the center where the filter has influence.
\end{itemize}

From the definition of Gabor filters, as given in Equation
\ref{Eqn:GF}, it is seen that the filters are generated by
multiplying two components: a Gaussian Function $G_\theta\left(
x,y\right)$ (Equation \ref{Eqn:G}) and a Sinusoid
$S_{\omega,\theta}\left(x,y\right)$ (Equation \ref{Eqn:S}). The
following discussions detail the two components of Equation
\ref{Eqn:GF}.

\subsubsection{Gaussian Function}
The 2D Gaussian function defines the spatial spread of the Gabor
filter. This spread is defined by the variance parameters of the
Gaussian, along the $x$ and $y$ direction together with the
orientation parameter $\theta$. Figure \ref{Fig:G}(a) shows a 3D
representation of the Gaussian mask generated with $\sigma_x = 10$
and $\sigma_y = 15$ and rotation angle $\theta = 0$. The image in
Figure \ref{Fig:G}(b) shows the region of spatial influence of an
elliptical mask on an image, where the variance in the $x$ direction
is larger than the variance in the $y$ direction.

\begin{figure}
\begin{center}
\includegraphics [width=2.3in] {GaussianDiffVar.eps}
\includegraphics [width=2.3in] {GaussianDiffVarImage.eps}
(a) \hspace{2.25in} (b) \caption{(a) 3D representation of a Gaussian
mask; $\sigma_x = 10$, $\sigma_y = 15$ and $\theta=0$
\newline (b)Image of the Gaussian mask $\sigma_x = 10$, $\sigma_y
= 15$ and $\theta=0$} \label{Fig:G}
\end{center}
\end{figure}


Typically the Gaussian filter has the same variance along both the
$x$ and $y$ directions, that is $\sigma_x = \sigma_y = \sigma$.
Under such conditions the rotation parameter $\theta$ does not play
any role as the spread will be circular.

\subsubsection{Sinusoid}
The 2D complex Sinusoid defined by Equation \ref{Eqn:S} generates
the two Sinusoidal components of the Gabor filters which (when
applied to an image) extracts the local frequency content of the
intensity variations in the signal. The complex Sinusoid has two
components (the real and the imaginary parts) which are two 2D
sinusoids that are phase shifted by $\frac{\pi}{2}$ radians. Figure
\ref{Fig:CS}(a) shows the 3D representation of a Sinusoidal signal
(either real or imaginary) at $\omega = 0.554$ radians and $\theta =
0$ radians, while Figure \ref{Fig:CS}(b) and \ref{Fig:CS}(c) show an
image of the real and imaginary parts of the same complex Sinusoid,
respectively. It can be seen that the two filters are similar,
except for the $\pi$ radian phase shift.

\begin{figure}
\begin{center}
\includegraphics [width=2in] {Sinusoid.eps}
\includegraphics [width=1.25in] {SinusoidRealImage.eps}
\includegraphics [width=1.25in] {SinusoidImagImage.eps} \newline
(a) \hspace{1.65in} (b) \hspace{1in} (c)

\caption{(a)3D representation of a Sinusoid $S_{\omega,\theta}$
\newline (b)Image representation of the real part of the complex
Sinusoid $\Re\left\{S_{\omega,\theta}\right\}$\newline (c)Image
representation of the imaginary part of complex Sinusoid
$\Im\left\{S_{\omega,\theta}\right\}$} \label{Fig:CS}
\end{center}
\end{figure}

Multiplying the Gaussian and the sinusoid generates the complex
Gabor filter, as defined in Equation \ref{Eqn:GF}. If
$\sigma_x=\sigma_y=\sigma$, then the real and imaginary parts of
this complex filter can be described as follows.

\begin{equation}
\Re\left\{\Psi_{\omega,\theta}  \left(x,y\right) \right\} =
\frac{1}{2\pi\sigma^2}\cdot G_\theta\left( x,y\right)\cdot
\Re\left\{S_{\omega,\theta}\left(x,y\right)\right\}
\end{equation}

\begin{equation}
\Im\left\{\Psi_{\omega,\theta}  \left(x,y\right) \right\} =
\frac{1}{2\pi\sigma^2}\cdot G_\theta\left( x,y\right)\cdot
\Im\left\{S_{\omega,\theta}\left(x,y\right)\right\}
\end{equation}

Figure \ref{Fig:GF}(a) shows the 3D representation of a Gabor filter
(either real or imaginary) at $\omega = 0.554$ radians, $\theta = 0$
radians, and $\sigma = 10$ and Figure \ref{Fig:GF}(b) and
\ref{Fig:GF}(c) show an image with the real and imaginary parts of
the complex filter.

\begin{figure}
\begin{center}
\includegraphics [width = 2in] {GaborFilter.eps}
\includegraphics [width = 1.25in] {GaborFilterRealImage.eps}
\includegraphics [width = 1.25in] {GaborFilterImagImage.eps}\newline
(a) \hspace{1.65in} (b) \hspace{1in} (c)


\caption{(a)3D representation of a Gabor filter
$\Psi_{\omega,\theta}$ \newline (b)Image representation of the real
part of Gabor filter  $\Re\left\{\Psi_{\omega,\theta}\right\}$
\newline (c)Image representation of the imaginary part of Gabor filter
$\Im\left\{\Psi_{\omega,\theta}\right\}$} \label{Fig:GF}
\end{center}

\end{figure}

In order to extract a Gabor feature at a location $\left(x,y\right)$
of an image $I$, the real and imaginary parts of the filter are
applied separately to the same location in the image, and a
magnitude is computed from the two results. Thus, the Gabor filter
coefficient at a location $\left(x,y\right)$ in an image $I$ with a
Gabor filter $\Psi_{\omega,\theta}$ is given by
\begin{equation}
C_{\Psi}\left(x,y\right) = \sqrt{\left(I \left(x,y\right) *
\Re\left\{\Psi_{\omega,\theta}\left(x,y\right)\right\}\right)^2 +
\left(I \left(x,y\right) *
\Im\left\{\Psi_{\omega,\theta}\left(x,y\right)\right\}\right)^2}
\label{Eqn:GaborCoeff}
\end{equation}

In our experiments, a {\it Gabor filter bank} was created by varying
three parameters of $\Psi_{\omega,\theta}$: (1) the frequency
parameter $\omega$, (2) the orientation parameter $\theta$, and (3)
the variance parameter $\sigma$. We chose five values for each of
these parameters thereby generating 125 different Gabor filters.

\begin{itemize}
\item $\omega = \left(2^{(-f+2)/2}\cdot \pi\right)$ where, $f =
\{0, 1, 2, 3, 4\}$ \item $\theta = \left(\frac{\pi}{2} \cdot \frac
{1} {5} \cdot t \right)$ where, $t = \{0, 1.25, 2.5, 3.75, 5\}$
\item $\sigma = \{5, 10, 15, 20, 25\}$
\end{itemize}

\section{The Learning Algorithm}
The proposed method uses the above described Gabor filters to find
distinguishing features (and corresponding feature locations) within
a face image. That is, for each person in the database, the
algorithm finds a set of Gabor filters which, when applied at their
corresponding $(x,y)$ locations within the image will produce
coefficients that are unique for that individual. This means that
all of the 125 Gabor filters in the filter bank are applied at each
and every location of each of the individual's face images, and then
tested for their ability to distinguish every individual. Given a
$128 \times 128$ face image, there will be $128 \times 128 \times
125 \times n$ filter coefficients that will be generated per face
image per person, where $n$ is the number of characteristic features
to be extracted for each person. This must be computed for every
person in the training set, which further increases the search
space. To search such a vast space of parameter values (the size of
the Gaussian mask, the frequency of the complex sinusoid, the
orientation of the entire Gabor filter, and the $(x,y)$ location
where the filter is placed) it is important that some scheme for
effective search be incorporated into the system. To this end, we
have chosen Genetic Algorithms to conduct the search. For each
person in the training set, all of the face images that depict to
that person are indexed as positives, while all of the other face
images in the database are indexed as negatives. Dedicated Genetic
Algorithm based search is conducted with these positive and negative
images, with the aim of finding a set of Gabor filters and filter
locations that distinguish all the positives from the negatives.

\subsection{Genetic Algorithms}
When the parameter space is vast (as it is in our case) a Genetic
Algorithm (GA) searches for the optimum solution by randomly picking
parameter sets and evolving newer ones from the best performers.
This happens over many generations, hopefully resulting in the
optimum set of parameters. To start the search, the GA generates a
random set of {\it parents}. Each parent is characterized by the
presence of a {\it chromosome}. The chromosome internally encodes
all the parameters that are used by the parent to perform the
intended operation. In our case, the intended operation is face
recognition. The parent uses the parameters that are found in its
chromosome to derive the Gabor features on the positive and negative
images.

Based on the ability of these features to distinguish a face from
all others in the database, the parent is ranked within its
population. This rank is also referred to as the {\it fitness of the
parent}. The ranking of all the parents, based on their fitness,
marks the end of a generation, and a new generation needs to be
created. New generations are formed based on three important aspects
of GAs, {\it Retention}, {\it Cross Over} and {\it Mutation}. A
portion of the newer generation is derived from the older
generation, using the above mentioned methods, and the rest of the
new generation is created randomly, maintaining the same overall
number of parents between generations. Once a new population has
been formed, the process of ranking parents occurs (as explained
earlier) and a new generation is born out of that ranking. This
iterative process continues until the parents in a certain
generation are fit enough to achieve the given task (with the
desired amount of success) or until the desired number of
generations have evolved.

\subsubsection{Use of Genetic Algorithms in Face Recognition}
GAs have been used in face recognition to search for optimal sets of
features from a pool of potentially useful features that have been
extracted from the face images. Liu et al ~\cite{Liu2002} used a GA
along with Kernel Principal Component Analysis (KPCA) for face
recognition. In their approach, KPCA was first used to extract
facial image features. After feature extraction using the KPCA, GAs
were employed to select the optimal feature subset for recognition -
or more precisely the optimal non-linear components. Xu et al
~\cite{Xu2004} used GAs along with Independent Component Analysis to
recognize faces. After obtaining all the independent components
using the Fast ICA algorithm, a genetic algorithm was introduced to
select optimal independent components.

Wong and Lam ~\cite{Wong1999} proposed an approach for reliable face
detection using genetic algorithms with eigenfaces. After histogram
normalization of face images and computation of eigenfaces, the 'k'
most significant eigenfaces were selected for the computation of the
fitness function. The fitness function was based on the distance
between the projection of a test image and that of the training-set
face images. Since GAs are computationally intensive, the search
space for possible face regions was limited to possible eye regions
alone.

Karungaru et al ~\cite{Karungaru2004} performed face recognition
using template matching. Template matching was performed using a
genetic algorithm to automatically test several positions around the
target, and to adjust the size of the template as the matching
process progressed. The template was a symmetrical T-shaped region
between the eyes, which covered the eyes, nose and mouth.

Ozkan ~\cite{Ozkan2006} used genetic algorithms for feature
selection in face recognition. In this work, the Scale Invariant
Feature Transform (SIFT) ~\cite{lowe_distinctive_2003} was used to
extract features. Since SIFT was originally designed for object
recognition in general, genetic algorithms were used to identify
SIFT features, which are more suitable to face recognition.

Huang and Weschler ~\cite{Huang1999} developed an approach to
identify eye location in face images using navigational routines,
which were automated by learning and evolution using genetic
algorithms. Specifically, eye localization was divided into two
steps: (i) the derivation of the saliency attention map, and (ii)
the possible classification of salient locations as eye regions. The
saliency map was derived using a consensus between navigation
routines that were encoded as finite state automata (FSA) exploring
the facial landscape and evolved using genetic algorithms (GAs). The
classification stage was concerned with the optimal selection of
features and the derivation of decision trees for confirmation of
eye classification using genetic algorithms.

Sun and Yin ~\cite{Sun2005} applied genetic algorithms for feature
selection in 3D face recognition. An individual face model was
created from a generic model and two views of a face. Genetic
algorithms were used to select optimal features from a feature space
composed of geometrical structures, the labeled curvature types of
each vertex in the individualized 3D model.

Sun et al ~\cite{sun2002} approached the problem of gender
classification using a genetic algorithm to select features. A
genetic algorithm was used to select a subset of features from a
low-dimensional representation, which was obtained by applying PCA
and removing eigenvectors that did not seem to encode information
about gender.

As is evident from these citations, many feature-based approaches
towards face recognition use genetic algorithms for feature
selection. However, these approaches employ a single feature space
derived from a set of face images. We believe that it is more
effective to employ aimed at extracting person-specific features,
and that an effective way to do this is by using genetic algorithms.
As observed by \cite{Turk2005}, humans initially learn to recognize
faces based on person-specific characteristic features. This
suggests that better recognition performance might be achieved by
representing each person's face in a person-specific feature space
that is learned using GAs.

The following paragraphs describe how we employed GAs to solve the
problem of finding person-specific Gabor features aimed at face
recognition.

\begin{figure}
\begin{center}
\includegraphics[width = 2in] {chromosome.eps}
\caption{A typical chromosome used in the proposed method.}
\label{Fig:Chromosome}
\end{center}
\end{figure}

\subsubsection{The Chromosome}
Each parent per generation encodes the parameters of a set of Gabor
filters in the form of a chromosome. In our implementation, each
Gabor filter is represented by five parameters. If there are $n$
Gabor filters, parameters for all of these filters are encoded into
the chromosome in a serial manner, as shown in Figure
\ref{Fig:Chromosome}. Thus the length of the chromosome is $5n$. The
number of Gabor filters being used per face image determines the
length of the chromosome. As shown in Figure \ref{Fig:Chromosome},
each parameter in the chromosome is encoded as a gene. The
boundaries of these genes defines the regions where the chromosome
undergoes both the crossover and mutation. The genes can be
considered as the primary element of the parent responsible in the
evolution.

\begin{figure}
\begin{center}
\includegraphics[width = 4in] {firstgeneration.eps}
\caption{Stages in the creation of the first generation of parents}
\label{Fig:FirstGen}
\end{center}
\end{figure}

\subsubsection{Creation of the first generation}
Figure \ref{Fig:FirstGen} depicts the first generation of parents,
which are created randomly. Each parent's chromosome is filled
randomly with parameter values where, each parameter value is within
the allowed range for that parameter. Thus, in our experiment, each
parent potentially has the parameters needed for it to perform face
recognition using Gabor filters for feature extraction.

\begin{figure}
\begin{center}
\includegraphics[width = 3in] {latergeneration.eps}
\caption{Deriving newer parents from the current generation}
\label{Fig:laterGen}
\end{center}
\end{figure}

Once these parents are created, each parent in the gene pool is
evaluated based on its capacity to perform face recognition. To this
end, a fitness function is defined, which takes into account the
ability of each parent to distinguish an individual from all others
based on the most distinguishing features on the individual's face.

This fitness function also takes into account the similarity of the
extracted features, and discourages the selection of features that
are highly correlated with each other. This ensures that the face
images will be searched for multiple distinguishing characteristics.
Subsection \ref{Sec:FF} explains in detail the fitness function used
in our experiments. The parents with the best fitness are ranked
higher, and have the highest probability of being picked for using
genetics the next generation. At the end of the rank ordering
process, the parents are arranged in a descending order, based on
their fitness. This rank ordering determines the probability of each
parent being used to create the subsequent generation. If a parent
has a higher fitness, it will have a higher probability of being
cloned into the next generation, or of otherwise being involved in
reproduction.

\subsubsection{Creation of the newer generations}
The newer generations are created from the older population using
{\it clones}, {\it mutants}, and {\it crossovers} of the fittest
parents. To better search for the optimal parameter set, new random
parents are created every generation. This reduces the likelihood
that the algorithm will get stuck in a local minimum in the search
space.

Figure \ref{Fig:laterGen} shows crossover creates a newer
generation, using the fittest parents from the older generation.

The number of offsprings created from mutation, cloning, and
crossover are determined by parameters of the Genetic algorithm. The
number of clones, mutants, and corssovers are controlled by the
following parameters:

\begin{enumerate}
\item {\it Cloning Rate} This parameter controls the number of
parents from the previous generation that will be retained without
undergoing any changes in their genetic structure.
\item{\it Crossover Rate} This parameter controls the number of offsprings that will be
born from crossing the parents from the previous generation.
\item {\it Mutation Rate} This parameter determines how many of the
crossed offsprings will then be mutated.
\item {\it Cloning Distribution Variance} After determining the number of offsprings be to cloned, the
index of the parents for cloning are chosen using a normal
distribution random number generator, with the mean zero and
variance equal to this parameter. Since the parents from the
previous generation have been rank ordered in descending order of
fitness, the zeroth parent will be the top performer (which
coincides with the mean of the random number generator, and has the
highest probability of getting picked).
\item{\it Crossover Distribution Variance} This parameter (which is
similar to the Cloning Distribution Variance) is used to choose the
index of the parents who will undergo Crossover.
\end{enumerate}


\begin{figure}
\begin{center}
\includegraphics[width = 4in] {crossover.eps}
\caption{Typical crossing of two parents to create an offspring}
\label{Fig:CrossOver}
\end{center}
\end{figure}


\subsubsection{{\it Crossover}}
As discussed earlier, the parents for crossover are selected by a
random number generator. Between these parents, the points of
crossover are determined by choosing locations of crossover
randomly. As seen in the Figure \ref{Fig:CrossOver}, these locations
are arbitrary gene boundary locations and at these locations the
gene content from the two parents gets mixed. The offspring thus
created now contains parts of the genes coming from the contributing
parents. The motivation for this step is the fact that, as more and
more generations pass, the fittest parents undergoing crossover will
already contain the better sets of parameters, and their crossing
might bring together the better sets of parameter values from both
the parents.

\begin{figure}
\begin{center}
\includegraphics[width = 4in] {mutation.eps}
\caption{Mutation of a newly created offspring} \label{Fig:Mutation}
\end{center}
\end{figure}

\subsubsection{{\it Mutation}}
In addition to the process of crossover at gene boundaries in the
chromosome, the values of some parameters within the genes might be
changed randomly. This is illustrated in the Figure
\ref{Fig:Mutation}. Such mutations help in exploring the local
parameter space more thoroughly. Mutations can be seen as small
perturbations to the larger search that explores the vast parameter
space, searching for the global minima.

\section{Methodology}
Most feature-based face recognition methods use feature detectors
that are not tailored specifically for face recognition, and they
make no attempt to selectively choose feature detectors based
specifically on their usefulness for face recognition. The method
described in this paper uses Gabor wavelets as feature detectors,
but evaluates the usefulness of each particular feature detector
(and a corresponding $(x,y)$ location) for distinguishing between
the faces within our face database. Given the very large number of
possible Gabor feature detectors and locations, we use a Genetic
Algorithm (GA) to explore the space of possibilities, with a fitness
function that propagates parents with a higher ability to
distinguish between the faces in the database. By selecting the
Gabor feature detectors and locations that are most useful for
distinguishing each person from all of the other people in the
database, we define a unique (i.e. person-specific) feature space
for each person.

\subsection{The FacePix (30) Database}
\label{sec:facepix}

All experiments were conducted with face images from the FacePix
(30) database ~\cite{BLACK2002}. FacePix(30) was compiled to contain
face images with pose and illumination angles annotated in 1 degree
increments. Figure \ref{fig-facepix} shows the apparatus that is
used for capturing the face images. A video camera and a spotlight
are mounted on separate annular rings, which rotate independently
around a subject seated in the center. Angle markings on the rings
are captured simultaneously with the face image in a video sequence,
from which the required frames are extracted.

\begin{figure}[h]
    \centering
        \includegraphics[scale=0.7]{facepix.eps}
    \caption{The data capture setup for FacePix(30)}
    \label{fig-facepix}
\end{figure}

This database has face images of 30 people across a spectrum of pose
and illumination angles. For each person in the database, there are
three sets of images. (1) The {\it pose angle set} contains face
images of each person at pose angles from +90º to –90º (2) The {\it
no-ambient-light set} contains frontal face images with a spotlight
placed at angles ranging from +90º to -90º with no ambient light,
and (3) The {\it ambient-light set} contains frontal face images
with a spot light placed at angles placed at angels from +90º to
-90º in the presence of ambient light. Thus, for each person, there
are three face images available for every angle, over a range of 180
degrees. Figure \ref{fig:facepiximages} provides two examples
extracted from the database, showing pose angles and illumination
angles ranging from -90º to +90º in steps of 10º. For earlier work
using images from this database, please refer ~\cite{Little:2005}.
Work is currently in progress to make this database publicly
available.

\begin{figure}[h]
    \centering
        \includegraphics[width = 4in]{facepiximages.eps}
    \caption{Sample face images with varying pose and illumination from the FacePix(30) database}
    \label{fig:facepiximages}
\end{figure}

We selected at random two images out of each set of three frontal
(0º) (Figure \ref{facepix}) images for training, and used the
remaining image for testing. The genetic algorithms used the
training images to find a set of Gabor feature detectors that were
able to distinguish each person’s face from all of the other people
in the training set. These feature detectors were then used to
recognize the test images.

\begin{figure}[h]
    \centering
        \subfigure[]{
          \label{fig-embeddingsa}
          \includegraphics[scale=0.7]{Img1.eps}}
     \hspace{.05in}
     \subfigure[]{
          \label{fig-embeddingsb}
          \includegraphics[scale=0.7]{Img2.eps}}
     \hspace{.05in}
     \subfigure[]{
          \label{fig-embeddingsc}
          \includegraphics[scale=0.7]{Img3.eps}}
    \caption{Sample frontal images of one person from the FacePix(30) Database}
    \label{facepix}
\end{figure}

In order to evaluate the performance of our system, we used the same
set of training and testing images with face classification
algorithm based on low-dimensional representation of face images
extracted through Principal Component Analysis
~\cite{sirovich_low-dimensional_1987}. Specifically, the performance
of the implementation of PCA-based face recognition followed by
~\cite{Pent1991} was used in our experiments.

\subsection{The Gabor Features}
Each Gabor feature corresponds to a particular Gabor wavelet (i.e. a
particular special frequency, a particular orientation, and a
particular Gaussian-defined spatial extent) applied to a particular
(x, y) location within a normalized face image. (Given that 125
different Gabor filters were generated, by varying $\omega$,
$\sigma$ and $\theta$ in 5 steps each, and given that each face
image contained $128 \times 128 = 16,384$ pixels, there was a pool
of $125 \times 16384 = 2,048,000$ potential Gabor features to choose
from.) We used an N-dimensional vector to represent each person's
face in the database, where N represents the predetermined number of
Gabor features that the Genetic Algorithm selected from this pool.
Figure \ref{fig:facedots} shows an example face image, marked with 5
locations where Gabor features will be extracted (i.e. N = 5). Given
any normalized face image, real number Gabor features are extracted
at these locations using Equation \ref{Eqn:GaborCoeff}. This process
can be envisioned as a projection of a 16,384-dimensional face image
onto an N dimensional subspace, where each dimension is represented
by a single Gabor feature detector.

\begin{figure}
\begin{center}
\includegraphics[scale = 0.5] {Res2.eps}
\caption{A face image marked with 5 locations where unique Gabor
features were extracted} \label{fig:facedots}
\end{center}
\end{figure}

Thus, the objective of the proposed methodology is to extract an N
dimensional real-valued person-specific feature vector to
characterize each person in the database. The N (x, y) locations
(and the spatial frequency and spatial extent parameters of the N
Gabor wavelets used at these locations) are chosen by a GA, with a
fitness function that takes into account the ability of each Gabor
feature detector to distinguish one face from all the other faces in
the database.

\subsection{The Genetic Algorithm}
Every GA is controlled in its progress through generations with a
few control parameters such as,
\begin{itemize}
\item the number of generations of evolution ($n_g$)
\item the number of parents per generation ($n_p$)
\item the number of parents cloned per generation ($n_c$)
\item the number of parents generated through cross over ($n_{co}$)
\item the number of mutations in every generation ($n_m$)
\end{itemize}

In our experiments, the GA used the following empirically-chosen GA
parameters: $n_g = 50$, $n_p = 100$, $n_c=6$, $n_{co}=35$ and
$n_m=5$.

\subsubsection{The Fitness Function \label{Sec:FF}}
The fitness function of a genetic algorithm determines the nature of
the search conducted over the parameter space. For face recognition
applications, the fitness function is the capacity of a parent to
classify the individuals accurately. In our proposed method, the
fitness function needs to take both the Gabor features and the
corresponding feature locations into consideration when evaluating
face classification. We define here a fitness function that has two
components to it. One determines the capacity of the parent to
isolate an individual's face image from the others in the database,
and the other evaluates whether the feature is redundant with other
extracted features (i.e. whether a feature detector produces
coefficients that are highly correlated with the coefficients
produced by another feature detector.) Thus the fitness $F$ can be
defined as

\begin{equation}
F = w_{D} D - w_{C} C \label{Eqn:Fitness}
\end{equation}

where $D$ is the distance measure weighted by $w_{D}$, and $C$
represents the correlation measure which measure the similarity
between the coefficients that have been extracted. The correlation
measure $C$ is weighted by the factor $w_{C}$.

If a parent extracts features from a face image that distinguish one
individual from all the others very well (compared to the other
parents within the same generation) then the distance measure $D$
will be the largest for that parent, making its fitness $F$ large.
If the correlation between the extracted features is small, $C$ will
be small, which also makes the fitness $F$ large. Thus, the
correlation measure serves as a {\it penalty} for extracting the
same feature from the face image multiple times, even though that
particular feature might be the best distinguishing feature on that
face.

The correlation between coefficients was used instead of spatial
separation to counter the problem of similar features being
extracted, because the Gabor filters might not be able to represent
the underlying image characteristic completely. If there are some
large image features on the face (such as beard) that require
multiple Gabor features within a certain spatial locality. Setting a
hard lower limit on this spatial separation might lead to
insufficient representation of that large image feature, in terms of
the Gabor filters.

Consider a parent searching for a unique set of $M$ Gabor filters to
distinguish one individual's face from all other faces. Let this set
of filters be referred to as $S$. Thus, $S = \left\{G_1, G_2,
\cdots, G_M \right\}$ where, $G_m$ represents the $m^{th}$ Gabor
filter.

If the set all individuals in the database is referred to as $I =
\left\{i_1, i_2, \cdots, i_j\right\}$ with $J$ number of
individuals, then for every individual $i$ in $I$ a set $S_i$ has to
be extracted. To achieve this, all the images in the database
depicting individual $i$ are marked as positives, and the ones not
depicting that individual are marked as negatives. Let the set of
positive images be referred to as $P_i$ (with $L$ number of images)
and the set of negatives be referred to as $N$ (with $K$ number of
images). Thus, $S_i = \left\{G_{1i}, G_{2,i}, \cdots, G_{mi}
\right\}$, $P_i = \left\{p_{1i}, p_{2i}, \cdots, p_{li}\right\}$ and
$N_i = \left\{ n_{1i}, n_{2i}, \cdots, n_{ki} \right\}$ are the sets
of Gabor filters, positive images and negatives images set
respectively for the individual $i$.

\begin{itemize}
\item {\bf The Distance Measure} $D$ \\
A parent trying to recognize an individual $i$ with a Gabor filter
set $S_i$ can be thought of as a transformation that projects all of
the face images from the image space to a $M$-dimensional space,
where the dimensions are defined by the $M$ Gabor filters in the set
$S_i$. Thus, all of the images in the two sets $P_i$ and $N_i$ can
be considered as points on this $M$-dimensional space. Since the
goal of the genetic algorithm is to find the set $S_i$ which best
distinguishes the individual $i$ from others, in our method we
search for the $M$ dimensional space (defined by a parent) that best
separates the points formed by the sets $P_i$ and $N_i$. Figure
\ref{Fig:Fitness} is an illustration of hypothetical set of face
images projected on a 2 dimensional space defined by a set of 2
Gabor filters $S_i = \left\{ G_0, G_1 \right\}$. As shown in the
figure, the measure $D$ is the minimum of all the Euclidian
distances between every positive and negative points.

Thus, $D$ can be defined as follow:
\begin{equation}
D = \min_{\forall \hspace{0.09in} l,k} \left[\delta_M
\left(\phi_M(p_{li}), \phi_M(n_{ki})\right)\right]
\end{equation}

where, \\
 $\delta_M (A, B) = \sqrt {(a_1 - b_1)^2 + (a_2 - b_2)^2 +
\cdots + (a_m - b_m)^2}$ is the $M$-dimensional Euclidian distance
between $A$ and $B$. $a_x$ and $b_x$ corresponds the $x^{th}$-coordinate of $A$ and $B$ respectively\\
$\phi_M(X)$ is the transformation function that projects image $X$
from the image space to the $M$-dimensional space defined by the set
of Gabor filters.

\begin{figure}
\begin{center}
\includegraphics[width = 3in] {fitness.ps}
\caption{Distance Measure $D$ for the fitness function}
\label{Fig:Fitness}
\end{center}
\end{figure}

\item {\bf The Correlation Measure} $C$ \\
In the proposed method, in addition to having every parent selecting
the Gabor filter set $S_i$ that can best distinguish the individual
$i$ from all the others in the database, it is necessary to ensure
that this set of Gabor filters does not include filters that extract
identical image features. If there were no such constraint, the
algorithm might find one very distinguishing image feature on the
face image and, over generations of evolution, all of its Gabor
filters might converge to this one image feature. To avoid this, the
correlation measure $C$ determines the correlation between the image
features extracted at all the locations pointed to by the
chromosome. To test for correlations between the Gabor features at
the different spatial locations, we use the entire set of 125 Gabor
filters to thoroughly characterize the textural context at these
locations.

Assuming that there are $M$ Gabor features that we are looking for
on the face image of individual $i$, let $(x_m, y_m), m = 1, 2,
\dots,M$ be the $M$ points that have been selected genetically in
the chromosome. To find the correlations of the image features
extracted at each of these points, the $N$ Gabor filters $G_i, i =
1, 2, \dots, N$ are used to characterize each of the points. Let the
coefficients of such a characterization be represented by a matrix
$A$. Thus, matrix A is $M \times N$ in dimension, where the rows
correspond to the $M$ locations and $N = 125$ refers to the Gabor
filter coefficients. Thus,

\begin{equation}
A = \left[ \begin{array}{cccc}
g_{(1,1)} & g_{(1,2)} & \ldots & g_{(1,N)}\\
g_{(2,1)} & g_{(2,2)} & \ldots & g_{(2,N)}\\
\vdots  & \vdots  & \vdots & \vdots \\
g_{(m,1)} & g_{(m,2)} & \ldots & g_{(m,N)}
\end{array} \right]
\end{equation}

where, $g_{(m,n)}$ is the coefficient obtained by applying the
$n^{th}$ Gabor filter to the image at the point $(x_m,y_m)$.

The Correlation measure can now be defined in terms of matrix $A$ as
follows

\begin{equation}
C = \log \left(\det \left( diag(B) \right) \right) - \log \left(
\det (B) \right) \label{Eqn:CorrelationMeasure}
\end{equation}

where, $diag (B)$ returns the diagonal matrix corresponding to $B$,
and $B$ is the covariance matrix defined by $B = \frac{1}{N - 1}
(AA^T)$.

Examining the Equation \ref{Eqn:CorrelationMeasure}, it can be seen
that the first log term gets closer to the second log term when the
off diagonal elements of B reduces. The diagonal elements of the
matrix $B$ corresponds to the variance of the $M$ image locations,
whereas the off diagonal elements correspond to the covariance
between pairs if locations. Thus, as the covariance between the
image points decreases, the value of the overall correlation
parameter decreases.

\item {\bf Normalization of} $D$ {\bf and} $C$ \\
In order to have an equal representation of both the Distance
measure $D$ and the Correlation term $C$ in the fitness function, it
is necessary to normalize the range of values that they can take.
For each generation, before the fitness values are used to rank the
parents, parameters $D$ and $C$ are normalized to range between $0$
and $1$.

\begin{eqnarray}
D_{norm} = \frac{D - D_{Min}}{D_{Max} - D_{Min}} \\
C_{norm} = \frac{C - C_{Min}}{C_{Max} - C_{Min}}
\end{eqnarray}

where, the $Max$ represents the maximum value of $D$ or $C$ in a
single generation across all the parents and $Min$ refers to the
minimum value.

\item {\bf Weighting factors} $w_{D}$ {\bf and} $w_{C}$ \\
The influence of the two components of the fitness function are
controlled by the weighting factors $w_{D}$ and $w_{C}$. We used the
relation $w_{C} = 1 - w_{D}$ to control the two parameters
simultaneously. With this relationship, a value of $w_{D} \approx 1$
will subdue the effect of the Correlation measure, causing the
genetic algorithm to choose the Gabor filters on the most prominent
image feature alone. On the other hand, $w_{D} \approx 0$ will
subdue the Distance measure, deviating the genetic algorithm from
the main goal of face recognition. Thus an optimal value for the
weight $w_{D}$ has to be estimated empirically, to suit the face
image database in question.

\end{itemize}

\section{Results}
To evaluate the relative importance of the two terms ($D$ and $C$)
in the fitness function, we ran the proposed algorithm on the
training set several times with 5 feature detectors per chromosome,
while changing the weighting factors in the fitness function for
each run, setting $w_D$  to 0, .25, .50, .75, and 1.00, and
computing $w_C = (1-w_D)$. Figure \ref{RES1} shows the recognition
rate achieved in each case.

\begin{figure}
\begin{center}
\includegraphics[width = 4in] {Result2.eps}
\caption{The recognition rate versus the number Gabor feature
detectors} \label{RES1}
\end{center}
\end{figure}


We then ran the proposed algorithm on the training set 5 times,
while changing the number of Gabor feature detectors per parent
chromosome for each run to 5, 10, 15, 20, and 25. In all the trials,
$w_D$=0.5. Figure \ref{RES2} shows the recognition rate achieved in
each case.


\begin{figure}
\begin{center}
\includegraphics[width = 4in] {Result1.eps}
\caption{Recognition rate with varying $w_D$}\label{RES2}
\end{center}
\end{figure}

\subsection{Discussion of Results}
Figure \ref{RES1} shows that the recognition rate of the proposed
algorithm when trained with 5, 10, 15, 20, and 25 Gabor feature
detectors increases monotonically, as the number of Gabor feature
detectors (N) is increased. This can be attributed to the fact that
increasing the number of Gabor features essentially increases the
number of dimensions for the Gabor feature detector space, allowing
for greater spacing between the positive and the negative clusters.

Figure \ref{RES2} shows that for N = 5 the recognition rate was
optimal when the distance measure D and the correlation measure C
were weighted equally, in computing the fitness function F. The dip
in the recognition rate for $w_D=0.75$ and $w_D=1.0$ indicates the
significance of using the correlation factor C in the fitness
function. The penalty introduced by C ensures that the GA searches
for Gabor features with different textural patterns. If no such
penalty were be imposed, the GA might select Gabor features that are
clustered on one salient facial feature, such as a mole.

The best recognition results for the proposed algorithm (93.3\%)
were obtained with 25 Gabor feature detectors. The best recognition
performance for the PCA algorithm was reached at about 15
components, and flattened out beyond that point, providing a
recognition rate for the same set of faces that was less than
83.3\%. This indicates that, for the face images used in this
experiment (which included substantial illumination variations) the
proposed method performed substantially better than the PCA
algorithm.

\subsection{Person-specific feature extraction}
When the FacePix(30) face database was built, all but one person
were captured without eyeglasses or a hat.  Figures
\ref{fig:result1}(a) and \ref{fig:result1}(b) show the results of
extracting 10 and 20 distinguishing features from that person's face
images. The important things to note about these results are:
\begin{enumerate} \item  At least half of the extracted
Gabor features (8 of the 10) and (10 of the 20) are located on (or
near) the eyeglasses. \item As the number of Gabor features was
increased from 10 to 20, more Gabor features are seen toward the
boundaries of the images. This is due to the fact that the genetic
algorithm chooses Gabor feature locations based on a Gaussian
probability distribution that is centered over the image, and
decreases toward the boundaries of the images.
\end{enumerate}


\begin{figure}[h]
    \centering
        \subfigure[]{
          \label{loga}
          \includegraphics[scale=0.2]{res1.eps}}
          \hspace{.05in}
     \subfigure[]{
          \label{logb}
          \includegraphics[scale=0.2]{res1b.eps}}
    \caption{10 and 20 person-specific features extracted for a particular individual in the database}
    \label{fig:result1}
\end{figure}

These results suggest that person-specific feature extraction might
be useful for face recognition in small face databases, such as
those typical of a social interaction assistance device for people
who are blind.

\section{Conclusions and Future Work}
As mentioned earlier, the proposed person-specific approach to
evolutionary feature selection in face images is well-suited for
applications such as those that enhance social interaction for
people who are blind, because people do not generally disguise their
appearance in normal social situations, and even when some
significant change occurs (such as a man shaving off his beard) the
system can continue to evolve as it captures new images with each
encounter.

A wearable social interaction assistant prototype has been
implemented using a pair of eyeglasses equipped with a tiny
unobtrusive video camera in the nose bridge ~\cite{Sreekar2005} and
is shown in Figure \ref{Susie}. The analog video output from this
camera is passed through a video digitizer, and the resulting
digital stream is then fed into a portable laptop computer. A video
stream is captured of any person standing in front of the
eyeglasses.  A face detection algorithm, based on Adaboost ~\cite{
violajones01}, is then used to identify the frames of the video
where a face is present, and to localize that face within that
frame. This detected face is then cropped and compared to indexed
faces in a face database.

\begin{figure}[h]
    \centering
        \includegraphics[scale=0.7]{susie.eps}
        \caption{Wearable face recognition platform}
    \label{Susie}
\end{figure}


The performance of the proposed approach for identifying
person-specific features relies, to a large extent, on obtaining
near-frontal views of faces.  To offset this limitation, there is
ongoing work ~\cite{ Balasubramanian2007} to perform
person-independent head pose estimation on the face images obtained
from this platform. It is expected that this will help us select
face images from the video stream with near-frontal views, which
will improve the performance of our algorithm in identifying
person-specific features.

Another factor that limits the performance of our algorithm is
illumination variations in the captured images. Especially
problematic are variations between outdoor-indoor and day-night
settings. (Of course, this limitation is not unique to our
algorithm.) As a strategy to provide additional light unobtrusively
under adverse lighting conditions, we are employing infra-red LED
illuminators in conjunction with an infrared-sensitive camera.

In summary, while there have been many different feature-based
approaches to face recognition over the last two decades of
research, we have proposed a novel methodology based on the
discovery and extraction of person-specific characteristic features
to improve face recognition performance for small face databases.
This approach is aimed at facilitating social interaction in casual
settings. The use of Gabor features, in tandem with a genetic
algorithm to discover characteristic person-specific features has
been inspired by the human visual system and is based on knowledge
that has been developed about the process by which humans recognize
faces.  We believe that more needs to be learnt about human face
recognition, and that as more is learnt, the knowledge can be put to
use to develop more robust face recognition algorithms.


\chapter{EXOCENTRIC SENSING}
\DoubleSpacing
\setlength{\parindent}{.5in}

In behavioral psychology, influences of interpersonal distances on social interactions between people have been studied for over four decades. The term proxemics, coined by Edward T. Hall, describes influence of interpersonal distances in animal and man \cite{hall_hidden_1990}. The following list describes the American proxemic distances; note that such distances vary with culture and environment.

\begin{enumerate}[1.]
\item Intimate Distance (Close Phase): 0-6 inches
\item Intimate Distance (Far Phase): 6-18 inches
\item Personal Distance (Close Phase): 1.5-2.5 feet
\item Personal Distance (Far Phase): 2.5-4 feet
\item Social Distance (Close Phase): 4-7 feet
\item Social Distance (Far Phase): 7-12 feet
\item Public Distance (Close Phase): 12-25 feet
\item Public Distance (Far Phase): 25 feet or more
\end{enumerate}

Proxemics plays a very important role in interpersonal communication, but people who are blind and visually impaired do not have access to this information. In \cite{ram_people_1998}, Ram and Sharf introduced The People Sensor: an electronic travel aid, for individuals who are blind, designed to help detect and localize people and objects in front of the user. The distance between the user and an obstacle is found using ultrasonic sensors and communicated through the rate of short vibratory pulses, where the rate is inversely proportional to distance. However, the researchers did not do any user testing to determine the usefulness of their technology. Similar to this system, our technology uses the haptic belt described in Chapter 2 for delivering the proxemics information to an individual who is blind or visually impaired.

Tactile rhythms delivered using a vibrotactile belt were used in \cite{erp_waypoint_2005} to convey distance information during waypoint navigation. Time between vibratory pulses was varied using one of two schemes: monotonic (rate is inversely proportional to distance) or three-phase-model (three distinct rhythms mapped to three distances). Distinct tactile rhythms are promising for use with multidimensional tactons \cite{barralon_development_2007} \cite{brown_first_2005}, which are vibratory signals used to communicate abstract messages \cite{brown_first_2005} by changing the dimensions of the signal including frequency, amplitude, location, rhythm, etc. Based on pilot test results, we chose to pursue distinct rhythms over monotonic rhythms as users find it difficult to identify interpersonal distances using monotonic rhythms as the vibratory signal varies smoothly with changes in distance.

From the sensing perspective we resort to the camera that is on the user's glasses and through the use of computer vision technology, face detection, we extract non-verbal cues for social interaction, including the number of people in the user's visual field, where people are located relative to the user, coarse information related to gaze direction (pose estimation algorithms could be used to extract finer estimates of pose), and the approximate distance of the person from the user based on the size of the face image.

\section{Conceptual Framework}
As shown in Figure 1, the output of the face detection process (indicated by a green rectangle on the image) provided by the Social Interaction Assistant is directly coupled with the haptic belt. Every frame in the video sequence captured by the Social Interaction Assistant is divided into 7 regions. After face detection, the region to which the top-left corner of the face detection output belongs is identified (as shown by the star in Figure 3). This region directly corresponds to the tactor on the belt that needs to be activated to indicate the direction of the person with respect to the user. To this end, a control byte is used to communicate between the software and the hardware components of the system. Regions 1 through 7 are coded into 7 bits on the parallel port of a PC. Depending on the location of the face image, the corresponding bit is set to 1. The software also controls the duration of the vibration by using timers. The duration of a vibration indicates the distance between the user and the person in his or her visual field. The longer the vibration, the closer the people are, which is estimated by the face image size determined during the face detection process.

An overall perspective of the system and its process flow is given below. When a user encounters a person in his or her field of view, the face is detected and recognized (if the person is not in the face database, the user can add it). The delivery of information comprises two steps: Firstly, the identity of the person is audibly communicated to the user (we are currently investigating the use of tactons \cite{brewster_tactons:_2004} to convey identities through touch, but this is part of future work). Secondly, the location of the person is conveyed through a vibrotactile cue in the haptic belt, where the location of the vibration indicates the direction of the person and the duration of vibration indicates the distance between the person and the user. Based on user preference, this information can be repeatedly conveyed with every captured frame, or just when the direction or distance of the person has changed. The presence of multiple people in the visual field is not problematic as long as faces are not occluded and can be detected and recognized by the Social Interaction Assistant. We are currently investigating how to effectively and efficiently communicate non-verbal communication cues when the user is interacting with more than one person.

**********************************
In this chapter we introduce the sensing and the delivery end of the system that can deliver proxemics information to an individual who is blind or visually impaired. From the sensing end, we describe a face detection methodology that is capable of identifying exact boundaries of the face region through which we model the distance of the interaction partner from the person who is using the device. From the delivery end, we describe user tests that were conducted to determine the use of tactons for conveying direction and distance information.
**********************************



\section{Accurate Face Detection}\label{Introduction} \vspace{-0.2in} Face
detection has become an important first step towards solving
plethora of other computer vision problems like face recognition,
face tracking, pose estimation, intent monitoring and other face
related processing. Over the years many researchers have come up
with algorithms, that have over time, become very effective in
detecting faces in complex backgrounds. Currently, the most popular
face detection algorithm is the Viola-Jones \cite{viola_robust_2004}
face detection algorithm whose popularity is boosted of by its
availability in the open source computer vision library, OpenCV.
Other popular face detection algorithms are identified in
\cite{hjelmýs_face_2001} and \cite{ming-hsuan_yang_detecting_2002}.

Most face detection algorithms learn faces by modeling the intensity
distributions in upright face images. These algorithms tend to
respond to face-like intensity distributions in image regions that
do not depict any face as they are not contextually aware of the
presence or absence of a human face. These spurious responses make
the results unsuitable for further processing that requires accurate
face images as inputs, such as the ones mentioned above. Figure
\ref{Fig:ExampleFalseDetect} shows an example where a face detection
algorithm detects two faces - one true and the other false.

\begin{figure}[h]
\centering
\includegraphics[width=3in]{Figure1.jpg}
\caption{An example false face detection.} \label{Fig:ExampleFalseDetect}
\end{figure}

The problem of false face detection has motivated some researchers
to develop heuristic approaches aimed for validating the face
detection results. Most of these heuristics integrate primitive
context into the problem by searching for skin tone in the output
subimages. However, this simple approach often fails to distinguish
faces from non-faces, because face detectors often fail to center
the cropping box precisely around the detected face. This produces a
significant patch of skin colored pixels, but only a partial face.
This centering problem can be dealt with by extracting the skin
colored regions and comparing their shape to an ellipse. While such
heuristics, are simple, and somewhat effective, their validation is
not reliable enough to meet the needs of higher level face
processing tasks. Further, they do not provide a confidence metric
for their validation.


\begin{figure}[h]
\centering
\includegraphics[width=3.5in]{Figure2.jpg}
\caption{Block diagram.}
\label{Fig:BlockDiagram}
\end{figure}

This paper treats the problem of face detection validation in a
systematic manner, and proposes a learning framework that
incorporates both contextual and structural knowledge of human
faces. A face validation filter is designed by combining two
statistical modelers, 1) a human skin-tone detector with a dynamic
background modeler (Module $1$), and 2) an evidence-aggregating
human face silhouette random field modeler (Module $2$), which
provides a confidence metric on its validation task. The block
diagram in Figure \ref{Fig:BlockDiagram} shows the functional flow
of data through the two modules in the proposed framework. The
details of the statistical models and their learning will be
presented later in the paper, which is organized as follows. Section
2 reviews some of the earlier research. Section 3 introduces the
proposed framework, with details on the learning process. Section 4
discusses the experiments carried out to test the proposed
framework. Section 5 presents the results while Section 6 discusses
them. Section 7 concludes the paper and discusses future work.

%-------------------------------------------------------------------------
\section{Related Work}\label{RelatedWork} \vspace{-0.2in} As
mentioned earlier, the problem of face detection validation has not
been treated methodically before, though the problem has been
handled by many as an integral component of face detection
algorithms. All the past work in this area can be broadly
characterized into two groups: a) Low level image feature models
mostly based on skin color such as \cite{a_hadid_hybrid_2006},
\cite{naseem_robust_2005} and \cite{m_wimmer_person_2006}, and b)
High level facial feature models such as \cite{hmid_fuzzy_2006},
\cite{tariq_face_2004} and \cite{yan-wen_wu_face_2008}.

The low level skin color based approaches try to reduce
computational complexity by first identifying skin color in images
so that search can be reduced. Most of the times, simple geometrical
properties of the retained skin regions are used to determine if the
region is a face. Such simplification of faces into trivial
geometrical structures results in false detections. The facial
feature based methods achieve face detection by individually
identifying the integral components of a face image such as eyes,
nose, etc. Though these schemes could be robust, the associated
computational load is high. Interested readers could find more
related references in \cite{ming-hsuan_yang_detecting_2002} and
\cite{hjelmýs_face_2001}. The framework proposed in this paper uses
statistically learnt knowledge about human faces to overcome
computational complexity thereby augmenting face validation to
existing face detection algorithms seamlessly.

%-------------------------------------------------------------------------
\section{Proposed Framework}\label{ProposedFramework}
\vspace{-0.2in} As shown in Figure \ref{Fig:BlockDiagram}, the
framework essentially has two statistically learnt models, Module
$1$ and Module $2$, that are cascaded to form the face detection
validation filter. The output from a face detector is sent to Module
$1$, which distinguishes the skin pixels in the face region from the
background pixels, thereby constucting a skin region mask. This skin
region mask then becomes the input to Module $2$, which is
essentially an aggregate of random field models learnt from manually
labeled ({\emph true}) face detection outputs. The results of each
random field model within the aggregate are then combined, using
rules of Dempster-Shafer Theory of Evidence
\cite{sentz_combination_2002}. This {\emph combining of evidence}
provides a metric for the belief (i.e. confidence) of the system in
its final validation. The two modules are detailed in the following
subsections.
%-------------------------------------------------------------------------
\subsection{Module $1$: Human Skin Tone Detector with Dynamic
Background Modeler}\label{Module1} \vspace{-0.2in} Most of the skin
tone detectors used for human skin color classification use prior
knowledge, which is provided in the form of a parametric or
non-parametric model of skin samples that are extracted from images
- either manually, or through a semiautomated process. In this paper
we employ such an a priori model, in combination with a dynamic
background modeler, so that the skin vs. non-skin boundary is
accurately determined. Accurate skin region extraction is essential
for Module $2$, as it validates images based on their structural
properties. The two functional components of Module $1$ are:

\subsubsection{{\em a-priori} Bi-modal Gaussian Mixture Model
for Human Skin Classification}\label{Bi-ModGaussian}
\vspace{-0.1in}A normalized RGB color space has been a popular
choice among researchers for parametric modeling of human skin
color. The normalized RGB (typically represented as nRGB) of a pixel
$X$ with $X_r$, $X_g$, $X_b$ as its red, green and blue components
respectively, is defined as:
\begin{equation}
X_{i|i \in \{r,g,b\}}^{nRGB} =
\frac{X_i}{\left(\sum\limits_{\forall_{i|i\in\{r,g,b\}}}X_i\right)}
\end{equation}
Normalized RGB space has the advantage that only two of the three
components, nR, nG or nB, is required at any one time to describe
the color. The third component can be derived from the other two as:
\begin{equation}
X_{i|i \in \{nR,nG,nB\}}^{nRGB} =  1 -\left(
\sum_{\forall_{k|(k\in\{nR,nG,nB\}, k \ne i)}}X_k \right)
\end{equation}

\vspace*{-0.3in}

\begin{figure}[h]
\centering
\includegraphics[width=3in]{Figure3.jpg}
\caption{Skin pixels in nRGB space.} \label{Fig:nRGBProject}
\end{figure}

\vspace{0.1in} In our experiments, we found that skin pixels form a
tight cluster when projected on nG and nB space as shown in the
Figure \ref{Fig:nRGBProject}. The study was based on a skin pixel
database, consisting of nearly $150,000$ samples, built by randomly
sampling skin regions from $1040$ face images collected on the web
as well as from FERET face database \cite{phillips_feret_1997}.
Further analysis also showed that the cluster formed on the 2D nG-nB
space had two prominent density peaks which motivated the modeling
of skin pixels with a Bi-modal Gaussian mixture model learnt using
Expectation Maximization (EM) with a $k$-means initialization
algorithm \cite{bilmes_gentle_1998}. The Bi-modal Gaussian mixture
model is represented as.
\begin{eqnarray}
f^{skin}_{X|X=[nG,nB]}(x) & = & w_1 f_{Y_1}(x;\Theta_1=[\mu_1,\Sigma_1]) +  \nonumber  \\
       &  & w_2 f_{Y_2}(x;\Theta_2=[\mu_2,\Sigma_2])
\end{eqnarray}

\subsubsection{Dynamically Learnt Multi-modal Gaussian Model for
Background Pixel Classification}\label{DynamicModel} \vspace{-0.1in}
As mentioned earlier, classification of regions into face or
non-face requires accurate skin vs. non-skin classification. In
order to achieve this, we learn the background color surrounding
each face detector output dynamically. To this end we extract an
extra region of the original image around the face detector's
output, as shown in Figure \ref{Fig:Extraregion}. Since the size of
the face detector output varies from image to image, it is necessary
to normalize the size. This is done by downsampling the size of the
original image to produce a face detector output region containing
$90$x$90$ pixels. The extra region pixels surrounding the face are
then extracted from the $100$x$100$ region around this $90$x$90$
normalized face region.

\begin{figure}[h]
\centering
\includegraphics[width=2.5in]{Figure4.jpg}
\caption{Extra region for background modeling.} \label{Fig:Extraregion}
\end{figure}

Once the outer pixels are extracted, a Multi-modal Gaussian Mixture
is trained using EM with $k$-means initialization, similar to the
earlier case with skin pixel model. The resultant model can be
represented as.
\begin{equation}
f^{non-skin}_{X|X=[R,G,B]}(x) =
\sum\limits_{i=1}^{m}w(i)f_{Y_i}\left(x;\Theta_i=[\mu_i,\Sigma_i]\right)
\end{equation}
where, $m$ is the number of mixtures in the model. We found
empirically that a value of $m=2$ or $m=3$ modeled the backgrounds
with sufficient accuracy.

\subsubsection{Skin and Background Classification using the learnt
Multi-modal Gaussian Models}\label{SkinnBackground} \vspace{-0.1in}
The skin and non-skin models, $f^{skin}_{X|X=[nG,nB]}(x)$ and
$f^{non-skin}_{X|X=[R,G,B]}(x)$ respectively, are used for
classifying every pixel in the scaled face image obtained as
explained in the Section \ref{DynamicModel}. Example skin-masks are
shown in Figure \ref{Fig:Skinmasks}. This example shows two sets of
images - one corresponding to a {\emph true} face detection result,
and another {\emph false} face detection result.

\begin{figure}[h]
\centering
\includegraphics[width=4in]{Figure5.jpg}
\caption{Example of {\em true} and {\em false} face detection.} \label{Fig:Skinmasks}
\end{figure}

The structural analysis through Random Field models explained in the
next section will describe the design concepts that will help
distinguish between {\emph true} and {\emph false} face detections shown
in Figure \ref{Fig:Skinmasks}.
%-------------------------------------------------------------------------
\subsection{Module $2$: Evidence-Aggregating Human Face Silhouette
Random Field Modeler}\label{Module2} \vspace{-0.2in}
 In order to validate the skin region
extracted as explained in Section \ref{Module1}, we build
statistical models from examples of faces. We developed statistical
learners inspired by Markov Random Fields (MRF) to capture the
variations possible in {\emph true} skin masks (face silhouette). The
following subsections describes MRF models and the variant we
created for our experiments.

\subsubsection{Random Field (RF) Models}\label{MRF} \vspace{-0.1in}
In this work, we used a minor variant of MRFs to learn the structure
of a {\emph true} face skin mask. MRFs encompass a class of
probabilistic image analysis techniques that rely on modeling the
intensity variations and interactions among the image pixels. MRFs
have been widely used in low level image processing including, image
reconstruction, texture classification and image segmentation
\cite{perez_markov_1998}.

In an MRF, the sites in a set, $\mathcal S$, are related to one
another via a neighborhood system, which is defined as ${\mathcal
N}=\{{\mathcal N}_i, i \in \mathcal S\}$, where ${\mathcal N}_i$ is
the set of sites neighboring $i$, $i \notin {\mathcal N}_i$ and $i
\in {\mathcal N}_j \Longleftrightarrow j \in {\mathcal N}_i$.

A random field X said to be an MRF on $\mathcal S$ with respect to a
neighborhood system $\mathcal N$, if and only if,
\begin{eqnarray}&& P({\mathbf x})>0, \forall \mathbf x \in \mathcal X  \\ && P(x_i\vert x_{{\mathcal S}-\{i\}})=P(x_i\vert x_{{\mathcal N}_i}) \label{Eqn:5} \end{eqnarray}
where, $P(x_i\vert x_{{\mathcal S}-\{i\}})$ represents a Local
Conditional Probability Density function defined over the
neighborhood $\mathcal N$. The variant of MRF that we created for
our experiments relaxed the constraints imposed by MRFs on $\mathcal
N$. Typically, MRFs requires that sites in set $\mathcal S$ be
contiguous neighbors. The relaxation in our case allows for distant
sites to be grouped into the same model.

We empirically found out that modeling the skin-region validation
problem into one single RF gave poor results. We devised $5$ unique
RF models with an Dempster-Shafer Evidence aggregating framework
that could not only validate the face detection outputs, but also
provide a metric of confidence. Thus, Equation \ref{Eqn:5} could be
alternatively seen as a set $P({\mathbf x}) = \{P^1({\mathbf x}),
\ldots, P^5({\mathbf x})\}$, each having their own neighborhood
system $\mathcal N^k = \{\mathcal N^1, \mathcal N^2, \ldots,
\mathcal N^5\}$, such that
\begin{equation}
P^k(x_i\vert x_{{\mathcal S}-\{i\}})=P(x_i\vert x_{{\mathcal
N^k_i}})
\end{equation}

\subsubsection{Pre-processing}\label{Preprocessing} \vspace{-0.1in} As described earlier,
each face detector output is normalized and expanded to produce a
$100$x$100$ pixel image, from which a binary skin mask is generated.
A morphological opening and closing operation is then performed on
the skin mask (to eliminate isolated skin pixels), and the mask is
then partitioned into one hundred $10$x$10$ blocks, as shown in
Figure \ref{Fig:RowColumnBlocks}. The number of mask pixels (which
represent skin pixels) are counted in each block, and a $10$x$10$
matrix is constructed, where each element of this matrix could
contain a number between 0 and 100. This $10$x$10$ matrix is then
used as the basis for determining whether the face detector output
is indeed a face.

\begin{figure}[h]
\centering
\includegraphics[width=4in]{Figure6.jpg} \caption{Pre-processing.}
\label{Fig:RowColumnBlocks}
\end{figure}

\subsubsection{The Neighborhood System}\label{Neighborhood} \vspace{-0.1in} The determination of whether the
face detector output is actually a face is based on heuristics that
are derived from anthropological human face models
\cite{vezjak_anthropological_1994} and through our own statistical
analysis. These include:
\begin{enumerate}
\item Human faces are horizontally symmetrical (i.e. along any row of blocks $R_i$)
about a  central vertical line joining the nose bridge, the tip of
the nose and the chin cleft, as shown in Figure
\ref{Fig:RowColumnBlocks}.  In particular, our analysis of a large
set of frontal face images showed that the counts of skin pixels in
the 10 blocks that form each row in Figure \ref{Fig:RowColumnBlocks}
were roughly symmetrical across this central line.

\item The variations along the verticals ($C_i$'s) are negligible enough
that in building a Local Conditional Probability Density function,
each $R_i$ can be considered independent of the other. That is, for
example, modeling variations of $C_0$ w.r.t $C_1$ on $R_1$ is
similar to modeling variations of $C_0$ w.r.t $C_1$ on any other
$R_{i|i\ne1}$. Thus, analysis of Local Conditional Probability could
be restricted to single $R_i$ at a time, as shown in Figure
\ref{Fig:Neighborhood}.
\end{enumerate}

\begin{figure}[h]
\centering
\includegraphics[width=4in]{Figure7.jpg} \caption{Neighborhood System.}
\label{Fig:Neighborhood}
\end{figure}

The different neighborhood systems $\mathcal N^k$, used in the RF
models, $P^k(x\vert x_{\mathcal N^k})$, can be defined as (Refer
Figure \ref{Fig:Neighborhood}):
\begin{equation}
\mathcal N^k = \left\{C_{j | j \in \{|k|, 0^-, 0^+\}}\right\}
\end{equation}

\subsubsection{Local Conditional Probability Density (LCPD)}\label{LCPD} \vspace{-0.1in} To model the
variations on the skin-region mask, we choose to build 2D histogram
for each of the $5$ RF over their unique neighborhood system. The
design of the dimensions were such that they captured the various
structural properties of {\emph true} skin masks. The two dimensions
(represented in a histogram pool ${\mathbf H}^k$) with individual
element of the pool, ${\mathbf z}$, can be defined as:
\begin{itemize}
\item $\mathbf{H}^{k|k=\{1,2,3,4\}} = \left\{ \mathbf{z}\right\}$, where,
\begin{equation}\mathbf{z} = [x_{C_{0^{\pm}}}, \delta(x_{C_{0^{\pm}}},
x_{C_{\pm k}})] , \forall R_{j} \label{Eqn:9}
\end{equation}

\item$\mathbf{H}^{k=5} = \left\{
\mathbf{z}\right\}$, where,
\begin{equation}
\mathbf{z} = [\mu(x_{C_{0^+}},x_{C_{0^-}}), \mu(x_{C_{-4}},
x_{C_{+4}})] , \forall R_{j} \label{Eqn:10}
\end{equation}
\end{itemize}

where, $x_{C_k}$ is the count of skin pixels in the block $C_k$. The
two functions $\delta(.,.)$ and $\mu(.,.)$ are defined as
\begin{eqnarray}
\delta(x_{C_{0^{\pm}}}, x_{C_{\pm i}}) & = & \left\{
\begin{array}{l l}
x_{C_{0^+}} - x_{C_{+i}}, & i>0 \\
x_{C_{-i}} - x_{C_{0^-}}, & i<0 \\
\end{array}
\right. \\
\mu(a,b) & = & \frac{a+b}{2}
\end{eqnarray}
In order to estimate the LCPD on these $5$ histogram pools, we use
Parzen Window Density Estimation (PWDE) technique, similar to
\cite{paget_texture_1997}, with a 2D Gaussian window. Thus, each of
LCPD can now be defined as
\begin{equation}
\begin{array}{l}
 \hspace*{-0.15in}P^k(\mathbf z) = \frac{1}{(2\pi)^{\frac{d}{2}} n h_{opt}^d} \sum\limits_{j=1}^n
 \exp \\     \\
\hspace{1in} \left[-\frac{1}{2 h^2_{opt}} \left(\mathbf{z} - \mathbf
{H}^k_j\right)^T \Sigma^{-1} \left( \mathbf{z} - \mathbf
{H}^k_j\right) \right]    \nonumber
\end{array}
\end{equation}

where, $n$ is the number of samples in the histogram pool
$\mathbf{H}^k$, $d$ is number of dimensions (in our case $2$),
$\Sigma$ and $h_{opt}$ are the covariance matrix over $\mathbf{H}^k$
and the optimal window width, respectively, defined as:
\begin{eqnarray}
\Sigma = \left[\begin{array}{cc} \sigma_{1} & 0 \\ 0 &
\sigma_{2}\end{array} \right], & h_{opt} =
\frac{\sigma_{1}+\sigma_{2}}{2} \left\{\frac{4}{n(2d+1)}
\right\}^{1/(d+4)}          \nonumber
\end{eqnarray}
Figure \ref{Fig:LCPDs} shows the $5$ LCPDs learnt over a set of
$390$ training frontal face images.

\begin{figure}[h]
\centering
\includegraphics[width=4in]{Figure8.jpg}
\caption{Frontal face Local Conditional Probability Density (LCPD) models.} \label{Fig:LCPDs}
\end{figure}


\subsubsection{Human Face Pose}\label{HumanFacePose} \vspace{-0.1in} During our studies we discovered that
the structure of the skin-region varies based on the pose of
detected face as shown in Figure \ref{Fig:PoseMasks}. Combining face
examples from different pose into one set of RFs seemed to dilute
the LCPDs and hence the discriminating capability. This motivated us
to design three different sets of RFs, one for each pose. This was
accomplished by grouping {\emph true} face detections into three
piles, Turned right ($r$), Facing front ($f$), and, Turned Left
($l$).

\begin{figure}[h]
\centering
\includegraphics[width=4in]{Figure9.jpg}
\caption{Skin-region masks.}
\label{Fig:PoseMasks}
\end{figure}

Thus, the final set of LCPDs could be described by the super set.
\begin{equation}
P(\mathbf z) = \left\{P^{k|k=\{1,\ldots,5\}}_{m|m=\{r,f,l\}}(\mathbf
z) \right\}
\end{equation}

\subsection{Combining Evidence}\label{CombiningEvidence}
\vspace{-0.2in} Given any test face detection output, $\mathbf{z}$
is extracted (as described in Equation \ref{Eqn:9} and \ref{Eqn:10})
and projected on the LCPD set $P(\mathbf z)$ to get a set of
likelihoods $l_m^k$. As in the case of any likelihood analysis, we
combined the joint likelihood of multiple projections using
log-likelihood function, $L_m^k = \ln \left(l_m^k\right)$, such
that,
\begin{equation}
\prod\limits_{\forall {\mathbf z} \in {\mathbf H}^k_m}\ln
\left(l_m^k(\mathbf z)\right) =  \sum\limits_{\forall {\mathbf z}
\in {\mathbf H}^k_m}L_m^k(\mathbf z)
\end{equation}
Given these log-likelihood values, one can set hard thresholds on
each one of them to validate a face subimage discretely as {\emph
true} or {\emph false}. We incorporated a piece-wise linear decision
model (soft threshold) instead of a hard threshold on the acceptance
of a face subimage. This is illustrated in the Figure
\ref{Fig:Thresholds}. Each LCPD $P^k(\mathbf z)$ was provided with
an upper and lower threshold of acceptance and rejection
respectively. The upper and lower bounds were obtained by observing
$P^k(\mathbf z)$ for the three face poses $P^k_{r,f,l}(\mathbf z)$.
Thus, any log-likelihood values lesser than the lower threshold
($L_L$) would result in a decision against the test input
(Probability $0$), while any log-likelihood value greater that the
upper threshold ($L_U$) would be a certain accept (probability $1$).
Anything in between would be assigned a probability of acceptance.
\begin{figure}[h]
\centering
\hspace{-0.3in}\includegraphics[width=2.5in]{Figure10.jpg}
\caption{Soft threshold.}
\label{Fig:Thresholds}
\end{figure}
In order to combine the decisions from the five LCPD $P^k(\mathbf
Z)$, we resort to Dempster-Shafer Theory of Evidence.

\subsubsection{Dempster-Shafer Theory of Evidence (DST)}\label{DST}
\vspace{-0.1in} The Dempster-Shafer theory is a mathematical theory
of evidence \cite{sentz_combination_2002} which is a generalization
of probability theory with probabilities assigned to sets rather
than single entities.

If $X$ is an universal set with power set, $\mathbf{P}(X)$ (Power
set is the set of all possible sub-sets of $X$, including the empty
set $\emptyset$), then the theory of evidence assigns a belief mass
to each subset of the power set through a function called the basic
belief assignment (BBA), $m:\mathbf{P}(X) \rightarrow [0,1]$, when
it complies with the two axioms. a) $m(\emptyset) = 0 $ and b)
$\sum\limits_{\mathbf{A} \in \mathbf{P}(X)} m(\mathbf{A})= 1$. The
mass, $m(A)$, of a given member of the power set expresses the
proportion of all relevant and available evidence that supports the
claim that the actual state belongs to $A$ and to no particular
subset of $A$. In our case, $m(A)$ correlates to the probability
assigned by each of  LCPDs towards the subimage being a face or not.

The true use of DST in our application becomes clear with the {\emph
rules of combining evidences} which was proposed as an immediate
extension of DST. According to the rule, the combined mass
(evidence) of any two expert's opinions, $m_1$ and $m_2$, can be
represented as:
\begin{equation}
m_{1,2}(A) = \frac{1}{1-K}\sum\limits_{B\cap C = A, A \ne
\emptyset}m_1(B) m_2(C) \label{Eqn:16}
\end{equation}
where,
\begin{equation}
K = \sum\limits_{B \cup C = \emptyset}m_1(B) m_2(C) \label{Eqn:17}
\end{equation}is a measure of the conflict in the experts opinions. The
normalization factor, $(1-K)$, has the effect of completely ignoring
conflict and attributing any mass associated with conflict to a null
set.

The $5$ LCPDs, $P^k(\mathbf z)$, were considered as experts towards
voting on the test input as a face or non-face. In order to use
these mapped values in Equation \ref{Eqn:16} - \ref{Eqn:17}, we
normalized evidences generated by the experts to map between
$[0,1]$, and any conflict of opinions were added into the conflict
factor, $K$. For the sake of clarity, we show an example of
combining two expert opinions in Figure \ref{Fig:DST}. The same idea
could be extended to multiple experts.

\begin{figure}[h]
\centering
\includegraphics[width=3.5in]{Figure11.jpg}
\caption{An example of combining
evidence from two experts under Dempster-Shafer Theory.}
\label{Fig:DST}
\end{figure}

\subsection{Coarse Pose estimation}\label{CoarsePoseEstimation}
\vspace{-0.2in} Since the RF models were biased with pose
information, we also investigated the possibility of determining the
pose of the face based on the evidences obtained from the LCPDs. We
noticed that the LCPDs $P^3(\mathbf z)$, $P^4(\mathbf z)$ and
$P^5(\mathbf z)$ were capable of not only discriminating faces from
non-faces, but were also capable of voting towards one of $3$ pose
classes, Looking right, Frontal, and Looking Left along with a
confidence metric. Due to space constraints, the procedure is not
explained in detail, but it is similar to what was followed for face
versus non-face discrimination as explained in Section
\ref{CombiningEvidence}.

%-------------------------------------------------------------------------
\section{Experiments}\label{Experiments} \vspace{-0.2in} In all our
experiments, Viola-Jones face detection algorithm
\cite{viola_robust_2004} was used for extracting face subimages. The
proposed face validation filter was tested on two face image data
sets, 1. {\emph The FERET Color Face Database}, and 2. {\emph An
in-house face image database} created from interview videos of
famous personalities.

In order to prepare the data for processing, face detection was
performed on all the images in both the data sets. The number of
face detections do not directly correlate to the number of unique
face images as there are plenty of false detections. We manually
identified each and every face detection to be {\emph true} or {\emph
false} so that ground truth could be established. The details of
this manual labeling is shown below:
\begin{enumerate}
\item {\em FERET}
\begin{itemize}
\item Number of actual face images: $14,051$
\item Number of faces detected using Viola-Jones algorithm: $6,208$
\item Number of {\emph true} detections: $4,420$
\item Number of {\emph false} detections: $1,788$ ($28.8$\%)
\end{itemize}
\item {In-house database}
\begin{itemize}
\item Number of actual face images: $2,597$
\item Number of faces detected using Viola-Jones algorithm: $2,324$
\item Number of {\emph true} detections: $2,074$
\item Number of {\emph false} detections: $250$ ($10.7$ \%)
\end{itemize}
\end{enumerate}

 %-------------------------------------------------------------------------
\section{Results} \label{Sec:Results} \vspace{-0.2in}
 In order to compare the
 performance of the proposed face validation filter, we defined four parameters:
 \begin{enumerate}
 \item Number of false detections (NFD)
 \begin{equation}
\mbox{NFD} = \mbox{Count of false detections} \nonumber
 \end{equation}
 \item False detection rate (FDR): \begin{equation}
 \mbox{FDR} = \frac{\mbox{\# of false detections}}{\mbox{Total \# of face
 detections}} \mbox{x} 100 \nonumber
 \end{equation}
 \item Precision (P)
 \begin{equation}
 \mbox{P} = \frac{\mbox{\# of true detections}}{\mbox{\# of true detections} + \mbox{\# of false
 detections}} \nonumber
 \end{equation}
 \item Capacity (C)
 \begin{equation}
 \mbox{C} = \left(\frac{\mbox{\# of true detections}}{\mbox{\# of actual faces in database}}\right) -
 \mbox{FDR} \nonumber
 \end{equation}
 \end{enumerate}



\begin{table}[h]
\caption{Face detection validation results on FERET database.} \label{Tab:FERET}
\centering
 \begin{tabular}{|c||c|c|}
   \hline
   % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
    & Before Validation & After Validation \\
    \hline
    \hline
    NFD  &  $1,788$ & $208$ \\
   FDR & $28.8$ \% & $3.35$ \% \\
   P & $0.7120$ & $0.9551$ \\
   C & $0.026$ & $0.281$ \\
 \hline
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Face detection validation results on the in-house face database.}
\centering
 \begin{tabular}{|c||c|c|}
   \hline
   % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
    & Before Validation & After Validation \\
    \hline
    \hline
   NFB & $250$  &  $2$\\
   FDR & $10.76$ \% & $0.01$ \%\\
   P & $0.892$ & $0.999$ \\
   C & $0.691$ & $0.798$ \\
   \hline
 \end{tabular}

\label{Tab:Inhouse}
\end{table}


  As explained in Section \ref{CoarsePoseEstimation}, the framework
  was extensible to perform coarse pose estimation. Figure
  \ref{Fig:Result} shows the result of passing two frames of a video
  sequence as input the face validation filter. The frames were
  extracted from a video of the same individual exhibiting arbitrary facial
  motion. The frames were $0.55$ seconds apart. As can be noticed,
  the head pose is slightly different between the two frames. The
  pose estimation results are shown below the two frames.

\begin{figure}[h]
\centering
\includegraphics[width=3.5in]{Figure12.jpg}
\caption{Coarse pose estimation.} \label{Fig:Result}
\end{figure}

\section{Discussion of Results}
\vspace{-0.2in} Performance analysis of the proposed face validation
filter can be understood through the four parameters defined in
Section \ref{Sec:Results}. {\textbf NFB} and {\textbf FDR} are direct
measurements of the number of mistakes (naming non-faces as faces)
made by the face detection algorithm on the two data sets. As can be
verified from Table \ref{Tab:FERET} and \ref{Tab:Inhouse}, there is
a significant reduction in the false detections through the
introduction of the filter.

The precision parameter, {\textbf P}, can be perceived as the
probability that a face detection result retrieved at random will
truly contain a face. It can be seen that the precision of the
system drastically improves with the introduction of the face
validation filter thereby assuring a {\emph true} face subimage at the
output.

The capacity parameter, {\textbf C}, measures the relative difference
between face detection and false detection rates of a face detection
system. Alternately, {\textbf C} can be considered to measure the net
{\emph true} face detection ability of any algorithm on a specific
face data set. {\textbf C} ranges from $-1$ to $1$. $-1$ when none of
the faces in the database are detected with all reported detections
being wrong. $1$ when all the faces in the database are detected
with no false detections. It can be seen from Tables \ref{Tab:FERET}
and \ref{Tab:Inhouse} that the capacity of the face detection
system, when combined with face validation filter, is significantly
higher and moves towards $1$. One can thus infer that the combined
system has better {\emph true} face detection ability.

Finally, Figure \ref{Fig:Result} shows the coarse pose estimation
results. The two frames in the figure shows cases when the face is
slightly turned right, with one ({\textbf A}) turned more right than the
other ({\textbf B}). The face validation filter verifies that the faces
are actually turned right and the belief values represent a scale on
the amount of rotation. Since we did not do any specific mapping of
the belief values to pose angle, we could not confirm quantitatively
how accurate the pose estimations were. Through visual consort, one
can verify that the labeling is meaningful.

\chapter{EXOCENTRIC SENSING: ACCURATE TRACKING OF PEOPLE}
The problem of person localization in general is very broad in its scope and wide varieties of challenges such as variations in articulation, scale, clothing, partial appearances, occlusions, etc make this a complex problem. Narrowing the focus, this paper targets person localization in real world video sequences captured from the wearable camera of the Social Interaction Assistant. Specifically, we focus on the task of localizing a person who is approaching the user to initiate a social interaction or just conversation. In this context, the problem of person localization can be constrained to the cases where the person of interest is facing the user.

\begin{figure}[h]
\centering
\includegraphics[width=4in]{ClosePerson.JPG}
\caption{Person of interest at a short distance from camera}
\label{Fig:Figure100}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=4in]{FarPerson.JPG}
\caption{Person of interest at a large distance from camera}
\label{Fig:Figure101}
\end{figure}

When such a person of interest is in close proximity, his/her presence can be detected by analyzing the incoming video stream for facial features (Figure \ref{Fig:Figure100}). But when such a person is approaching the user from a distance, the size of the facial region in the video appears to be extremely small. In this case, relying on facial features alone would not suffice and there is a need to analyze the data for full body features (Figure \ref{Fig:Figure101}). In this work, we have concentrated on improving the effectiveness of the SIA by applying computer vision techniques to robustly localize people using full body features. Following section discusses some of the critical issues that are evident when performing person localization from the wearable camera setup of the SIA

\section{Challenges in Person Localization from a wearable camera platform}
A number of factors associated with the background, object, camera/object motion, etc. determine the complexity of the problem of person localization from a wearable camera platform. Following is a descriptive discussion of the imminent challenges that we encountered while processing the data using the SIA.

\subsection{Background Properties}
When the Social Interaction Assistant is used in natural settings, it is highly possible that there are objects in the background which move, thus causing the background to be dynamic. Also, there are bound to be regions in the background whose image features are highly similar to that of the person, thus leading to a cluttered background. Due to these factors, the problem of distinguishing the person of interest from the background becomes highly challenging in this context. Figures \ref{Fig:Figure103} and \ref{Fig:Figure104} illustrate the contrast in the data due to the nature of the background.

\begin{figure}[h]
\centering
\includegraphics[width=4in]{SimpleBackground.JPG}
\caption{Simple Background}
\label{Fig:Figure103}
\includegraphics[width=4in]{FarPerson.JPG}
\caption{Complex Background}
\label{Fig:Figure104}
\end{figure}

\subsection{Object Properties}
As we are interested in person localization, it can be clearly seen that the object is non-rigid in nature as there are appearance changes that occur throughout the sequence of images.  Further, significant scale changes and deformities in the structure can also be observed. Also, when analyzing video frames of persons approaching the user, the basic image features in various sub-regions of the object vary vastly. For example, the image features from the facial region are considerably different from that of the torso region. Tracking detected persons from one frame to another will require individualized tracking of each region to maintain confidence. This non-homogeneity of the object poses a major hurdle while applying localization algorithms and has not been studied much in the literature. Figure \ref{Fig:Figure105} shows the simplicity of the data when these problems are not present, while Figure \ref{Fig:Figure106} highlights complex data formulations in a typical interaction scenario.

\begin{figure}[h]
\centering
\includegraphics[width=4in]{rigidobject.JPG}
\caption{Rigid, Homogeneous Object}
\label{Fig:Figure105}
\includegraphics[width=4in]{nonrigidobject.JPG}
\caption{Non-Rigid, Deformable, Non-Homogeneous Object}
\label{Fig:Figure106}
\end{figure}

\subsection{Object/Camera Motion}
Traditionally, most computer vision applications use a static camera where strong assumptions of motion continuity and temporal redundancy can be made. But in our problem, as it is very natural for users to move their head continuously, the mobile nature of the platform causes abrupt motion in the image space (Compare Figure \ref{Fig:Figure107} and Figure \ref{Fig:Figure108}). This is similar to the problem of working with low frame rate videos or the cases where the object exhibits abrupt movements. Recently, there has been an increase of interest in dealing with this issue in computer vision research [5] [6-8]. Some important applications which are required to meet real-time constraints, such as teleconferencing over low bandwidth networks, and cameras on low-power embedded systems, along with those which deal with abrupt object and camera motion like sports applications are becoming common place [8]. Though solutions have been suggested, person localization through low frame rate moving cameras still remains an active research topic.

\begin{figure}[h]
\centering
\includegraphics[width=4in]{FarPerson.JPG}
\caption{Static Camera}
\label{Fig:Figure107}
\includegraphics[width=4in]{MotionBlur.JPG}
\caption{Mobile Camera}
\label{Fig:Figure108}
\end{figure}


\subsection{Other Important Factors Affecting Effective Person Tracking}
As the SIA is intended to be used in uncontrolled environments, changing illumination conditions need to be taken into account. Further, partial occlusions, self occlusions, in-plane and out-of-plane rotations, pose changes, blur and various other factors can complicate the nature of the data. See  Figure \ref{Fig:Figure108} for example situations where various factors can affect the video quality.

\begin{figure}[h]
\centering
\includegraphics[width=4in]{light.JPG}
\caption{Changing Illumination, Pose Change and Blur}
\label{Fig:Figure108}
\end{figure}

Given the nature of this problem, in this chapter we focus on the problem of robust localization of a single person approaching a user of the SIA using full-body features. Issues arising due to cluttered background along with object and camera motion have been handled towards providing robustness. In the following section we discuss some of the important related work in the computer vision literature.

\section{Related Computer Vision Work in Person Localization and Tracking}
Historically, two distinct approaches have been used for searching and localizing objects in videos. On one hand, there are detection algorithms which focus on locating an object in every frame using specific spatial features which are fine tuned for the object of interest. For example, haar-based rectangular features [9] and histograms of oriented gradients [10] can develop detectors that are very specific to objects in videos. On the other hand, there are tracking algorithms which trail an object using generic image features, once it is located, by exploiting the temporal redundancy in videos. Examples of features used by tracking algorithms include color histograms [11] and edge orientation histograms [12].

\subsection{Detection Algorithms}
As mentioned previously, detection algorithms exploit the specific, distinctive features of an object and apply learning algorithms to detect a general class of objects. They use information related to the relative feature positions, invariant structural features, characteristic patterns and appearances to locate objects within the gallery image. But, when the object is complex, like a person, it becomes difficult for these algorithms to achieve generality thereby failing even under minute non-rigidity. A number of human factors such as variations in articulation, pose, clothing, scale and partial occlusions make this problem very challenging.

When assumptions about the background cannot be made, learning algorithms which take advantage of the relative positions of body parts are used to build classifiers. The kind of low-level features generally used in this context are gradient strengths and gradient orientations [13,10], , entropy and haar-like features. Some of the well-known higher level descriptors are histogram of oriented gradients [10] and covariance features [14]. Efforts have been made to make these descriptors scale invariant as well.

In order to make these algorithms real-time, researchers have popularly resorted to two kinds of approaches. One category includes part-based approach such as Implicit Shape Models [5] and constellation models [15] which place emphasis on detecting parts of the object before integrating, while the other category of algorithms tries to search for relevant descriptors for the whole object in a cascaded manner[16].  Shape-based Chamfer matching [25] is a popular technique used in multiple ways for person detection as the silhouette gives a strong indication of the presence of a person. In recent times, Chamfer matching has been used extensively by the person detection and localization community. It has been applied with hierarchically arranged templates to obtain the initial candidate detection blocks so that they can be analyzed further by techniques such as segmentation, neural networks, etc. It has also been used as a validation tool to overcome ambiguities in detection results obtained by the Implicit Shape Model technique [18].

\subsection{Tracking Algorithms}
Assuming that there is temporal object redundancy in the incoming videos, many algorithms have been proposed to track objects over frames and build confidence as they go. Generally they make the simplifying assumption that the properties of the object depend only on its properties in the previous frame, i.e. the evolution of the object is a Markovian process of first order. Based on these assumptions, a number of deterministic as well as stochastic algorithms have been developed.

Deterministic algorithms usually apply iterative approaches to find the best estimate of the object in a particular image in the video sequence [16]. Optimal solutions based on various similarity measures between the object template and regions in the current image, such as sum of squared differences (SSD), histogram-based distances, distances in eigenspace and other low dimensional projected spaces and conformity to particular object models, have been explored [16]. Mean Shift is a popular, efficient optimization-based tracking algorithm which has been widely used.

Stochastic algorithms use the state space approach of modeling dynamic systems and formulate tracking as a problem of probabilistic state estimation using noisy measurements [20]. In the context of visual object tracking, it is the problem of probabilistically estimating the object's properties such as its location, scale and orientation by efficiently looking for appropriate image features of the object. Most of these stochastic algorithms perform Bayesian filtering at each step for tracking, i.e. they predict the probable state distribution based on all the available information and then update their estimate according to the new observations. Kalman filtering is one such algorithm which fixes the type of the underlying system to be linear with Gaussian noise distributions and analytically gives an optimal estimate based on this assumption. As most tracking scenarios do not fit into this linear-Gaussian model and as analytic solutions for non-linear, non-Gaussian systems are not feasible, approximations to the underlying distribution are widely used from both parametric and non-parametric perspective.

Sequential monte-carlo based Particle Filtering techniques have gained a lot of attention recently. These techniques approximate the state distribution of the tracked object using a finite set of weighted samples using various features of the system. For visual object tracking, a number of features have been used to build different kinds of observation models, each of which have their own advantages and disadvantages. Color histograms[11], contours[21], appearance models, intensity gradients[22], region covariance, texture, edge-orientation histograms, haar-like rectangular features [16] , to name a few. Apart from the kind of observation models used, this technique allows for variations in the filtering process itself. A lot of work has gone into adapting this algorithm to better perform in the context of visual object tracking.
	
While both the areas of detection and tracking have been explored extensively, there is an impending need to address some of the issues faced by low frame rate visual tracking of objects. Especially in the case of SIA, person localization in low frame rate video is of utmost importance. In this paper, we have attempted to modify the color histogram comparison based particle filtering algorithm to handle the complexities that occur mobile camera on the Social Interaction Assistant.

\section{Conceptual Framework}
As discussed in the previous section, detection and tracking offer distinctive advantages and disadvantages when it comes to localizing objects. In the case of SIA, thorough object detection is not possible in every frame due to the lack of computational power (on a wearable platform computing platform) and tracking is not always efficient due to the movement of the camera and the object's (interaction partner's) independent motion. Though there are clear advantages in applying these techniques individually, the strengths of both these approaches need to be combined in order to tackle the challenges posed by the complex setting of the SIA. In the past, a few researchers have approached the problem of tracking in low frame rate or abrupt videos by interjecting a standard particle filtering algorithm with independent object detectors [23]. In our experience, the Social Interaction Assistant offers a weak temporal redundancy in most cases. We exploit this information trickle between frames to get an approximate estimate of the object location by incorporating a deterministic object search while avoiding the explicit use of pre-trained detectors. Due to the flexibility in the design, particle filtering algorithms provide a good platform to address the issues arising due to complex data. These algorithms give an estimate of an object's position by discretely building the underlying distribution which determines the object's properties. But, real-time constraints impose limits on the number of particles and the strength of the observation models that can be used. This generally causes the final estimate to be noisy when conventional particle filtering approaches are applied. Unless the choice of the particles and the observation models fit the underlying data well, the estimate is likely to drift away as the tracking progresses. To mitigate these problems faced in the use of the SIA, we propose a new particle filtering framework that gets an initial estimate of the person's location by spreading particles over a reasonably large area and then successively corrects the position  though a deterministic search in a reduced search space. Termed as Structured Mode Searching Particle Filter (SMSPF), the algorithm uses color histogram comparison in the particle filtering framework at each step to get an initial estimate which is then corrected by applying a structured search based on gradient features and chamfer matching. The details of this algorithm are described in the next section.

\section{STRUCTURED MODE SEARCHING PARTICLE FILTER}
Assuming that an independent person detection algorithm can initialize this tracking algorithm with the initial estimate of the person location, this particle filtering framework focuses on tracking a single person under the following circumstances, namely
\begin{itemize}
\item Image region with the person is non-rigid and non-homogeneous
\item Image region with the person exhibits significant scale changes
\item Image region with the person exhibits abrupt motions of small magnitude in the image space due to the movement of the camera.
\item Background is cluttered.
\end{itemize}

The algorithm progresses by implementing two steps on each frame of the incoming video stream. In the first step (Figure \ref{Fig:Figure109}), an approximate estimate of the person region is obtained by applying a color histogram based particle filtering step over a large search space. This is followed by a refining second step (Figure \ref{Fig:Figure110}) where the estimate is corrected by applying a structured search based on gradient features and Chamfer matching.  These two steps have been described in detail below.

\begin{figure}[h]
\centering
\includegraphics[width=4in]{Step1.JPG}
\caption{SMSPF - Step 1}
\label{Fig:Figure109}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=4in]{Step2.JPG}
\caption{SMSPF - Step 2}
\label{Fig:Figure110}
\end{figure}

\subsection{Step 1: Particle Filtering Step}
In the context of SIA, as the person of interest can exhibit abrupt motion changes in the image space, it is extremely difficult to model the placement of the person in the current image based on the previous frame's information alone. When such data is modeled in the Bayesian filtering based particle filtering framework, the state of each particle's position becomes independent of its state in the previous step. Thus, the prior distribution can be considered to be a uniform random distribution over the support region of the image.

\begin{equation}
p\left( x_t^i|x^i_{t-1} = p(x_t^i)\right)
\end{equation}

As it is essential for particle filtering algorithm to choose a good set of particles, it would be useful to pick a good portion of them near the estimate in the previous step. By approximating this previous estimate to be equivalent to a measurement of the image region with the person in the current step, the proposal distribution of each particle can be chosen to be dependent only on the current measurement

\begin{equation}
q\left(x_t^i|x_{t-1}^iZ_t\right) = q\left(x_t^i|Z_t\right)
\end{equation}

Though the propagation of information through particles is lost by making such an assumption, it gives a better sampling of the underlying system. We employ a large variance Gaussian with its mean centered at the previous estimate for successive frame particle propagation. By using such a set of particles, a larger area is covered, thus accounting for abrupt motion changes and a good portion of them are picked near the previous estimate, thus exploiting the weak temporal redundancy.  As in [11], we have employed this technique using HSV color histogram comparison to get likelihoods at each of the particle locations. Since intensity is separated from chrominance in this color space, it is reasonably insensitive to illumination changes. We use an 8x8x4 HSV binning thereby allowing lesser sensitivity to changes in V when compared to chrominance. The histograms are compared using the well-known Bhattacharyya Similarity Coefficient which guarantees near optimality and scale invariance.

\begin{figure}
\centering
\includegraphics[width=4in]{StructuredSearch.JPG}
\caption{Structured Search}
\label{Fig:Figure111}
\end{figure}

With the above step alone, due to the small number of particles which are spread widely across the image, we can get an approximate location of the person. When such an estimate partially overlaps with the desired person region, the best match occurs between the intersection of the estimate and the actual person region as shown in Figure \ref{Fig:Figure111}. But, it is not trivial to detect this partial presence due to the existence of background clutter.  To handle this problem, we introduce a second step which uses efficient image feature representations of the desired person object and employs an efficient search around the estimate to accurately localize the person object.

\subsection{Step 2: Structured Search}
As the estimate obtained using widely spread particles gives the approximate location of the object, the search for the image block with a person in it can be restricted to a region around it. We have employed a grid-based approach to discretely search for the object of interest (a person) instead of checking at every pixel. By dividing the estimate into an m x n grid and sliding a window along the bins of the grid as shown in Figure \ref{Fig:Figure112}, the search space can be restricted to a region close to the estimate. By finding the location which gives the best match with the person template, we can localize the person in the video sequence with better accuracy.

\begin{figure}
\centering
\includegraphics[width=4in]{SearchStructure.JPG}
\caption{Sliding window of the Structured Search (Green: Estimate; Red: Sliding window).}
\label{Fig:Figure112}
\end{figure}

If this search is performed based on scale-invariant features, then it can be extended to identify scale changes as well. In order to achieve search over scale, the estimate and the sliding window need to be divided into different number of bins. If the search is performed using smaller number of bins as compared to the estimate, then shrinking of the object can be identified while searching with higher number of bins can account for dilation of the object. For example, if a (m-1) x (n-1) grid is used with the sliding window while a m x n grid is used with the estimate, then the best match will find a shrink in the object size. Similarly if an m x n grid sliding window is used with a (m-1) x (n-1) estimate grid, then dilations can be detected. It can be seen that this search is characterized by the number of bins m x n into which the sliding window and the estimate are divided. Based on the nature of the problem, the number of bins and the amount of sweep across scale and space can be adjusted. Currently, these parameters are being set manually, but the structured search framework can be extended to include online algorithms which can adapt the number of grid bins based on the evolution of the object.

If the object of interest was simple, then the best match across space and scale could be obtained by using simple feature matching techniques. But, due to the complex nature of the data, strong confidence is required while searching for the person region across scale. To this end, we propose to perform the structured search by analyzing the internal features of the person region as well as the external boundary/silhouette features and aggregating the confidence obtained from these two measures to refine the person location estimate in the image (Figure \ref{Fig:Figure113})

\begin{figure}
\centering
\includegraphics[width=4in]{ComparisonTracking.JPG}
\caption{Structured Search Matching Technique}
\label{Fig:Figure113}
\end{figure}

In literature, gradient based features have been widely used for person detection and tracking problems and their applicability has been strongly established by various algorithms like Histogram of Oriented Gradients (HoGs) [10]. Following this principle, we have used the Edge Orientation Histogram (EOH) features [12] in order to obtain the internal content information measure. For this purpose, a gradient histogram template (GHT) is initially built using a generic template image of a walking/standing person. This GHT is then compared with the gradient histogram of each structured search block using the Bhattacharyya histogram comparison as in [11] in order to find the block with the best internal confidence. In our implementation, orientations are computed using the Sobel operator and the gradients are then binned into 9 discrete bins. These features were extracted using the integral histogram concept [27] to facilitate computationally efficient searching.

Similarly, in order to obtain the boundary confidence measure, a generic person silhouette template (GPT) (as shown in Figure 13) is used to perform a modified Chamfer match on each of the search blocks. In general, Chamfer matching is used to search for a particular contour model in an edge map by building a distance transformed image of the edge map. Each pixel value in a distance transformed image is proportional to the distance to its nearest edge pixel. In order to compare the edge map to the contour map, we convolve the edge image with the contour map. If the contour completely overlaps with the matching edge region, we get a chamfer match value of zero. Based on how different the edge map is to the template contour, the chamfer match score will increase and move towards 1. A chamfer match score of 1 implies a very bad match.

While the theory of chamfer matching offers elegant search score, in reality, especially with clutter within the object's silhouette, it is very difficult to get an exact match score. In SIA, since the data is very noisy and complex, certain modifications need to be made with the Chamfer matching algorithm in order achieve good performance. The following section details a modified Chamfer match algorithm introduced in this work.

\subsection{Chamfer Matching in Structured Search}
As discussed above, Chamfer matching gives a measure of confidence on the presence of the person within an image based on silhouette information. We have incorporated this confidence into the structured search in order to detect the precise location of the person around the particle filter estimate. An edge map of the image under consideration is first obtained which is then divided into (m x n) windows in accordance with the structured search and an elliptical ring mask is then applied to each of these windows as shown in Figure \ref{Fig:Figure114}. This mask is applied so as to eliminate the edges that arise due to clothing and background thereby emphasizing the silhouette edges which are likely to appear in the ring region if a window is precisely placed on the object perimeter. A distance transformed image of the window is then obtained using the masked edges.

\begin{figure}
\centering
\includegraphics[width=5.5in]{Campher.JPG}
\caption{Incorporating Chamfer Matching into Structured Search}
\label{Fig:Figure114}
\end{figure}

By applying the modified chamfer matching (with a generic person contour resized to the current particle filter estimate), a confidence number in locating the desired object within the image region can be obtained. Similar to the Chamfer matching as before, a value close to 0 indicates a strong confidence of the presence of a person and vice versa. As 1 is the maximum value that can be obtained by the chamfer match, this measure can be incorporated into the match score of the structured search using the following equation.

\begin{equation}
\mbox{BoundaryConf} = (1 - \mbox{ChamferMatch})
\end{equation}

The standard form of Chamfer Matching gives a continuous measure of confidence in locating an object in an edge map. But, in our case, when the elliptical ring mask is used to filter out the noisy edges in each search block, this nature of Chamfer match is lost. Since the primary goal of the structured search is to find a single best matching location of the person, it is more advantageous to use the filter mask at the cost of losing this continuous nature of the chamfer match. Further, as it is very likely that the person region is close to the approximate estimate obtained from the first step, one of the search windows of the structured search is bound to capture the entire person object thus resulting in a good match score.

From the above discussion, it can be seen that combining the knowledge about the internal structure of the person region with the silhouette information results in a greater confidence in the SMSPF algorithm. Further, using such complementary features in the structured search robustly corrects the approximate estimate obtained from the particle filtering step while handling various problems associated with search across scale.

\section{Experiments and Datasets}
\subsection{Datasets}
The performance of the structured mode searching particle filter (SMSPF) has been tested using three datasets where a single person faces the camera while approaching it. There are significant scale changes in each of these sequences. Further, non-rigidity and deformability of the person region can also be clearly observed. Different scenarios with varying degrees of complexity of the background and camera movement have been considered. Following is a brief description of these datasets.
\begin{enumerate}[(a)]
\item \emph{DataSet \footnote{Collected at CUbiC }}: Plain Background; Static Camera; 320x240 resolution
\item \emph{DataSet \footnote{CASIA  Gait Dataset B with subject approaching the camera [4]}}: Slightly cluttered Background; Static Camera; 320x240 resolution
\item \emph{DataSet \footnote{Collected at CUbiC}}: Cluttered Background;  Mobile Camera; 320x240 resolution
\end{enumerate}

Figure \ref{Fig:Figure115} shows the sample results on each of the datasets used.

\begin{figure}
\centering
\includegraphics[width=4in]{Results.JPG}
\caption{SMSPF Results}
\label{Fig:Figure115}
\end{figure}

\subsection{Evaluation Metrics}
In order to test the robustness of this algorithm and the applicability in complex situations, its performance has been compared with the Color Particle Filtering algorithm [25]. Assuming that a detection algorithm can detect persons in at least some frames, the image region containing the person in each of the test sequences has been manually set. The following two criteria have been used to evaluate their performance [3].
\begin{itemize}
\item Area Overlap (A0)
\item Distance between Centroids  (DC)
\end{itemize}

Manually labeled rectangular regions around the person in the image have been used as the ground truth. Suppose $\mbox{gTruth}_i$ is the ground truth in the $\mbox{i}^{th}$ frame and $\mbox{track}_i$ is the rectangular region output by a tracking algorithm, then the area overlap criterion is defined as follows

\begin{equation}
AO\left(\mbox{gTruth}_i,  \mbox{track}_i\right) = \frac{Area\left(\mbox{gTruth}_i \cap \mbox{track}_i\right)}{AO\left(\mbox{gTruth}_i \cup  \mbox{track}_i\right)}
\end{equation}

The average area overlap can be computed for each data sequence as

\begin{equation}
AvgAOR=\frac{1}{N}\sum\limits_{i=1}^N AO
\end{equation}

Similar to [3], we use Object Tracking Error (OTE) which is the average distance between the centroid of the ground truth bounding box and the centroid of the result given by a tracking algorithm

\begin{equation}
OTE = \frac{1}{N}\sum\limits_{i=1}^N \sqrt{\left(Centroid_{gTruth_i} - Centroid_{Truth_i} \right)}
\end{equation}

In order to evaluate the performance of these algorithms using a single metric which encodes information from both area overlap and the distance between centroids, we have used a measure termed as the Tracking Evaluation Measure (TEM) which is the harmonic mean of the average area overlap fraction (AvgAOR) and a non-linear mapping of the Object tracking error (OTE).

\begin{equation}
TEM=2*\frac{AvgAOR . e^{-k.OTE}}{AvgAOR + e^{-k.OTE}}
\end{equation}

where $k$ is a constant which exponentially penalizes the cases where the distance between centroids is large.

\section{Results}
Particle Filtering has been widely used to handle complex scenarios by maintaining multiple hypotheses. As mentioned in [21], in order to handle abrupt motion changes, it is essential that the particles are widely spread while tracking. Following this principle, we have compared the performance of color particle filter (PF) [25] and the structured mode searching particle filter (SMSPF) by using a 2-D Gaussian with large variance as the system model. The position of the person and its scale have been included in the state vector. In order to compensate for the computational cost of structured search, only 50 particles were used for the SMSPF algorithm while 100 particles were used for the PF algorithm. A 10x10 grid with a sweep of 8 steps along the spatial dimension and 3 steps along the scale dimension were incorporated in the structured search.

\begin{figure}
\centering
\includegraphics[width=5.5in]{Result1.JPG}
\caption{AO (Dotted Line: Color PF; Solid Line: SMSPF)}
\label{Fig:Figure116}
\end{figure}

Figure \ref{Fig:Figure116} and Figure \ref{Fig:Figure117} illustrate the comparison of the area overlap ratio and the distance between centroids at each frame of an example sequence. The sample frames are shown beside the tracking results. From Figure \ref{Fig:Figure116}(a), it is evident that the SMSPF algorithm (red) shows a significant improvement over the color particle filter algorithm (green). Here, the area overlap ratio using SMSPF is much closer to 1 in most of the frames while the color particle filter drifts away causing this measure to be closer to 0. The distance between centroids measure also indicates a greater precision of the SMSPF algorithm as seen in Figure \ref{Fig:Figure117}(a) where the distance between centroids using color particle filter is much higher than that with SMSPF($\approx$ 0).

\begin{figure}
\centering
\includegraphics[width=5.5in]{Result2.JPG}
\centering
\caption{DC(Dotted Line: Color PF; Solid Line: SMSPF)}
\label{Fig:Figure117}
\end{figure}

Figure \ref{Fig:Figure118}, Figure \ref{Fig:Figure119} and Figure \ref{Fig:Figure120} show the Tracking Evaluation Measure (TEM) for Datasets 1, 2 and 3.  In majority of the cases, the SMSPF algorithm outperforms the color particle filtering algorithm with a higher TEM score.


\begin{figure}
\centering
\includegraphics[width=4.5in]{ResultBar1.JPG}
\caption{Evaluation Measure for DataSet 1}
\label{Fig:Figure118}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=4.5in]{ResultBar2.JPG}
\caption{Evaluation Measure for DataSet 2}
\label{Fig:Figure119}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=4.5in]{ResultBar1.JPG}
\caption{Evaluation Measure for DataSet 3}
\label{Fig:Figure120}
\end{figure}

The results presented as a comparison between Color PF and SMSPF shows that incorporating a deterministic structured search into the stochastic particle filtering framework improves the person tracking performance in complex scenarios. The SMSPF algorithm strikes a balance between specificity and generality offered by detection and tracking algorithms as discussed in Section 2. It uses specific structure-aware features in the search in order to handle non-homogeneity of the object and the cluttered nature of the background. On the other hand, generality is maintained by using simple, global features in the particle filtering framework so as to handle non-rigidity and deformability of the object. The clear advantage of using the structured search can be observed on the complex Dataset 3 which encompasses most of the challenges generally encountered while using the Social Interaction Assistant.


%\newpage
\vspace*{1in}
%\chapter*{REFERENCES\hfill} \addcontentsline{toc}{chapter}{REFERENCES}
\begin{SingleSpace}
\bibliographystyle{ieeetr}	
\bibliography{references}
\end{SingleSpace}
%\clearpage

%% maybe endnotes
%% maybe bibliography
% if appendices, then

\appendix
\addcontentsline{toc}{chapter}{APPENDIX}
\chapter{\uppercase{Algorithm for Estimating Rank Average of Groups}}
\label{AppendixA}
\clearpage
While analyzing the responses of participants to the online survey, the participants responses for each question are represented as entries $x_{i,q}$, where, $i$ represents the $i^{th}$ participant and $q$ represents the $q^{th}$ question. $i = 1,\ldots,N$ are the $N$ participants who responded on the survey, and $q = 1, \ldots, Q$ are the $Q$ questions. In the survey presented in Chapter XXX, $N = 28$ and $Q = 8$.

\subsection{Procedure}

\begingroup
\setlength{\parindent}{0in}
\emph{Input:} Each participants response is considered as an entry $e_m$ into a pool $E = \{x_{i,q}\}$, where, $m=1,\ldots,M$, and $M=N$x$Q$. \\ \\
\emph{Ouput:} The rank average for the $Q$ groups (questions), $\bar{R}_m$.\\ \\
\emph{Steps:}
\begin{enumerate}[1.]
\item Group $e_n \in E$ removing all group affiliations.
\item Order the entries from $1$ to $M$ and assign a rank $r_{iq}$.
\item Assign any tied values the average of the ranks they would have received had they not been tied.
\item Rank Average for each group is then given as
\begin{equation}
\bar{R}_m = \frac{\displaystyle\sum\limits_{i \in Q_m, q=m}r_{iq}}{n_m}
\end{equation}
Where, $Q_m$ represents the group $m$ with the cardinality $n_m$.
\end{enumerate}
\endgroup

Since no assumptions on the distribution of the response are made, unlike the mean, the rank average gives a non-parametric method for comparing the groups.

\chapter{\uppercase{Insert Appendix B Title here}}
\clearpage

% if Biographical sketch then
%\newpage
%\newpage
%\begin{center}\bibname\end{center} \addcontentsline{toc}{chapter}{\bibname}
%\clearpage
%[Enter your text here]
%\clearpage
%\newpage	

This LaTeX document was generated using the Graduate College Format Advising tool. Please turn a copy of this page in when you submit your document to Graduate College format advising. You may discard this page once you have printed your final document. DO NOT TURN THIS PAGE IN WITH YOUR FINAL DOCUMENT!


\end{document}		
		